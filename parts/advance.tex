\part{Advanced Topics\label{part:advanced-topics}}




\chapter{Proof Generation and Checking}\label{chapter:proof-gen-check}

%Advances in DNN verification have made it possible to reason about the safety and robustness of complex DNNs. 
As DNN tools become more complex (e.g., SOTA tools have 20K LoCs), they are more prone to bugs. VNN-COMP'23~\cite{brix2023fourth} showed that 3 of the top 7 participants produced unsound results, in which they incorrectly claiming unsafe DNNs are safe. This undesirable behavior defeats the purpose of DNN verification and hinders its practical adoption.

While checking counterexamples, which demonstrate the unsafety of a DNN, is relatively straightforward, certifying the safety of a DNN is far more challenging. That is, certifying \texttt{unsat} results---proving no counterexample exists---is far more challenging. This would require verifiers to track their decision steps, which are often complex and large. %Moreover, there are many verification approaches (e.g., branch and bound, conflict-driven learning, abstraction and refinement), making it difficult to standardize and compare results across different tools.
%This is unlike SAT solving, where most tools adopt the standard DPLL framework and thus allows for unified techniques in proof generation and checking (e.g., the DRAT family of algorithms for UNSAT proofs).

\section{Proof Generation}\label{sec:proofgen}

% To successfully deploy DNN verification tools in safety-critical applications, it is essential to provide guarantees that the verification results are correct, i.e., it won't claim an unsafe DNN as safe. One way to achieve this is to generate proofs of the verification results, i.e., a sequence of logical steps that demonstrates the validity of the verification result.
% Ideally, the proof generation process should be automated and integrated into the verification tool, so that the proof can be generated and checked automatically. The challenge is that there are many different DNN verification techniques, each with its own decision-making process, making it difficult to standardize proof generation across different tools. 



A proof of satisfiability (sat) is an input that violates the property, i.e., a counterexample.  We can easily  check such a counterexample $c$ by evaluating $\phi(c,N(c))$ (i.e., running the DNN on the counterexample).
In fact, VNN-COMPs already requires competing DNN verification tools to return counterexamples demonstrating satisfiability.  

In contrast, the proof of an unsatisfiability result (which explains why \emph{no possible inputs} can violate the property) is inherently more complex to generate (\autoref{sec:proofgen}), requires a more sophisticated encoding (\autoref{sec:prooflang}), and an efficient checking algorithm (\autoref{sec:proofchecking}). We are mainly interested in \texttt{unsat} proofs.


\begin{algorithm}[t]
    \footnotesize
  
    \Input{DNN $\mathcal{N}$, property $\phi_{in} \Rightarrow \phi_{out}$}
    \Output{($\unsat, \prooftree$) if property is valid, otherwise ($\sat, \counterexample$)}
    \BlankLine


    $\problems \leftarrow \{ \emptyset \}$ \tcp{initialize verification problems} 
    $\prooftree \gets \{ ~ \}$ \tcp{initialize proof tree}\label{line:prooftree}
    
    \While(\tcp*[h]{main loop}){$\problems$}{\label{line:dpllstart}
        % \tcp{$\sigma_i$ is the activation pattern of problem $i$-th}
        $\sigma_i \gets \Select(\problems)$ \tcp{process problem $i$-th}
        % \Parfor(\tcp*[h]{process in parallel}){$\sigma_i ~\In~ \problems$}{ \label{line:parfor}
            \If{\Deduce{$\mathcal{N}, \phi_{in}, \phi_{out}, \sigma_i$}}{\label{line:deduce}
                $(\counterexample, v_i) \leftarrow \Decide(\mathcal{N}, \phi_{in}, \phi_{out}, \sigma_i)$ \\ \label{line:decide}
                \If(\tcp*[h]{found a valid counter-example}){$\counterexample$}{
                    \Return{$(\sat, \counterexample)$} 
                }
                \tcp{create new activation patterns}
                $\problems \leftarrow \problems \cup \{ \sigma_i \land v_i ~;~ \sigma_i \land \overline{v_i} \}$ \;
            }
            \Else(\tcp*[h]{detect a conflict}){
                % $\clauses \leftarrow \clauses \cup \AnalyzeConflict(\igraph_i)$ \\ \label{line:analyze_conflict}
                $\prooftree \leftarrow \prooftree \cup \{ \sigma_i \}$ \tcp{build proof tree} \label{line:record_proof}
            }
        % }
        % \If(\tcp*[h]{no more problems}){$\isEmpty(\problems)$}{
        % }
        
    }\label{line:dpllend}
    \Return{$(\unsat, \prooftree)$}
    
    \caption{The \proofgen{} DNN verification with proof generation.}\label{alg:bab-proof}
\end{algorithm}



%\emph{Branch-and-bound} (BaB) is a common approach to DNN verification and used by major DNN verification tools including~\cite{bunel2020branch,wang2021beta,ferrari2022complete,duong2024harnessing,duong2023dpllt,ovalbab}.
%In BaB algorithms, ``branch'' means splitting the problem into smaller subproblems (like a search tree), and ``bound'' means computing the upper and lower bounds, using abstraction, of the problem to prune the search space. 



%Among modern DNN verification techniques, only the simplex-based approach attempts to generate proofs~\cite{desmartin2023towards,barbosa2023generating}. However, generating a proof for simplex-based approaches is non-trivial and incurs a significant overhead during the verification process~\cite{desmartin2023towards,barbosa2023generating}.

%Recently, an approach based on the DPLL(T) algorithm in SAT/SMT solving has shown much promise~\cite{duong2023dpllt}. For example, in its debut year, the DPLL(T)-based \neuralsat{} DNN verifier~\cite{duong2023dpllt} won the New Comer Award at VNN-COMP'23~\cite{brix2023fourth} and outperformed other state-of-the-art DNN verification tools, especially for very large networks~\cite{duong2024harnessing}. More importantly, DPLL(T) approach maintains \emph{implication graph}, an internal data structure to keep track of assignments, which essentially represent a  proof of the verification result. In other words, proofs are a built-in feature of DPLL(T) approach, and thus can be generated with minimal (in fact, \emph{zero}) overhead.

\subsection{Proof Generation for Branch and Bound (BaB) Algorithms}\label{sec:proogen}

As mentioned in~\autoref{sec:bab}, major DNN verification techniques share the common ``branch and bound'' (BaB) search algorithm. The BaB structure, shown in~ \autoref{alg:bab}, splits the problem into smaller subproblems and use abstraction to compute bounds to prune the search space. This commonality allows us to bring proof generation capabilities with minimal overhead to existing DNN verification tools.

\autoref{alg:bab-proof} extends~\autoref{alg:bab} to show \proofgen, a BaB-based DNN verification algorithm with proof generation capability. The key idea is to introduce a proof tree (\autoref{line:prooftree}) and recording the branching decisions to the proof tree (\autoref{line:record_proof}).
The proof tree is a binary tree structure, where each node represents a neuron and its left and right edges represent its activation decision (active or inactive).
At the end of the verification process, the proof tree is returned as the proof of \texttt{unsat} result. 



% \bab{}, a reference architecture~\cite{nakagawa2014consolidating} 
% for modern DNN verifiers that we use to illustrate our observations.  
% \bab{} takes as input a ReLU-based DNN $\mathcal{N}$ and a formulae $\phi_{in}\Rightarrow \phi_{out}$ representing the property of interest.

%\bab{} iterates between two components: \texttt{Decide} (branching, \autoref{line:decide}), which decides (assigns) an activation status value for a neuron, and \texttt{Deduce} (bounding, \autoref{line:deduce}), which checks the feasibility of the current activation pattern. 
%To add proof generation capability, \bab{} is instrumented with a proof tree (\texttt{proof}) variable (\autoref{line:prooftree}) to record these branching decisions. The proof is represented as a binary tree structure, where each node represents a neuron and its left and right edges represent its activation decision (active or inactive). %The proof tree is then used to generate a proof in the \prooflang{} format (\autoref{sec:proof-format}).




\begin{figure}[t]
    \begin{minipage}[b]{\linewidth}
        \centering
        \begin{minipage}[t]{0.48\textwidth}
            \centering  
            \includegraphics[width=\linewidth]{figure/proof_net.pdf}
            \caption*{(a)}
        \end{minipage}
        \begin{minipage}[t]{0.48\textwidth}
            \centering
            \includegraphics[width=\linewidth]{figure/proof_tree.pdf}
            \caption*{(b)}
        \end{minipage}
        \caption{(a) A simple DNN.  (b) A proof tree produced verifying the property $(x_1, x_2) \in [-2.0, 2.0] \times [-1.0, 1,0] \Rightarrow (y_1 > y_2)$.}
        \label{fig:example}
    \end{minipage}
\end{figure}
%White nodes correspond to branching nodes where \neuralsat{} makes decisions to split ReLU neurons.

\paragraph{Example} We reuse the example in~\autoref{sec:bab} to illustrate \proofgen{}. Recall the goal is to verify that the DNN in \autoref{fig:example}(a) (a redraw of~\autoref{fig:example1}) has the property $(x_1, x_2) \in [-2.0, 2.0] \times [-1.0, 1,0] \Rightarrow (y_1 > y_2)$. \proofgen{} generates the proof tree in \autoref{fig:example}(b) to show unsatisfiability, i.e., the property is valid.

First, \proofgen{} initializes the activation pattern set \functiontextformat{ActPatterns} with an empty activation pattern $\emptyset$. Then \proofgen{} enters a loop (\autoref{line:dpllstart}-\autoref{line:dpllend}) to search for a satisfying assignment or a proof of unsatisfiability. In the first iteration, \proofgen{} selects the only available activation pattern $\emptyset \in \functiontextformat{ActPatterns}$. 
It calls~\functiontextformat{Deduce} to check the feasibility of the problem based on the current activation pattern. \functiontextformat{Deduce} uses abstraction to approximate that from the input constraints the output values are feasible for the given network. 
Since \functiontextformat{Deduce} cannot decide infeasibility, \proofgen{} randomly selects a neuron to split (\functiontextformat{Decide}). Let us assume that it chooses $v_4$ to split, which essentially means the problem is split into two independent subproblems: one with $v_4$ active and the other with $v_4$ inactive.
\proofgen{} then adds $v_4$ and $\overline{v_4}$ to \functiontextformat{ActPatterns}.

In the second iteration, \proofgen{} has two subproblems (that can be processed in parallel). For the first subproblem with $v_4$, \functiontextformat{Deduce} cannot decide infeasibility, so it selects $v_2$ to split. It then conjoins $v_4$ with $v_2$ and then with $\overline{v_2}$ and adds both conjuncts to \texttt{ActPatterns}. 
For the second subproblem with $\overline{v_4}$ inactive, \functiontextformat{Deduce} determines that the problem is unsatisfiable and \proofgen{} saves the node $v_4$ to the proof tree, as node 3, to indicate one unsatisfiable pattern, i.e., whenever the network has $v_4$ being inactive, the problem is unsatisfiable.

In the third iteration, \proofgen{} has two subproblems for $v_4 \land v_2$ and $v_4 \land \overline{v_2}$. For the first subproblem, \functiontextformat{Deduce} cannot decide infeasibility, so it selects $v_1$ to split. It then conjoins $v_1$ and then $\overline{v_1}$ to the current activation pattern and adds them to \functiontextformat{ActPatterns}. For the second subproblem, \functiontextformat{Deduce} determines that the problem is unsatisfiable and \proofgen{} saves the node $v_4 \land \overline{v_2}$ to the proof tree, as node 5.

In the fourth iteration, \proofgen{} has two subproblems for $v_4 \land v_2 \land v_1$ and $v_4 \land v_2 \land \overline{v_1}$. Both subproblems are determined to be unsatisfiable, and \proofgen{} saves them to the proof tree as nodes 6 and 7, respectively.

Finally, \proofgen{} has an empty \texttt{ActPatterns}, stops the search, and returns \texttt{unsat} and the proof tree. 

\section{Proof Language}\label{sec:prooflang}

\lstdefinelanguage{SMTLIB}{
    morekeywords={assert, declare-const, declare-pwl, or, and, },
    alsoletter={-},
    morecomment=[l];,
    morestring=[b]",
    morekeywords=[2]{Real, Int, ReLU},
    keywordstyle=[2]\color{codepurple},
}

\lstdefinestyle{SMTLIB-style}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{black},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false, 
    breaklines=true, 
    captionpos=b, 
    keepspaces=true, 
    numbers=left, 
    numbersep=5pt, 
    showspaces=false, 
    showstringspaces=false,
    showtabs=false, 
    tabsize=2,
}

\lstset{
    language=SMTLIB,
    style=SMTLIB-style
}

\newcommand{\lra}[1]{
    \textcolor{green!40!black}{\langle} 
    \textit{\textcolor{green!40!black}{#1}} 
    \textcolor{green!40!black}{\rangle}
}

\begin{figure}
{\small
\begin{align*}
    \lra{proof}         &::= \lra{declarations} \ \lra{assertions} \\
    \lra{declarations}  &::= \lra{declaration} \ | \ \lra{declaration} \ \lra{declarations} \\
    \lra{declaration}   &::= (\textbf{declare-const} \ \lra{input-vars} \ \textbf{Real}) \\
                        & \quad ~| \ (\textbf{declare-const} \ \lra{output-vars} \ \textbf{Real}) \\
                        & \quad ~| \ (\textbf{declare-pwl} \ \lra{hidden-vars} \ \lra{activation}) \\
    \lra{input-vars}    &::= \lra{input-var} \ | \ \lra{input-var} \ \lra{input-vars} \\
    \lra{output-vars}    &::= \lra{output-var} \ | \ \lra{output-var} \ \lra{output-vars} \\
    \lra{hidden-vars}    &::= \lra{hidden-var} \ | \ \lra{hidden-var} \ \lra{hidden-vars} \\
    \lra{activation}    &::= ~\text{ReLU} \ | \ \text{Leaky ReLU} \ | \ \ldots \\
    \lra{assertions}    &::= \lra{assertion} \ | \ \lra{assertion} \ \lra{assertions} \\
    \lra{assertion}     &::= (\textbf{assert} \ \lra{formula}) \\
    \lra{formula}       &::= (\lra{operator} \ \lra{term} \ \lra{term}) \\
                        & \quad ~| \ (\textbf{and} \ \lra{formula}+) \ | \ (\textbf{or} \ \lra{formula}+) \\
                        % & \quad ~| \ (\textbf{and} \ \lra{formula} \ \lra{formula}) \\
                        % & \quad ~| \ (\textbf{or} \ \lra{formula}+) \\
                        % & \quad ~| \ (\textbf{or} \ \lra{formula} \ \lra{formula}) \\
    \lra{term}          &::= \lra{input-var} \ | \ \lra{output-var} \\ 
                        & \quad ~| \ \lra{hidden-var} \ | \ \lra{constant} \\
    \lra{operator}      &::= ~ < \ | \ \leq \ | \ > \ | \ \geq \\
    \lra{input-var}     &::= ~\text{X\_}\lra{constant} \\
    \lra{output-var}    &::= ~\text{Y\_}\lra{constant} \\
    \lra{hidden-var}    &::= ~\text{N\_}\lra{constant} \\
    \lra{constant}      &::= ~\textbf{Int} \ | \ \textbf{Real}
\end{align*}
}
\caption{The \prooflang{} proof language.}\label{fig:grammar}
\end{figure}

In~\autoref{sec:proofgen} we have shown that the broad branch-and-bound (BaB) class of DNN verification techniques  can generate a binary tree that represents a proof of unsatisfiability.
Rather than record such proofs in an verifier-specific format, it is more desirable to have a standard format
that is human-readable, is compact, can be efficiently generated by verification tools, and can
be efficiently and independently processed by proof checkers.  

To meet this goal, we introduce \prooflang{}, a proof language to specify DNN proofs.
This language is inspired by the SMTLIB format~\cite{barrett2010smt} used for SMT solving, which has also been adopted by the  VNNLIB language~\cite{vnnlib} to specify DNNs and their properties for  verification.

%accepted by our proof check algorithm (\autoref{sec:proof-checker}

\autoref{fig:grammar} outlines the \prooflang{} syntax and grammar, represented as production rules. 
A proof is composed of \textit{declarations} and \textit{assertions}. Declarations define the variables and their types within the proof. Specifically, \textit{input variables} (prefixed with \functiontextformat{X}) and \textit{output variables} (prefixed with \functiontextformat{Y}) are declared as real numbers, representing the inputs and outputs of the neural network, respectively. Additionally, \textit{hidden variables} are declared with specific piece-wise linear (PWL) activation functions, such as ReLU or Leaky ReLU. These hidden variables correspond to the internal nodes of the neural network that process the input data through various activation functions.

Assertions are logical statements that specify the conditions or properties that must hold within the proof. Assertions over input variables are \emph{preconditions} and those over output variables are \emph{post-conditions}. Each assertion is composed of a \textit{formula}, which can involve terms and logical operators. Formulas include simple comparisons between terms (e.g., less than, greater than) or more complex logical combinations using \functiontextformat{and} and \functiontextformat{or} operators. The terms used in these formulas can be input variables, output variables, hidden variables, or constants.

\begin{figure}
\begin{lstlisting}[style=SMTLIB-style, language=SMTLIB]
; Declare variables
(declare-const X_0 X_1 Real)
(declare-const Y_0 Y_1 Real)
(declare-pwl N_1 N_2 N_3 N_4 ReLU)

; Input constraints
(assert (>= X_0 -2.0))
(assert (<= X_0  2.0))
(assert (>= X_1 -1.0))
(assert (<= X_1  1.0))

; Output constraints
(assert (<= Y_0 Y_1)) 

; Hidden constraints
(assert (or 
    (and (<  N_4 0))
    (and (<  N_2 0) (>= N_4 0))
    (and (>= N_2 0) (>= N_1 0) (>= N_4 0))
    (and (>= N_2 0) (<  N_1 0) (>= N_4 0))))
\end{lstlisting}
\caption{\prooflang{} example format of the proof tree in \autoref{fig:example}b. 
%\tvn{seems to take too much space. Could we put multiple statements in a line?}\hd{is it compact enough?}\tvn{yep. thanks}
}
\label{fig:proof_example}
\end{figure}

%\autoref{fig:proof_example} shows an example of a proof in \prooflang{} using the network in \autoref{fig:example}a. 

The \texttt{declare-*} statements declare input, output, and hidden variables, while the \texttt{assert} statements specify the constraints on these variables (i.e., the pre and postcondition of the desired property).
The hidden constraints represent the activation patterns of the hidden neurons in the network (i.e., the proof tree). Each \texttt{and} statement represents a tree path that represents an activation pattern. 



\subsection{Example} The proof in \autoref{fig:proof_example} corresponds to the proof tree in \autoref{fig:example}b. The statement \texttt{(and (< N\_4 0))} corresponds to the rightmost path of the tree with $\overline{v_4}$ decision (leaf 3).  The statement \texttt{(and (< N\_2 0) (>= N\_4 0))} corresponds to the path with $v_4 \land \overline{v_2}$ (leaf 5). 

The \prooflang{} language is intentionally designed to (a) not explicitly include weights/bias terms to minimize size of the proof structure, and (b) explicitly reflect a DNF structure to enable easy parallelization.
The DNN weight and bias terms are readily available in the standard ONNX~\cite{onnx} format, which is typically used to represent the DNN input to a \proofgen{}-based DNN verification tool and can be accessed by any \prooflang{} checker like the one described next in~\autoref{sec:proofchecking}.


\section{Proof Checker}\label{sec:proofchecking}
Finally, we need to check that the generated proof is correct and that the original NN verification problem is indeed unsatisfiable. The checker must be efficient to handle large proofs and trusted of its results (if it verifies the proof, then the original NNV problem is proved).


To achieve this, we present \proofcheck{}, a proof checker for \prooflang{} proofs.
\proofcheck{} is verifier-independent and support \prooflang{} proofs generated by different verification tools. \proofcheck{} also has several optimizations to handle large proofs efficiently.

\subsection{The Core \proofcheck{} Algorithm}


The goal of \proofcheck{} is to verify that the \prooflang{} tree generated by a DNN verification tool is correct (i.e., the proof tree is a proof of unsatisfiability of the DNN verification problem).
\proofcheck{} thus must verify that the constraint represented by each \emph{leaf} node in the proof tree is unsatisfiable. To check each node, \proofcheck{} forms an MILP problem (\autoref{sec:milp}) consisting of the constraint in \autoref{eq:prob} (the DNN, the input condition, and the negation of the output) with the constraints representing the activation pattern encoded by the tree path to the leaf node. \proofcheck{} then invokes an LP solver to check that the MILP problem is infeasible, which indicates unsatisfiability of the leaf node. 

\begin{algorithm}[t]
    \footnotesize

    \Input{DNN $\mathcal{N}$, property $\phi_{in} \Rightarrow \phi_{out}$, $\prooftree$}
    \Output{\certified if proof is valid, otherwise \uncertified}
    \BlankLine

    \If{$\neg$ \RepOK(\prooftree)}{
        \RaiseError{Invalid proof tree}
    }

    \tcp{initialize MILP model with inputs}
    $\model \leftarrow \CreateStabilizedMILP(\mathcal{N}, \phi_{in}, \phi_{out})$ \label{line:build_model}

    
    $\leaf \gets \mynull$ \tcp{initialize current processing node}
    
    \While{$\prooftree$}{\label{line:proof_loop}
        
        % \tcp{Get $k$ leaf nodes from $\prooftree$}
        % $[\leaf_1, ..., \leaf_k] \gets \GetLeafNodes(\prooftree, k)$ \\ \label{line:get_leaf}
        $\leaf \gets \Select(\prooftree, \leaf)$ \tcp{get next node to check} \label{line:select} \label{line:get_leaf}

        % \tcp{Process $k$ nodes in parallel}
        % \Parfor{$\leaf_i ~\In~ [\leaf_1, ..., \leaf_k]$}{ \label{line:parfor}
        
        $\model \gets \AddConstrs(\model, \leaf)$  \tcp{add constraints} \label{line:add_proof_leaf_constrs}
        
        % $\minimum \gets \Minimize(\model)$ \tcp{} \label{line:optimize1}
        \If{$\Feasible(\model)$}{\label{line:proof_check_objective1}
            \Return{\uncertified} \tcp{cannot certify}
        }
        
        % $\Backtrack(\prooftree, \leaf)$ \tcp{process interior node}
        
        % \While{$\StoppingConditions(\leaf_i)$}{ \label{line:stop_condition}
        %     % \tcp{remove constraints of old $\leaf_i$ }
        %     % $\model \gets \RemoveConstrs(\model, \leaf_i)$  \\
        %     % \tcp{move to $\leaf_i$'s parent}
        %     % $\leaf_i \gets \ShortenSplitConstrs(\leaf_i)$ \\
        %     % \tcp{add constraints of new (looser) $\leaf_i$ }

        %     \tcp{move to $\leaf_i$'s parent}
        %     $\model \gets \Backjump(\leaf_i)$  \\ \label{line:proof_backjump}

        %     \tcp{Prove the new $\leaf_i$}
        %     $\minimum \gets \Minimize(\model)$ \\ \label{line:optimize2}
        %     \If(\tcp*[h]{cannot certify}){$\minimum \le 0$}{
        %         \Break
        %     }
        %     \tcp{if parent is proved, remove its children}
        %     $\prooftree \gets \RemoveLeafNodes(\prooftree, \leaf_i)$ \label{line:proof_prune}
        % }
        }
    % }
    \Return{\certified}

    \caption{\proofcheck{} algorithm.}
    % \Description{}
    \label{fig:algorithm}
\end{algorithm}


\autoref{fig:algorithm} shows a minimal (core)  \proofcheck{} algorithm, which takes as input a DNN $\mathcal{N}$, a property $\phi_{in} \Rightarrow \phi_{out}$, a proof tree $\prooftree$, and returns \certified if the proof tree is valid and \uncertified otherwise. 
\proofcheck{} first checks the validity of the proof tree (\autoref{line:build_model}), i.e., the input must represent a proper \prooflang{} proof tree (\autoref{sec:prooflang}).  
If the proof tree is invalid, \proofcheck{} raises an error.
\proofcheck{} next creates a MILP model (\autoref{line:build_model}) representing the input. % (\autoref{sec:neuron-stabelization}).
\proofcheck{} then enters a loop (\autoref{line:proof_loop}) that selects a (random) leaf node from the proof tree (\autoref{line:select}) and adds its MILP constraint to the model (\autoref{line:add_proof_leaf_constrs}). It then checks the model using an LP solver to determine whether the leaf node is unsatisfiable. If the LP solver returns feasibility, \proofcheck{} returns \uncertified, i.e., it cannot verify the input proof tree. 
\proofcheck{} continues until all leaf nodes are checked and returns \certified, indicating the proof tree is valid.



\subsubsection{Example} For the \prooflang{} proof in \autoref{fig:proof_example}, we need to check that the four leaf nodes 3, 5, 6, and 7 of the proof tree in \autoref{fig:example}b are unsatisfiability. Assume \proofcheck{} first selects node 3, it forms the MILP problem for leaf node 3 by conjoining the constraint representing $0.6v_1 + 0.9v_2 - 0.1 \le 0$ (i.e., $\overline{v_4}$) %\tvn{Hai check}\hd{by setting $a_1^{(2)}=0$ representing that $\hat{z}_1^{(2)} \le 0$ (see \autoref{eq:mip}e) or \emph{implicitly} conjoining the inequality $0.6\hat{z}^{(1)}_0 + 0.9\hat{z}^{(1)}_1 - 0.1 \le 0$ (or ${z}_1^{(2)} \le 0$), where $\hat{z}^{(1)}_0$, $\hat{z}^{(1)}_1$ represent the outputs of $v_1$ and $v_2$, respectively.} 
with the constraints in \autoref{eq:prob} representing the input ranges and the DNN with the objective of optimizing the output. \proofcheck{} then invokes an LP solver, which determines that this MILP is infeasible, i.e., leaf node 3 indeed leads to unsatisfiability. \proofcheck{} continues this process for the other three leaf nodes and returns \certified as all leaf nodes are unsatisfiable.


\subsubsection{MILP Formulation}\label{sec:milp}

\proofcheck{} formulates MILP problems~\cite{tjeng2019evaluating} and check for feasible solutions using off-the-shelf LP solving. Formally, the MILP problem is defined as:
\begin{equation}
    \begin{aligned}
        &\mbox{(a)}\quad z^{(i)} = W^{(i)} \hat{z}^{(i-i)} + b^{(i)};\\
        &\mbox{(b)}\quad y = z^{(L)};  x = \hat{z}^{(0)}; \\
        &\mbox{(c)}\quad \hat{z}_j^{(i)} \ge {z}_j^{(i)}; \hat{z}_j^{(i)} \ge 0; \\
        &\mbox{(d)}\quad a_j^{(i)} \in \{ 0, 1\} ;\\
        &\mbox{(e)}\quad \hat{z}_j^{(i)} \le {a}_j^{(i)} {u}_j^{(i)}; \hat{z}_j^{(i)} \le {z}_j^{(i)} - {l}_j^{(i)}(1 - {a}_j^{(i)}); \\
    \end{aligned}
    \label{eq:mip}
\end{equation}

\noindent where $x$ is input, $y$ is output, and $z^{(i)}$, $\hat{z}^{(i)}$, $W^{(i)}$, and $b^{(i)}$ are the pre-activation, post-activation, weight, and bias vectors for layer $i$, respectively. 
This encodes the semantics of a ReLU-based DNN:  
(a) the affine transformation computing the pre-activation value for a neuron in terms of outputs in the preceding layer;
(b) the inputs and outputs in terms of the adjacent hidden layers;
(c) assertion that post-activation values are non-negative and no less than pre-activation values;
(d) neuron activation status indicator variables that are either 0 or 1; and
(e) constraints on the upper, $u_j^{(i)}$, and lower, $l_j^{(i)}$, bounds of the pre-activation value of the $j$th neuron in the $i$th layer.
Deactivating a neuron, $a_j^{(i)} = 0$, simplifies the first of the (e) constraints to $\hat{z}_j^{(i)} \le 0$, and activating a neuron simplifies the second to $\hat{z}_j^{(i)} \le z_j^{(i)}$, which is consistent with the semantics of $\hat{z}_j^{(i)} = max(z_j^{(i)},0)$.


\subsubsection{Correctness}\label{sec:checker-core-correctness}

 \autoref{fig:algorithm} returns \certified iff the input \prooflang{} proof tree is unsatisfiable. This proof tree encodes a disjunction of constraints, one per tree path, where each constraint represents an activation pattern of the network (the leaf node). The algorithm checks each constraint using LP solving and only returns certified iff every one of them is unsatisfiable.  
%Moreover, while pruning optimization (\autoref{sec:pruning}) in \autoref{fig:algorithm} allows the checker to skip children nodes if the parent node is unsatisfiable, this is still sound because the structure of the proof tree guarantees that the constraints of a child is more restricted its parent, and thus if the parent node is unsatisfiable, the children nodes must also be unsatisfiable. The algorithm  terminates because the proof tree (or the DNF) is finite and the checker will eventually check all nodes.
We note that this correctness argument assumes that the LP solver is correct -- in practice
multiple solvers could be used to guard against errors in that component.  We note that
it is standard for proof checkers to assume the correctness of a small set of external tools, e.g., checkers that use  theorem provers assume the correctness of the underlying prover~\cite{lammich2023grat}.

\subsection{Optimizations}
While the core \proofcheck{} algorithm in \autoref{fig:algorithm} is minimal, it can be inefficient. \proofcheck{} employs several optimizations to improve its efficiency. These are crucial for checking large proof trees generated by DNN verification tools for challenging problems.


\subsubsection{Neuron Stabilization}\label{sec:neuron-stabelization} 

\begin{algorithm}[t]
    \footnotesize

    \Input{DNN $\mathcal{N}$, property $\phi_{in} \Rightarrow \phi_{out}$, parallel factor $k$}
    \Output{MILP $\model$}
    \BlankLine
    

    $\model \leftarrow \InputMILP(\phi_{in})$  \tcp{input property}    \label{line:create_input}

    \tcp{Add MILP constraints for each layer of network}
    \For{$\layer ~\In ~\mathcal{N}$}{
        \If{$\isPiecewiseLinear(\layer)$}{
            \tcp{add constraints \autoref{eq:mip} (c), (d), (e)} 
            $\model \leftarrow \PiecewiseLinearMILP(\layer, \phi_{in}, \phi_{out})$ \\ \label{line:create_pwl_layer}
        }
        \Else(\tcp*[h]{this layer is linear}){
            \tcp{add constraints \autoref{eq:mip} (a), (b)} 
            $\model \leftarrow \LinearMILP(\layer, \phi_{in}, \phi_{out})$ \\ \label{line:create_linear_layer}

            \tcp{estimate upper and lower bounds} 
            $\layerbounds \gets \EstimateBounds(\layer)$ \\ \label{line:estimate_bounds}
            
            \tcp{select unstable neurons to be stabilized} 
            $[v_1, ..., v_k] \leftarrow \GetUnstableNeurons(\layerbounds)$ \\ \label{line:find_unstable}
        
            \tcp{stabilize selected neurons in parallel}
            \Parfor{$v_i ~\In~[v_1, ..., v_k]$}{ \label{line:parallel_stabilize}
                % \tcp{lower is closer to 0 than upper}
                \tcp{optimize lower first}
                \If{$(v_i.lower + v_i.upper) \ge 0$}{
                    $\Maximize(\model, v_i.lower)$ \\ \label{line:maximize1}
                    \If(\tcp*[h]{still unstable}){$v_i.lower < 0$}{
                        $\Minimize(\model, v_i.upper)$ \label{line:minimize1}
                    }
                }
                % \tcp{upper is closer to 0 than lower}
                % \tcp{optimize upper first}
                \Else(\tcp*[h]{optimize upper first}){
                    $\Minimize(\model, v_i.upper)$ \\ \label{line:minimize2}
                    \If(\tcp*[h]{still unstable}){$v_i.upper > 0$}{
                        $\Maximize(\model, v_i.lower)$  \label{line:maximize2}
                    }
                }
            }
        }
    }

    $\model \gets \AddObjective(\model, \phi_{out})$ \tcp{output property}
    
    \Return{$\model$}
    
    % \vspace*{-0.2in}
    \caption{\texttt{CreateStabilizedMILP} procedure.}
    \label{fig:stabilize}
    % \Description{}
\end{algorithm}

A primary challenge in DNN analysis is the presence of large numbers of piecewise-linear constraints (e.g., ReLU) which generate a large number of branches and yield large proof trees. In the MILP formulation, this creates many disjunctions which are hard to solve. To reduce the number of disjunctions, \proofcheck{} uses \emph{neuron stabilization}~\cite{duong2024harnessing}
to determine neurons that are \emph{stable}, either active or inactive, for all inputs defined by the property pre-condition.
For all stable neurons, the disjunctive ReLU constraint is replaced with a linear constraint that represents the neuron's value.   This simplifies the MILP problem.

\proofcheck{} uses the algorithm in \autoref{fig:stabilize} to traverse the DNN and compute stable neurons. The algorithm initializes the MILP model with input constraints (\autoref{line:create_input}) and then iterates over each layer of the network. 
%follows the~\autoref{eq:mip} to create corresponding 
Next, for each layer, it creates constraints (\autoref{line:create_pwl_layer} or \autoref{line:create_linear_layer}) depending on the layer type.
Moreover,  it uses approximation to estimate bounds of neuron values to determine neuron stability (\autoref{line:estimate_bounds}).
Next, it filters unstable neurons (\autoref{line:find_unstable}) and attempt to make them stable by optimizing either their lower ($\Maximize$) or upper ($\Minimize$) bounds.



\subsubsection{Pruning Leaf Nodes}\label{sec:pruning}
%\hd{``Pruning'' sounds better}

Another optimization \proofcheck{} employs is that it does not check child nodes if the parent node is unsatisfiable.
In an \prooflang{} proof tree, a child node adds constraints to the parent (e.g., node 6 adds the constraint of $v_1$ to node 4, which adds the constraint of $v_2$ to node 2 in \autoref{fig:example}b). Thus, if we determine that the constraint of the parent is unsatisfiable, we can skip the child nodes, which must also be unsatisfiable.  

%Simply checking parent nodes would actually increase MILP cost, because they have shorter tree paths and thus have a greater number of disjunctive constraints.

\proofcheck{} uses a backtracking mechanism to check the parent node only when the child nodes are infeasible. Specifically, it starts checking a leaf node $l$. If it determines unsatisfiability it will check the parent $p$ of $l$. If $p$ is unsatisfiable it immediately removes the children of $p$ (more specifically the sibling of $l$). Next it backtracks to the parent of $p$ and repeats until meeting a stopping criteria. This optimization reduces the number of LP problems that need to be solved, making the proof checking process more efficient. 

We implement a backjumping strategy that allows for backtracking multiple levels, $N$, rather than a single level at a time.
A large value of $N$ offers the chance for greater pruning if an unsatisfiable node is found by
backjumping, but such nodes also represent less constrained, and therefore, more complex MILP problems
and are less likely to be unsatisfiable.
The default value in $\proofcheck{}$ is $N=2$ is selected to enable a modest degree of pruning,
while being close enough to a proven unsatisfiable node that it has a reasonable chance of itself being unsatisfiable.
Future work will explore tuning $N$ to a given verification problem.


% \hd{Without X, \proofcheck{} will have to check all the children.}

% \hd{It backjumps when the leave node is proved and X optimization is enabled. 
% We can keep backing up until it cannot prove anymore, however, after N times, each time half the length of an activation pattern, an activation pattern is shortened to $1 / 2^{N}$ (e.g., if $N=2$, after 2 times of backing up, an activation pattern is shortened to 1/4 of its length). 
% This is when the pruned leaves by those proved interiors are getting overlapped (due to parallelization, we backjump on multiple leaves), so more backjumps might not helpful. 
% Instead, we start over with a new iteration with remaining leaves.
% I experienced with $N = \{1, 2, 3, 4, 5\}$ and I settled down $N=2$ for all experiments (fixed N) which seems to be good enough in our experiments. 
% Increasing $N$ does not improve the performances but hurts especially when dealing with CNNs.
% }
% \matt{I think my main point is ``How much of this is important to \proofcheck{}?'' whatever is important needs to be described.  You don't need to describe all of the discarded alternatives.  If we settled on a particular strategy, then define that strategy and say why it works well, e.g., balances cost of failing to prove unsat for parents with ability to prune sub-trees.  Right now we just say "simple backtracking mechanism", but your description makes seem like it is not all that simple.}
% This optimization reduces the number of LP problems that need to be solved, making the proof checking process more efficient.

%. \proofcheck{} thus uses a simple backtracking mechanism to check the parent node only when the child nodes are infeasible. This optimization reduces the number of LP problems that need to be solved, making the proof checking process more efficient.
%However, checking the parent node is more expensive than checking the child nodes (otherwise we can just directly check the root node). \proofcheck{} thus uses a simple backtracking mechanism to check the parent node only when the child nodes are infeasible. This optimization reduces the number of LP problems that need to be solved, making the proof checking process more efficient.

\subsubsection{Parallelization} Finally, the structure of \prooflang{} proof tree is designed to be easily parallelized.  Each tree path is an independent sub-proof and partitions of the tree allow checker to leverage multiprocessing to check large proof trees efficiently. \proofcheck{} uses a parameter $k$ to control the number of leaf nodes to be checked in parallel.


\subsubsection{Correctness} These three optimizations preserve the correctness of the \proofcheck{} algorithm (\autoref{sec:checker-core-correctness}). First, neuron stabilization just simplifies the MILP problem and removes irrelevant, i.e., stable, neurons, which by definition are not affected by activation patterns. Second, while backtracking allows the checker to skip some nodes, this is sound because the structure of the proof tree guarantees that the constraints of a child is more restrictive than its parent, and thus if the parent is unsatisfiable, the children must also be. Finally, parallelization is sound because each tree path is independent and the checker will eventually check all nodes.

%\subsection{The \proofcheck{} Algorithm and Implementation}


\section{Evaluation}\label{sec:evaluation}  
Our goals are to understand how checking of \prooflang{}s performs, how it can be optimized, and how robust checking is to
verification optimizations.
We focus our evaluation on the following research questions:

\noindent\mbox{~~}\textbf{RQ1} (\autoref{sec:rq1}): How does \proofcheck{} perform and how does it compare prior work? 

\noindent\mbox{~~}\textbf{RQ2} (\autoref{sec:rq2}): How does proof checking optimization improve performance?

\noindent\mbox{~~}\textbf{RQ3} (\autoref{sec:rq3}): How does proof checking performance vary with verification algorithms and optimizations?



\subsection{Experimental Design}\label{sec:exp_design}
We describe the selection of benchmarks, baselines, metrics, and treatments used
to explore the above RQs.

\ignore{
\begin{table*}[t]
    \footnotesize
    \centering
    \caption{Benchmark instances. U: \texttt{unsat}, S: \texttt{sat}, ?: \texttt{unknown}.}\label{tab:benchmarks}
    % \vspace*{-3mm}
    % \resizebox{\textwidth}{!}{
    \begin{tabular}{c|ccrr|cc}
        \toprule
        % \multirow{2}{*}{\textbf{Benchmarks}} &\multicolumn{3}{c}{\textbf{Networks}}  &\multicolumn{2}{|c}{\textbf{Tasks}}\\
        % \multirow{2}{*}{\textbf{Benchmarks}} &\multicolumn{2}{c}{\textbf{Networks}} &  \multicolumn{2}{c|}{\textbf{Per Network}} &\multicolumn{2}{c}{\textbf{Tasks}}\\
        \multirow{2}{*}{\textbf{Benchmarks}} &\multicolumn{4}{c}{\textbf{Networks}} & \multicolumn{2}{|c}{\textbf{Tasks}}\\
        & Type & Networks & Neurons & Parameters & Properties & Instances (U/S/?)\\
  
        \midrule
        \multirow{4}{*}{FNN\_SMALL}         & \multirow{4}{*}{FNN + ReLU} & 2 x FNN(32) &   64 & 27K & 25 & 25/0/0 \\
                                            &                             & 4 x FNN(32) &  128 & 29K & 25 & 25/0/0 \\
                                            &                             & 2 x FNN(64) &  128 & 55K & 25 & 25/0/0 \\
                                            &                             & 4 x FNN(64) &  256 & 63K & 25 & 25/0/0 \\
        
        \midrule
        \multirow{4}{*}{FNN\_MEDIUM}        & \multirow{4}{*}{FNN + ReLU} & 2 x FNN(256)  &    512 &  269K & 25 & 25/0/0 \\
                                            &                             & 4 x FNN(256)  &   1024 &  401K & 25 & 25/0/0 \\
                                            &                             & 6 x FNN(256)  &   1536 &  532K & 25 & 25/0/0 \\
                                            &                             & 6 x FNN(512)  &   3072 & 1.7M & 25 & 25/0/0 \\
        \midrule
        \multirow{4}{*}{CNN\_SMALL}         & \multirow{4}{*}{CNN + ReLU} & 1 x CNN(3, 21), 1 x FNN(128)  &   320 & 27K & 25 & 25/0/0 \\
                                            &                             & 1 x CNN(3, 19), 1 x FNN(128)  &   428 & 41K & 25 & 25/0/0 \\
                                            &                             & 1 x CNN(3, 17), 1 x FNN(128)  &   560 & 58K & 25 & 25/0/0 \\
                                            &                             & 1 x CNN(3, 13), 1 x FNN(128)  &   896 & 100K & 25 & 25/0/0 \\
        \midrule
        \multirow{4}{*}{CNN\_MEDIUM}        & \multirow{4}{*}{CNN + ReLU} & 1 x CNN(3, 9), 1 x CNN(5, 11), 1 x FNN(128)  &   1828 & 67K & 25 & 25/0/0 \\
                                            &                             & 1 x CNN(3, 7), 1 x CNN(5, 9),  1 x FNN(128)  &   2560 & 128K & 25 & 25/0/0 \\
                                            &                             & 1 x CNN(5, 9), 1 x CNN(7, 11), 1 x FNN(128)  &   2828 & 96K & 25 & 25/0/0 \\
                                            &                             & 1 x CNN(5, 7), 1 x CNN(7, 9),  1 x FNN(128)  &   3920 & 180K & 25 & 25/0/0 \\
        \midrule
        \textbf{Total}                      &                             & \textbf{16}                  &     &   & \textbf{400} & \textbf{400/0/0} \\
        \bottomrule
    \end{tabular}
    % }
\end{table*}
}


\paragraph{Benchmarks}
Evaluating these research questions requires neural network verification problems
that are valid, i.e., their formulation as satisfiability problems is UNSAT.

The yearly VNN-COMP competitions~\cite{brix2023first,brix2023fourth,bak2021second} includes a wide-variety of benchmarks,
but the competitions organizers observed that many benchmarks are too easy.
They contain large numbers SAT problems that can be solved by adversarial attacks 
or UNSAT problems that can be solved, without any state splitting, using coarse overapproximations.
They conclude that there is a need 
for benchmarks to be ``not so easy that every tool can solve all of them''~\cite{brix2023first}.
For this reason, we base our benchmark selection on a recent paper~\cite{duong2024harnessing} that developed harder
UNSAT benchmarks including:
ACAS Xu, RESNET\_A/B, CIFAR2020, MNISTFC, and MNIST\_GDVB.

We selected problems from these based on the applicability of our
current \proofcheck{} implementations.

We chose not to use ACAS Xu, because those problems have a very small
input dimension (5 dimensions) which triggers a divide-and-conquer problem
decomposition optimization in verifiers, like \neuralsat{} and \crown{}.
This results in a set of relatively small proofs, each of which
can be expressed in \prooflang{}, but whose simplicity do not add
value beyond the selected benchmarks discussed below.

We chose not to use the RESNET benchmarks because  \proofcheck{} does not currently handle residual blocks.  This is an engineering limitation and there is no fundamental reason the checking algorithm is not applicable.  

Of the CIFAR2020 networks, one contains BatchNorm layers which are not currently
supported by \proofcheck{}, but we note that adding this support is a straightforward
engineering issue.
The remaining CIFAR2020 benchmarks contain very large numbers of convolutional
kernels, which generates complex constraints that can be challenging to solve.
We adapt the smallest of the CIFAR2020 benchmarks, by varying the size of the
convolutional kernels and the number of CNN layers, to produce 
the CNN benchmark in the second row of \autoref{tab:benchmarks}.
\begin{table}[t]
    \footnotesize
    \centering
    \caption{Benchmarks consist of a 8 neural networks comprised of varying numbers of CNN (C) and FNN (F) layers, neurons, and parameters, each paired with 25 properties to form verification instances that have been confirmed to be UNSAT.}\label{tab:benchmarks}
    % \vspace*{-3mm}
    % \resizebox{\textwidth}{!}{
    \begin{tabular}{c|cccc|c}
        \toprule
        \multirow{2}{*}{\textbf{Name}} &\multicolumn{4}{c|}{\textbf{Networks}} & \textbf{Properties}\\
        & Num. & Layers & Neurons & Param. & Num. \\

        \midrule
        FNN        & 8 & 2-6F &  64-3072 & 27K-1.7M & 200  \\
        \midrule
        CNN        & 8 & 1-2C;1F  &  320-3920 & 41K-180K & 200  \\
        \bottomrule
    \end{tabular}
    % }
\end{table}

The MNISTFC and MNIST\_GDVB benchmarks contain 41 networks and a total of 106 UNSAT verification problems.
From those we selected 8 networks with varying numbers of layers and neurons, resulting
in models spanning several orders of magnitude in terms of parameters, to produce
the FNN benchmark in the first row of \autoref{tab:benchmarks}.

For each of the networks, following the approach from~\cite{duong2024harnessing}, 
we randomly selected centerpoints and radii for local-robustness properties until we found 25 property specifications that yielded UNSAT results.  This yielded 200 properties
each for the FNN and CNN benchmarks for a total of 400 verification problems and
their associated proofs.

\paragraph{Baselines}
To the best of our knowledge there is a single prior work published
on DNN verification proof checking~\cite{desmartin2023towards}.
That checker is designed to work only with a single verifier, \marabou{}, 
unlike \proofcheck{}.

While not strictly a baseline we also use the time to verify problems
as a baseline.  This is because it is instructive to understand 
the cost of proof checking after having generated a proof.
We have adapted two verifiers: \crown{} and \neuralsat{}, to 
generate \prooflang{} proofs for this study.

For RQ1 our focus is on proof checking performance, so we compare
\marabou{} and its proof checker with a single verifier, \neuralsat{},
and \proofcheck{}.  
RQ2 explores \proofcheck{} optimizations and it uses the unoptimized
\proofcheck{} performance as a baseline.
For RQ3 our goal is to understand how well \proofcheck{} accommodates
proofs generated by different verification algorithm variants, since no
other proof checker can do this there is no baseline for comparison.

\begin{figure}[t]
    \centering
    \begin{minipage}[t]{0.4\textwidth}
        \centering  
        \includegraphics[width=\linewidth]{figure/fnn.pdf}
    \end{minipage}
    \begin{minipage}[t]{0.4\textwidth}
        \centering  
        \includegraphics[width=\linewidth]{figure/cnn.pdf}
    \end{minipage}

    % \begin{minipage}[t]{0.45\textwidth}
    %     \centering  
    %     \includegraphics[width=\linewidth]{figure/MNIST_CNN_SMALL.pdf}
    % \end{minipage}
    % \hfill
    % \begin{minipage}[t]{0.45\textwidth}
    %     \centering  
    %     \includegraphics[width=\linewidth]{figure/MNIST_CNN_MEDIUM.pdf}
    % \end{minipage}
    \caption{Cactus-plots for verifiers and proof checkers of FNN (top) and CNN (bottom) benchmarks.}
    \label{fig:cactus-plots}
\end{figure}
\paragraph{Treatments}
RQ1 compares the best performing versions of \marabou{}'s proof checker and \proofcheck{}.
For RQ2, we consider two of the three optimizations
implemented in \proofcheck{} : proof tree pruning (X), and 
proof stabilization (S).  We kept a third optimization that controls the
degree of parallelization in proof checking fixed at a value of 64 to mitigate
experimental cost; since the independence of sub-proofs means that
proof checking is amenable to linear speedup we felt this aspect of experimentation
was less valuable.
For RQ3, we use both \neuralsat{} and \crown{} to generate proofs; this
constitutes a treatment for this research question as it varies the
verification algorithm.
For each verifier, we explore a base version of the verifier and an optimization:
the stabilization optimization for \neuralsat{} and replacing
the default branch-and-bound decision heuristic with the BaBSR~\cite{bunel2020branch} heuristic
in \crown{}.

\paragraph{Metrics}
In the verification community there are two metrics commonly used to
assess performance: time to solve the problems and number of problems solved
from a benchmark.  We report them both here.

For each \proofcheck{} problem we record:
the time to verify that the problem is UNSAT, the time to generate a proof,
the time for \proofcheck{} to finish.
If the sum of these for a problem is less than a specified timeout,
1000 seconds in our evaluation, then we say the problem is ``solved''.
For verifiers run alone, a problem is solved if the verification completes
within the timeout.

For each benchmark, we provide cactus plots which plot the time for a problem on the y-axis, and the number of problems solved on the x-axis; problems are sorted on the x-axis from least to largest.
As shown in~\autoref{fig:cactus-plots}, these plots allow one to observe both the time difference between baselines and treatments (vertical distance between lines at a point on the x-axis) and the ability of techniques to solve problems (the maximum x-coordinate for a given line).

We also report the size of proof trees that are generated in \prooflang{}.
In the absence of optimizations this defines the \textit{number of sub-proofs}
that need to be checked, but with optimizations the number of sub-proofs may
be reduced, e.g., when an interior node in the tree can be proven.
The complexity of  sub-proofs
may vary significantly, so to provide a more detailed characterization we
also report \textit{MILP complexity}.  \autoref{eq:mip} defines the general form
of each MILP problem, but the problems will vary based on how many of the $a$ variables
defined in \autoref{eq:mip}(d) have a fixed value -- either 0 or 1.   When this
happens the constraint in \autoref{eq:mip}(e) are simplified.
Consequently, we measure MILP complexity as the number of neurons that do \textit{not}
have a fixed value, i.e., the number of unstable neurons.  
\ignore{This does not account for the contribution of \autoref{eq:mip}a,b which is directly
related to network size and input/output dimension.   Is there any way to measure that?}
% \hd{I see. It seems hard since "a"s and constraints do not have the same unit.}

\paragraph{Experimental Setup}
All experiments were run on a Linux machine with an AMD Threadripper 64-core 4.2GHZ CPU, 128GB RAM, and an NVIDIA GeForce RTX 4090 GPU with 24 GB VRAM. 
We used a timeout of 1000 seconds for the combined time of running the verifier,
generating the proof, and then checking that proof.

\subsection{RQ1 : Proof Checking Performance}
\label{sec:rq1}
\autoref{fig:cactus-plots} presents data on the performance
of \proofcheck{} relative to both an underlying verifier, \neuralsat{}, and prior work on neural network proof checking, \marabou{}'s proof checker.  
In cactus plots like this, lines that extend further on the x-axis
are better -- more problems solved -- and lines that are lower are better -- faster solve times.
Another way to view these is to pick a point on the x-axis where the plots for two techniques are defined and think of the areas under the two curves as the ``total cost'' to solve that number of problems.
The dashed lines in the plots show the performance of the verifier and the solid lines show the performance of the verifier, proof generation, and the proof checker.  Several configurations of
\proofcheck{} are shown, but in this RQ we draw the readers attention
to the plots for the \proofcheck{}(S+X) configurations; the rest are discussed in detail below.

The cactus plot for the FNN benchmark (top)
shows that \marabou{} and its checker are able to solve 69 problems or 35\% of the benchmark, whereas \proofcheck{} can solve 186 or 93\%.  
For the CNN benchmark (bottom) \marabou{} and its checker can solve a single benchmark, whereas \proofcheck{} can solve 177 problems or 89\%.
In total, \proofcheck{} solved 363 problems or 91\%, whereas \marabou{}  solved 70 problems or 18\% of all instances.

\begin{figure*}[t]
\begin{subfigure}{0.4\linewidth}
    \centering
    % 
    \begin{minipage}[t]{0.75\textwidth}
        \centering  
        \includegraphics[width=\linewidth]{figure/SUB_PROOFS_NONE.pdf}
    \end{minipage}
    %
    \begin{minipage}[t]{0.235\textwidth}
        \centering  
        \includegraphics[width=\linewidth]{figure/SUB_PROOFS_SX.pdf}
    \end{minipage}
    \caption{Number of sub-proofs per problem with (right) and without (left) \proofcheck{} optimizations.}
    \label{fig:sub-proofs-plots}
\end{subfigure}
\hfill
\begin{subfigure}{0.59\linewidth}
    \centering 
    %
    \begin{minipage}[t]{0.5\textwidth}
        \centering  
        \includegraphics[width=\linewidth]{figure/MILP_COMPLEXITY_NONE.pdf}
    \end{minipage}%
    \begin{minipage}[t]{0.5\textwidth}
        \centering  
        \includegraphics[width=\linewidth]{figure/MILP_COMPLEXITY_SX.pdf}
    \end{minipage}

    \caption{Number of constraints of a given complexity per problem with (right) and without (left) \proofcheck{} optimizations.}
    \label{fig:constrs-proofs-plots}
\end{subfigure}
\caption{Data on proof size and complexity.   Y-axes are log-scale due to the range of values.}
\end{figure*}

% \matt{Some of the line number references are no longer resolving below.}
The shape of these cactus plots indicates a high-degree of variability in the cost of proof
checking relative to verification.
From~\autoref{fig:algorithm} it is clear that both the number of leaves in the tree
structure,~\autoref{line:get_leaf}, and the complexity of the model to be checked,~\autoref{line:proof_check_objective1}, are factors that contribute to the cost of proof checking.
To explore those factors we plot their variation across the benchmarks when running \proofcheck{}.

\autoref{fig:sub-proofs-plots} (left) plots a histogram of the number of sub-proofs solved per verification 
problem, i.e., the number of nodes of the proof tree.
When interpreting these plots, understand that the y-axis log scale means that vertical
distances have a different meaning as you move upward in the plot.
While the vast majority of the verification problems have proof trees of fewer then 2000 leaves, but 17 of them have larger trees up to a maximum of more than 10000 leaves.
Note also that even among the smaller sized proof trees, there are some problems that cannot be solved.
This is due to complexity of solving the MILP constraints at the leaves of those proof trees.

\autoref{fig:constrs-proofs-plots} (top) plots a histogram of the number of occurrences of MILP problems of
a given complexity across the benchmarks.  Here again we see a spread in data, but unlike with the number of sub-proofs the CNN benchmarks seem to have consistently larger constraints and there is a clear bias among the unsolved problems towards larger constraint size.
To optimize proof checking, we must address both of these sources of complexity.

\begin{tcolorbox}[left=1pt,right=1pt,top=1pt,bottom=1pt]
\textbf{RQ1 Findings}: Proof checking performance varies with both the size of the proof tree and the complexity of the MILP problems at the nodes of the tree.  \proofcheck{} can solve 91\% of the problems across the benchmarks and improves on prior work which can solve less than 18\%.
% \tvn{are these numbers correct?  the 2nd paragraph of RQ1 says prior work can do 24\% and \proofcheck{} can do 92\%}
\end{tcolorbox}

\subsection{RQ2 : Proof Checking Optimizations}
\label{sec:rq2}
The performance cactus plots~\autoref{fig:cactus-plots} present an ablation
study of the 
pruning (X) and stabilization (S) optimizations of \proofcheck{}.
The trend across both benchmarks is consistent with pruning (yellow) and stabilization (blue)
 improving the number of problems solved by 5\% and 36\%, respectively, over the unoptimized
\proofcheck{} (green).
The combination of optimizations (red) improves the number of solved problems by 46\%, which is more than the sum of their individual improvements demonstrating that the methods create opportunities for one another for further optimization.
\ignore{
- neither X nor S: 168 (F) + 88 (C) = 256
- X: 175 (F) + 93 (C) = 268
- S: 178 (F) + 170 (C) = 348
}

The \autoref{fig:sub-proofs-plots} (right) and \autoref{fig:constrs-proofs-plots} (right) explore the impact of the S and X optimizations on the number of sub-proofs and MILP complexity.  Across the benchmarks optimizations reduce 
the number of sub-proofs is to less than 1000 and
MILP complexity to less than 2000.   
The reduction in sub-proofs directly contributes to the increase
in performance of \proofcheck{}, but the reduction in
MILP complexity is more subtle.   
Integer programming, and thus MILP, is known to be
NP-Hard in general~\cite{garey1979computers}.
The stabilization optimization addresses this complexity by
calculating sets of variables that are forced to take on specific
values based on other constraints in the MILP problem.  For each
such variable, the constraints associated with it is effectively
eliminated.  We can observe this in comparing the left and
right of \autoref{fig:constrs-proofs-plots} where we see both
constraints of higher complexity eliminated and the peak of
the constraint distribution shifted downward from 400 to 100
constraints.  

\ignore{
Sub-problem size
- without S+X:
    + mean: 388
    + std: 1062
- with S+X
    + mean: 142
    + std: 152

MILP complexity
- without S+X: 
    + mean: 329
    + std: 274
- with S+X
    + mean: 493
    + std 449

We performed an analysis of the relationship between size of constraints and number of sub-proofs and determined that these factors are not strongly correlated.
For example, there is a 1 layer CNN model with 27k parameters whose verification
generates 680 sub-proofs where the complexity of the constraints in that proof are at most 109. 
On the other end of the spectrum, verification of a 2 layer CNN model with 180k parameters only requires 81 sub-proofs, but those proofs consist of constraints with complexity of at least 1721.
}

\begin{tcolorbox}[left=1pt,right=1pt,top=1pt,bottom=1pt]
\textbf{RQ2 Findings}: The \proofcheck{} optimizations each independently increase the number of proofs that can be checked and in combination they allow
an additional 46\% of the proofs in the benchmarks to be checked.
\end{tcolorbox}

\subsection{RQ3 : Proof Checking and Verifier Optimizations}
\label{sec:rq3}
\autoref{fig:algvariation} shows cactus-plots for two configurations of \neuralsat{} and \crown{} generated proofs across the benchmarks.   The performance of the verifiers, dashed lines, differ across configurations and they are able to 
verify between 337 and 400 problems in the benchmark.
\ignore{Number of Verified/Proved problems
    + abcrown(babsr):   337/335 = 99.4%
    + abcrown:          368/366 = 99.4%
    + neuralsat(SX):    387/363 = 93.7%
    + neuralsat(S)(SX): 400/384 = 96%
}
For both of the verifiers and configurations,
\proofcheck{} is able to check between 93.7\% and 99.4\% of the proofs that are generated.
This demonstrates that the \prooflang{} is able to encode proofs
generated by differing neural network verification algorithms, and
that \proofcheck{} can check them.

% \autoref{tab:sizestats}.
\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{figure/ablation.pdf}
    \caption{\proofcheck{} performance with different verification algorithms.}
    \label{fig:algvariation}
\end{figure}

\begin{table}[t]
    \caption{Proof statistics for best verifier configurations.}
    \label{tab:sizestats}
    \centering
    \begin{tabular}{c|cc|cc}
        \toprule
         \multirow{2}{*}{\textbf{Verifier}} & \multicolumn{2}{c}{\textbf{Num. Sub-Proofs}} & \multicolumn{2}{|c}{\textbf{MILP Complexity}}\\
         & Mean & Median & Mean & Median \\ 
         \midrule
         \neuralsat{}(S) & 95 & 36 & 601 & 545 \\ 
         \midrule
         \crown{} & 230 & 180 & 414 & 179\\
         \bottomrule
    \end{tabular}
\end{table}

We performed an analysis of both the number of sub-proofs and MILP complexity for the proofs generated by the two best performing verifier
configurations.  These values follow a skewed
distribution, so we report the mean and median values in \autoref{tab:sizestats}.   One can observe variation in the
structure of the proofs generated by these verifiers.
\neuralsat{} generates smaller proof trees, but where the
MILP problems are more complex.
In contrast, \crown{} generates significantly larger proof trees,
but with less complex MILP problems.
This variation suggests potential avenues for future work, especially,
when proof checking is important.

For example, \neuralsat{} might include an option to generate larger proof trees, but with smaller MILP problems.  Such proofs would then
be amenable to higher-degrees of parallel solving and mitigate the 
performance bottleneck presented by MILP solver implementations.
One might even consider strategies that use fast verification 
options during development and then when all properties are proven, shift to slower verification options that are more amenable to proof checking.

\ignore{
1. Longer verification time means proof tree will be larger since verifier has to explore larger space
2. If an instance is verified by \crown{}, it will likely be proved by \proofcheck{}. In other words, most of timeout instances are due to \crown{} cannot verify (1st phase).

- MILP complexity:
    + neuralsat(S)(SX)  : mean=601.36, std=475.88, median=545.00, min=3, max=1944
    + neuralsat(SX)     : mean=492.40, std=448.81, median=342.00, min=2, max=1956
    + abcrown           : mean=414.35, std=447.02, median=179.00, min=0, max=1957
    + abcrown(babsr)    : mean=409.00, std=448.89, median=166.00, min=0, max=1952

They share the same MILP base model since their base models are all generated from SX setting.
neuralsat(S)(SX) has larger MILP complexity in average means that its proof trees are often shallower than others. In other words, neuralsat(S) explores smaller space than others -- which is true.

- number of sub proofs:
    + neuralsat(S)(SX): mean=95.11,   std=117.13, median=36.00,  min=0, max=829
    + neuralsat(SX)   : mean=142.00,  std=151.72, median=106.00, min=1, max=1132
    + abcrown         : mean=229.96,  std=199.23, median=180.00, min=0, max=1152
    + abcrown(babsr)  : mean=228.10,  std=170.88, median=188.00, min=0, max=857

- Number of leaves:
    + neuralsat(S)(SX): mean=236.59,   std=447.80,   median=37.50,  min=0, max=3627
    + neuralsat(SX)   : mean=902.25,   std=4957.22,  median=219.00, min=6, max=91318
    + abcrown         : mean=3007.42,  std=12742.09, median=561.00, min=0, max=138918
    + abcrown(babsr)  : mean=11714.62, std=41909.72, median=781.50, min=0, max=509765
}


\begin{tcolorbox}[left=1pt,right=1pt,top=1pt,bottom=1pt]
\textbf{RQ3 Findings}: \prooflang{} is robust to variation in different verification algorithms and \proofcheck{} is applicable to any such proof and effective in checking
the vast majority of those arising from the benchmarks.
\end{tcolorbox}


\chapter{Benchmarks Generation}\label{chap:benchmarks-generation}

\chapter{Conclusion}

\appendix
\chapter{Logics and Linear Programming}\label{app:logics}

\section{Logics and Satisfiability}\label{app:logics}
\section{Linear Programming}\label{app:lp}

\tvn{Hai, give some LP background here.}

\bibliographystyle{abbrv}
\bibliography{book.bib}

\appendix

