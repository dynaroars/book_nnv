\documentclass[oneside,11pt,dvipsnames]{book}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[margin=1.5in]{geometry}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{soul}
% \usepackage[small,compact]{titlesec} %very powerful
\usepackage[most]{tcolorbox}
% \setsecnumdepth{subsection}
% \setcounter{tocdepth}{3}
\usepackage{multirow,multicol}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{epigraph}
\usepackage{cite}
\usepackage{caption}
\captionsetup{font=small}
\usepackage{graphicx}
\usepackage{pdfpages}
\usepackage{hyperref}
\usepackage{wrapfig}
\usepackage{subcaption}
\setlength\intextsep{0pt} % remove extra space above and below in-line float
\usepackage{hyperref}
\hypersetup{
  colorlinks,
  citecolor=black,
  filecolor=black,
  linkcolor=blue,
  urlcolor=blue,
}
\usepackage{booktabs}

\usepackage{tikz}
\usetikzlibrary{calc}
\usepackage{xcolor}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.97,0.97,0.97}

\usepackage{anyfontsize}
\usepackage{sectsty}

\usepackage[makeroom]{cancel}

\usepackage[linesnumbered,ruled,vlined]{algorithm2e}

\SetKwFor{Parfor}{parfor}{do}{endparfor}

\SetKwInOut{Input}{input}
\SetKwInOut{Output}{output}

\SetKw{Break}{break}
\SetKw{Continue}{continue}
\SetKw{In}{in}
\SetKwData{certified}{certified}
\SetKwData{prooftree}{proof}
\SetKwData{uncertified}{uncertified}
\SetKwData{model}{model}
\SetKwData{leaf}{node}
\SetKwData{mynull}{null}
\SetKwData{layer}{layer}
\SetKwData{layer}{layer}
\SetKwData{layerbounds}{layer\_bounds}
\DontPrintSemicolon





\SetAlgoCaptionSeparator{.}
\SetAlCapFnt{\footnotesize\rm}
\SetAlCapNameFnt{\footnotesize\rm}

\newcommand{\functiontextformat}[1]{\textrm{\texttt{#1}}}

% \newtoggle{usecomment}
% \settoggle{usecomment}{true}

% \newcommand{\tcomment}[1]{\iftoggle{usecomment}{#1}{}}
% \newcommand{\tvn}[1]{\iftoggle{usecomment}{{\color{red}{[TVN]: #1}}}{}}

% \newcommand{\mbd}[1]{\iftoggle{usecomment}{{\color{magenta}{[MBD]: #1}}}{}}
% \newcommand{\matt}[1]{\iftoggle{usecomment}{{\color{magenta}{[MBD]: #1}}}{}}

% \newcommand{\hd}[1]{\iftoggle{usecomment}{{\color{blue}{[HD]: #1}}}{}}

% \newcommand{\mynew}[1]{\iftoggle{usecomment}{{\color{red}{[NEW]: #1}}}{}}


%\newcommand{\ignore}[1]{}





\newtcolorbox{mybox}{
  enhanced,
  boxrule=0pt,frame hidden,
  borderline west={2pt}{0pt}{green!75!black},
  colback=green!10!white,
  sharp corners
}

\newenvironment{commentbox}[1][]{
  \small
  \begin{mybox}
    {\small \textbf{#1}}
  }{
  \end{mybox}
}

\newtcolorbox{mydomesticbox}{
  enhanced,
  boxrule=0pt,frame hidden,
  borderline west={2pt}{0pt}{red!75!black},
  colback=blue!10!white,
  sharp corners
}

\newenvironment{domesticbox}[1][]{
  \small
  \begin{mydomesticbox}
    {\small \textbf{#1}}
  }{
  \end{mydomesticbox}
}

\newcommand{\ignore}[1]{}

% for captions
\def\Section{\S}
\renewcommand{\figurename}{Fig.}
\renewcommand{\tablename}{Tab.}
\renewcommand{\algorithmcfname}{Alg.}

\makeatletter
% for cross references
% \renewcommand{\algorithmcflinename}{L.}
\renewcommand{\algorithmautorefname}{Alg.}
\renewcommand{\figureautorefname}{Fig.}
\renewcommand{\tableautorefname}{Tab.}
\renewcommand{\equationautorefname}{Eq.}
\renewcommand{\chapterautorefname}{\S\@gobble}
\renewcommand{\sectionautorefname}{\S\@gobble}
\renewcommand{\subsectionautorefname}{\S\@gobble}
\renewcommand{\appendixautorefname}{\S\@gobble}

\makeatother
\newcommand{\proofgen}{\texttt{BaB$_{\text{ProofGen}}$}}
\newcommand{\prooflang}{\texttt{BaB$_{\text{ProofLang}}$}}
\newcommand{\proofcheck}{\texttt{BaB$_{\text{ProofCheck}}$}}

\newcommand{\veristable}{\texttt{VeriStable}}
\newcommand{\crown}{\texttt{$\alpha\beta$-CROWN}}
\newcommand{\nnenum}{\texttt{nnenum}}
\newcommand{\marabou}{\texttt{Marabou}}
\newcommand{\eran}{\texttt{ERAN}}
\newcommand{\reluplex}{\texttt{Reluplex}}
\newcommand{\reluval}{\texttt{Reluval}}
\newcommand{\neurify}{\texttt{Neurify}}
\newcommand{\nnv}{\texttt{NNV}}
\newcommand{\ovalnnv}{\texttt{OVAL}}
\newcommand{\dnnv}{\texttt{DNNV}}
\newcommand{\verinet}{\texttt{VeriNet}}
\newcommand{\mnbab}{\texttt{MN-BaB}}
% \newcommand{\vnncomp}{VNN-COMP'23}
\newcommand{\planet}{\texttt{Planet}}
\newcommand{\bab}{\texttt{BaB$_{\text{NV}}$}}
\newcommand{\neuralsat}{\texttt{NeuralSAT}}
\newcommand{\mipverify}{\texttt{MIPVerify}}
\newcommand{\crowndefault}{\texttt{$\alpha\beta$-CROWN} (default)}


\newcommand{\mycomment}[3][\color{blue}]{{#1{{#2}: {#3}}}}
\newcommand{\tvn}[1]{\mycomment{TVN}{#1}}{}
\newcommand{\didi}[1]{\mycomment{Didier}{#1}}{}
\newcommand{\tl}[1]{\mycomment{ThanhLe}{#1}}{}
\newcommand{\red}[1]{{\color{red}{#1}}}
\newcommand{\xz}[1]{\mycomment{Xiaokuan}{[#1]}}{}


\begin{document}

\pagestyle{empty}
\begin{tikzpicture}[overlay,remember picture]

    % Background color
    \fill[
    black!2]
    (current page.south west) rectangle (current page.north east);
    
    % Rectangles
    \shade[
    left color=Dandelion, 
    right color=Dandelion!40,
    transform canvas ={rotate around ={45:($(current page.north west)+(0,-6)$)}}] 
    ($(current page.north west)+(0,-6)$) rectangle ++(9,1.5);
    
    \shade[
    left color=lightgray,
    right color=lightgray!50,
    rounded corners=0.75cm,
    transform canvas ={rotate around ={45:($(current page.north west)+(.5,-10)$)}}]
    ($(current page.north west)+(0.5,-10)$) rectangle ++(15,1.5);
    
    \shade[
    left color=lightgray,
    rounded corners=0.3cm,
    transform canvas ={rotate around ={45:($(current page.north west)+(.5,-10)$)}}] ($(current page.north west)+(1.5,-9.55)$) rectangle ++(7,.6);
    
    \shade[
    left color=orange!80,
    right color=orange!60,
    rounded corners=0.4cm,
    transform canvas ={rotate around ={45:($(current page.north)+(-1.5,-3)$)}}]
    ($(current page.north)+(-1.5,-3)$) rectangle ++(9,0.8);
    
    \shade[
    left color=red!80,
    right color=red!80,
    rounded corners=0.9cm,
    transform canvas ={rotate around ={45:($(current page.north)+(-3,-8)$)}}] ($(current page.north)+(-3,-8)$) rectangle ++(15,1.8);
    
    \shade[
    left color=orange,
    right color=Dandelion,
    rounded corners=0.9cm,
    transform canvas ={rotate around ={45:($(current page.north west)+(4,-15.5)$)}}]
    ($(current page.north west)+(4,-15.5)$) rectangle ++(30,1.8);
    
    \shade[
    left color=RoyalBlue,
    right color=Emerald,
    rounded corners=0.75cm,
    transform canvas ={rotate around ={45:($(current page.north west)+(13,-10)$)}}]
    ($(current page.north west)+(13,-10)$) rectangle ++(15,1.5);
    
    \shade[
    left color=ForestGreen,
    rounded corners=0.3cm,
    transform canvas ={rotate around ={45:($(current page.north west)+(18,-8)$)}}]
    ($(current page.north west)+(18,-8)$) rectangle ++(15,0.6);
    
    \shade[
    left color=ForestGreen,
    rounded corners=0.4cm,
    transform canvas ={rotate around ={45:($(current page.north west)+(19,-5.65)$)}}]
    ($(current page.north west)+(19,-5.65)$) rectangle ++(15,0.8);
    
    \shade[
    left color=OrangeRed,
    right color=red!80,
    rounded corners=0.6cm,
    transform canvas ={rotate around ={45:($(current page.north west)+(20,-9)$)}}] 
    ($(current page.north west)+(20,-9)$) rectangle ++(14,1.2);
    
 
    
    % Title
    \node[align=center] at ($(current page.center)+(0,-5)$) 
    {
        {\fontsize{24}{24} \selectfont {{Engineering A Verifier}}} \\[0.15in]
        {\fontsize{24}{24} \selectfont {{for Deep Neural Networks}}} \\[1in]    
        %{\fontsize{18}{18} \selectfont {{A Handbook for International Students}}} \\[1.in]

        {\fontsize{14}{19.2} \selectfont \textcolor{ForestGreen}{ \bf ThanhVu Nguyen}}\\[0.1in]
        {\fontsize{14}{19.2} \selectfont \textcolor{ForestGreen}{ \bf Hai Duong}}\\[0.1in]
    \today{} (latest version available on  \href{https://github.com/nguyenthanhvuh/book_nnv}{Github})
    };
    \end{tikzpicture}

    
\chapter*{Preface}

% \begin{mybox}
% This document will be updated regularly to reflect the latest information and updates in the admission process. Its latest version is available at

% \begin{center}
%   \href{https://nguyenthanhvuh.github.io/phd-cs-us/demystify.pdf}{nguyenthanhvuh.github.io/phd-cs-us/demystify.pdf},
% \end{center}

% \noindent and its \LaTeX{} source is also on \href{https://github.com/nguyenthanhvuh/phd-cs-us}{GitHub}. If you have questions or comments, feel free to create new \href{https://github.com/nguyenthanhvuh/phd-cs-us/issues}{GitHub issues} or \href{https://github.com/nguyenthanhvuh/phd-cs-us/discussions}{discussions}.

%\end{mybox}

\newpage
\tableofcontents

\input{parts/background}
\input{parts/approach}
\part{DNN Verification Algorithms}
\chapter{Search Algorithms}

\section{Branch-and-Bound (BaB)}\label{sec:bab}

    
\SetKwData{nextlayer}{layer$_{i+1}$}
\SetKwData{status}{status}
\SetKwData{minimum}{objval}

\SetKwFunction{InputMILP}{AddInputConstrs}
\SetKwFunction{GetUnstableNeurons}{GetUnstableNeurons}
\SetKwFunction{PiecewiseLinearMILP}{AddConstrsPWL}
\SetKwFunction{LinearMILP}{AddConstrsLinear}
\SetKwFunction{Maximize}{Maximize}
\SetKwFunction{Minimize}{Minimize}
\SetKwFunction{Feasible}{CheckFeasibility}
\SetKwFunction{Optimize}{Optimize}
\SetKwFunction{isPiecewiseLinear}{isPiecewiseLinear}
\SetKwFunction{CreateStabilizedMILP}{CreateStabilizedMILP}
\SetKwFunction{GetLeafNodes}{GetLeafNodes}
\SetKwFunction{AddConstrs}{AddConstrs}
\SetKwFunction{RemoveConstrs}{RemoveConstrs}
\SetKwFunction{AddObjective}{AddObjectives}
\SetKwFunction{ShortenSplitConstrs}{ShortenSplitConstrs}
\SetKwFunction{RemoveLeafNodes}{RemoveLeaves}
\SetKwFunction{StoppingConditions}{StoppingConditions}
\SetKwFunction{RepOK}{RepOK}
\SetKwFunction{RaiseError}{RaiseError}

\SetKwInOut{Input}{input}
\SetKwInOut{Output}{output}
\SetKw{Break}{break}
\SetKw{Continue}{continue}
\SetKwFunction{Backtrack}{Backtrack}
\SetKwFunction{Select}{Select}
\SetKwFunction{Decide}{Decide}
\SetKwFunction{BCP}{BCP}
\SetKwFunction{Deduce}{Deduce}
\SetKwFunction{AnalyzeConflict}{AnalyzeConflict}
\SetKwFunction{BooleanAbstraction}{BooleanAbstraction}
\SetKwFunction{AddClause}{AddClause}
\SetKwFunction{isTotal}{isTotal}
\SetKwFunction{randomattack}{RandomAttack}
\SetKwFunction{pgd}{PGDAttack}

\SetKwFunction{DPLLT}{DPLLT}
\SetKwFunction{isValid}{isValid}
\SetKwFunction{isEmpty}{isEmpty}
\SetKwFunction{LPSolver}{LPSolver}
\SetKwFunction{Solve}{Solve}
\SetKwFunction{FindLayerNodes}{FindLayerNodes}
\SetKwFunction{TightenInputBounds}{TightenInputBounds}
\SetKwFunction{Abstract}{Abstract}
\SetKwFunction{Check}{Check}
\SetKwFunction{Decide}{Decide}
\SetKwFunction{Imply}{Imply}
\SetKwFunction{Lower}{LowerBound}
\SetKwFunction{Upper}{UpperBound}
\SetKwFunction{GetInputBounds}{GetInputBounds}
\SetKwFunction{GetInputs}{GetInputs}
\SetKwFunction{GetNumInputs}{GetNumInputs}
\SetKwFunction{CurrentConflictClause}{CurrentConflictClause}
\SetKwFunction{StopCriterion}{StopCriterion}
\SetKwFunction{LastAssignedLiteral}{LastLiteral}
\SetKwFunction{LiteralToVariable}{LiteralToVariable}
\SetKwFunction{Antecedent}{Antecedent}
\SetKwFunction{BinRes}{BinRes}
\SetKwFunction{BacktrackLevel}{BacktrackLevel}
\SetKwFunction{AddClause}{AddClause}
\SetKwFunction{ActivationStatus}{ActivationStatus}
\SetKwFunction{Backjump}{Backjump}
\SetKwFunction{EstimateBounds}{EstimateBounds}

\SetKwData{problems}{ActPatterns}
\SetKwData{implicationgraph}{igraph}
\SetKwData{literal}{lit}
\SetKwData{variable}{var}
\SetKwData{antecedent}{ante}
\SetKwData{conflict}{conflict}
\SetKwData{none}{none}
\SetKwData{layerid}{lid}
\SetKwData{hiddenbounds}{hidden\_bounds}
\SetKwData{inputs}{inputs}
\SetKwData{inputbounds}{input\_bounds}
\SetKwData{outputbounds}{output\_bounds}
\SetKwData{infeasible}{INFEASIBLE}
\SetKwData{unreachable}{UNREACHABLE}
\SetKwData{maxinputs}{MAX\_NUM\_INPUT}
\SetKwData{assignment}{$\sigma$}
\SetKwData{dl}{dl}
\SetKwData{lpmodel}{solver}
\SetKwData{clauses}{clauses}
\SetKwData{conflict}{conflict}
\SetKwData{clause}{clause}
\SetKwData{igraph}{igraph}
\SetKwData{cex}{cex}
\SetKwData{sat}{sat}
\SetKwData{unsat}{unsat}

\SetKwData{submodel}{sub\_model}

\SetKwData{true}{true}
\SetKwData{false}{false}
    
\SetKwFunction{Restart}{Restart}

\SetKwData{counterexample}{cex}
\SetKwData{conflictclause}{conflict\_clause}
\SetKwData{isconflict}{is\_conflict}


\begin{algorithm}[t]
    \footnotesize
    \Input{DNN $\mathcal{N}$, property $\phi_{in} \Rightarrow \phi_{out}$}
    \Output{$\unsat$ if property is valid, otherwise ($\sat, \counterexample$)}
    \BlankLine

    $\problems \leftarrow \{ \emptyset \}$ \tcp{initialize verification problems} 
    % $\prooftree \gets \{ ~ \}$ \tcp{initialize proof tree}\label{line:prooftree}
    
    \While(\tcp*[h]{main loop}){$\problems$}{\label{line:dpllstart}
        % \tcp{$\sigma_i$ is the activation pattern of problem $i$-th}
        $\sigma_i \gets \Select(\problems)$ \tcp{process problem $i$-th}
        % \Parfor(\tcp*[h]{process in parallel}){$\sigma_i ~\In~ \problems$}{ \label{line:parfor}
            \If{\Deduce{$\mathcal{N}, \phi_{in}, \phi_{out}, \sigma_i$}}{\label{line:deduce}
                $(\counterexample, v_i) \leftarrow \Decide(\mathcal{N}, \phi_{in}, \phi_{out}, \sigma_i)$ \\ \label{line:decide}
                \If(\tcp*[h]{found a valid counter-example}){$\counterexample$}{
                    \Return{$(\sat, \counterexample)$} 
                }
                \tcp{create new activation patterns}
                $\problems \leftarrow \problems \cup \{ \sigma_i \land v_i ~;~ \sigma_i \land \overline{v_i} \}$ \;
            }
            %\Else(\tcp*[h]{detect a conflict}){
                % $\clauses \leftarrow \clauses \cup \AnalyzeConflict(\igraph_i)$ \\ \label{line:analyze_conflict}
            %    $\prooftree \leftarrow \prooftree \cup \{ \sigma_i \}$ \tcp{build proof tree} \label{line:record_proof}
            %}
        % }
        % \If(\tcp*[h]{no more problems}){$\isEmpty(\problems)$}{
        % }
        
    }\label{line:dpllend}
    \Return{\unsat}
    
    \caption{The \bab{} algorithm.}\label{alg:bab}
\end{algorithm}

Most modern DNN verifiers adopt the branch-and-bound (BaB) approach to search for activation patterns for DNN verification. A BaB search consists of two main components: (branch) splitting into the problem smaller subproblems 
by using \emph{neuron splitting}, which decides boolean values representing neuron activation status, 
and (bound) using abstraction and LP solving to approximate bounds on neuron values to determine 
the satisfiability of the partial activation pattern formed by the split.


\autoref{alg:bab} shows \bab{}, a reference BaB architecture~\cite{nakagawa2014consolidating} for modern DNN verifiers. \bab{} takes as input a ReLU-based DNN $\mathcal{N}$ and a formulae $\phi_{in}\Rightarrow \phi_{out}$ representing the property of interest.
\bab{} iterates between (i) branching (\autoref{line:decide}), which \emph{decides} (assigns) an activation status value for a neuron, and (ii) bounding (\autoref{line:deduce}), which \emph{deduces} or checks the feasibility of the current activation pattern. 
%To add proof generation capability, \bab{} is instrumented with a proof tree (\texttt{proof}) variable (\autoref{line:prooftree}) to record these branching decisions. The proof is represented as a binary tree structure, where each node represents a neuron and its left and right edges represent its activation decision (active or inactive). %The proof tree is then used to generate a proof in the \prooflang{} format (\autoref{sec:proof-format}).




\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\linewidth]{figure/proof_net.pdf}
    \caption{A simple DNN.}\label{fig:example1}
\end{figure}
%White nodes correspond to branching nodes where \neuralsat{} makes decisions to split ReLU neurons.

\paragraph{Example} \autoref{fig:example1}a illustrates a DNN and how \bab{} verifies that this DNN has the property $(x_1, x_2) \in [-2.0, 2.0] \times [-1.0, 1,0] \Rightarrow (y_1 > y_2)$. 

First, \bab{} initializes the activation pattern set \functiontextformat{ActPatterns} with an empty activation pattern $\emptyset$. Then \bab{} enters a loop (\autoref{line:dpllstart}-\autoref{line:dpllend}) to search for a satisfying assignment. In the first iteration, \bab{} selects the only available activation pattern $\emptyset \in \functiontextformat{ActPatterns}$. 
It calls \functiontextformat{Deduce} to check the feasibility of the problem based on the current activation pattern. \functiontextformat{Deduce} uses abstraction to approximate that from the input constraints the output values are feasible for the given network. 
Since \functiontextformat{Deduce} cannot decide infeasibility, \bab{} randomly selects a neuron to split (\functiontextformat{Decide}). Let us assume that it chooses $v_4$ to split, which essentially means the problem is split into two independent subproblems: one with $v_4$ active and the other with $v_4$ inactive.
\bab{} then adds $\{v_4\}$ and $\{\overline{v_4}\}$ to \functiontextformat{ActPatterns}.

In the second iteration, \bab{} has two subproblems (that can be processed in parallel). For the first subproblem with $v_4$, \functiontextformat{Deduce} cannot decide infeasibility, so it selects $v_2$ to split. It then conjoins $v_4$ with $v_2$ and then with $\overline{v_2}$ and adds both conjuncts to \texttt{ActPatterns}. 
For the second subproblem with $\overline{v_4}$ inactive, \functiontextformat{Deduce} determines that the problem is unsatisfiable.

In the third iteration, \bab{} has two subproblems for $v_4 \land v_2$ and $v_4 \land \overline{v_2}$. For the first subproblem, \functiontextformat{Deduce} cannot decide infeasibility, so it selects $v_1$ to split. It then conjoins $v_1$ and then $\overline{v_1}$ to the current activation pattern and adds them to \functiontextformat{ActPatterns}. For the second subproblem, \functiontextformat{Deduce} determines that the problem is unsatisfiable.

In the fourth iteration, \bab{} has two subproblems for $v_4 \land v_2 \land v_1$ and $v_4 \land v_2 \land \overline{v_1}$. Both subproblems are determined to be unsatisfiable.

Finally, \bab{} has an empty \texttt{ActPatterns}, stops the search, and returns \texttt{unsat}.




\part{Optimizations and Strategies}

\chapter{Adversarial Attacks}
\chapter{GPU and Multicore Parallelism}


\part{Survey of DNN Verification Tools}

\chapter{Popular Techniques and Tools}




\part{Advanced Topics\label{part:advanced-topics}}




\chapter{Proof Generation and Checking}\label{chapter:proof-gen-check}

%Advances in DNN verification have made it possible to reason about the safety and robustness of complex DNNs. 
As DNN tools become more complex (e.g., SOTA tools have 20K LoCs), they are more prone to bugs. VNN-COMP'23~\cite{brix2023fourth} showed that 3 of the top 7 participants produced unsound results, in which they incorrectly claiming unsafe DNNs are safe. This undesirable behavior defeats the purpose of DNN verification and hinders its practical adoption.

While checking counterexamples, which demonstrate the unsafety of a DNN, is relatively straightforward, certifying the safety of a DNN is far more challenging. That is, certifying \texttt{unsat} results---proving no counterexample exists---is far more challenging. This would require verifiers to track their decision steps, which are often complex and large. %Moreover, there are many verification approaches (e.g., branch and bound, conflict-driven learning, abstraction and refinement), making it difficult to standardize and compare results across different tools.
%This is unlike SAT solving, where most tools adopt the standard DPLL framework and thus allows for unified techniques in proof generation and checking (e.g., the DRAT family of algorithms for UNSAT proofs).

\section{Proof Generation}\label{sec:proofgen}

% To successfully deploy DNN verification tools in safety-critical applications, it is essential to provide guarantees that the verification results are correct, i.e., it won't claim an unsafe DNN as safe. One way to achieve this is to generate proofs of the verification results, i.e., a sequence of logical steps that demonstrates the validity of the verification result.
% Ideally, the proof generation process should be automated and integrated into the verification tool, so that the proof can be generated and checked automatically. The challenge is that there are many different DNN verification techniques, each with its own decision-making process, making it difficult to standardize proof generation across different tools. 



A proof of satisfiability (sat) is an input that violates the property, i.e., a counterexample.  We can easily  check such a counterexample $c$ by evaluating $\phi(c,N(c))$ (i.e., running the DNN on the counterexample).
In fact, VNN-COMPs already requires competing DNN verification tools to return counterexamples demonstrating satisfiability.  

In contrast, the proof of an unsatisfiability result (which explains why \emph{no possible inputs} can violate the property) is inherently more complex to generate (\autoref{sec:proofgen}), requires a more sophisticated encoding (\autoref{sec:prooflang}), and an efficient checking algorithm (\autoref{sec:proofchecking}). We are mainly interested in \texttt{unsat} proofs.


\begin{algorithm}[t]
    \footnotesize
  
    \Input{DNN $\mathcal{N}$, property $\phi_{in} \Rightarrow \phi_{out}$}
    \Output{($\unsat, \prooftree$) if property is valid, otherwise ($\sat, \counterexample$)}
    \BlankLine


    $\problems \leftarrow \{ \emptyset \}$ \tcp{initialize verification problems} 
    $\prooftree \gets \{ ~ \}$ \tcp{initialize proof tree}\label{line:prooftree}
    
    \While(\tcp*[h]{main loop}){$\problems$}{\label{line:dpllstart}
        % \tcp{$\sigma_i$ is the activation pattern of problem $i$-th}
        $\sigma_i \gets \Select(\problems)$ \tcp{process problem $i$-th}
        % \Parfor(\tcp*[h]{process in parallel}){$\sigma_i ~\In~ \problems$}{ \label{line:parfor}
            \If{\Deduce{$\mathcal{N}, \phi_{in}, \phi_{out}, \sigma_i$}}{\label{line:deduce}
                $(\counterexample, v_i) \leftarrow \Decide(\mathcal{N}, \phi_{in}, \phi_{out}, \sigma_i)$ \\ \label{line:decide}
                \If(\tcp*[h]{found a valid counter-example}){$\counterexample$}{
                    \Return{$(\sat, \counterexample)$} 
                }
                \tcp{create new activation patterns}
                $\problems \leftarrow \problems \cup \{ \sigma_i \land v_i ~;~ \sigma_i \land \overline{v_i} \}$ \;
            }
            \Else(\tcp*[h]{detect a conflict}){
                % $\clauses \leftarrow \clauses \cup \AnalyzeConflict(\igraph_i)$ \\ \label{line:analyze_conflict}
                $\prooftree \leftarrow \prooftree \cup \{ \sigma_i \}$ \tcp{build proof tree} \label{line:record_proof}
            }
        % }
        % \If(\tcp*[h]{no more problems}){$\isEmpty(\problems)$}{
        % }
        
    }\label{line:dpllend}
    \Return{$(\unsat, \prooftree)$}
    
    \caption{The \proofgen{} DNN verification with proof generation.}\label{alg:bab-proof}
\end{algorithm}



%\emph{Branch-and-bound} (BaB) is a common approach to DNN verification and used by major DNN verification tools including~\cite{bunel2020branch,wang2021beta,ferrari2022complete,duong2024harnessing,duong2023dpllt,ovalbab}.
%In BaB algorithms, ``branch'' means splitting the problem into smaller subproblems (like a search tree), and ``bound'' means computing the upper and lower bounds, using abstraction, of the problem to prune the search space. 



%Among modern DNN verification techniques, only the simplex-based approach attempts to generate proofs~\cite{desmartin2023towards,barbosa2023generating}. However, generating a proof for simplex-based approaches is non-trivial and incurs a significant overhead during the verification process~\cite{desmartin2023towards,barbosa2023generating}.

%Recently, an approach based on the DPLL(T) algorithm in SAT/SMT solving has shown much promise~\cite{duong2023dpllt}. For example, in its debut year, the DPLL(T)-based \neuralsat{} DNN verifier~\cite{duong2023dpllt} won the New Comer Award at VNN-COMP'23~\cite{brix2023fourth} and outperformed other state-of-the-art DNN verification tools, especially for very large networks~\cite{duong2024harnessing}. More importantly, DPLL(T) approach maintains \emph{implication graph}, an internal data structure to keep track of assignments, which essentially represent a  proof of the verification result. In other words, proofs are a built-in feature of DPLL(T) approach, and thus can be generated with minimal (in fact, \emph{zero}) overhead.

\subsection{Proof Generation for Branch and Bound (BaB) Algorithms}\label{sec:proogen}

As mentioned in~\autoref{sec:bab}, major DNN verification techniques share the common ``branch and bound'' (BaB) search algorithm. The BaB structure, shown in~ \autoref{alg:bab}, splits the problem into smaller subproblems and use abstraction to compute bounds to prune the search space. This commonality allows us to bring proof generation capabilities with minimal overhead to existing DNN verification tools.

\autoref{alg:bab-proof} extends~\autoref{alg:bab} to show \proofgen, a BaB-based DNN verification algorithm with proof generation capability. The key idea is to introduce a proof tree (\autoref{line:prooftree}) and recording the branching decisions to the proof tree (\autoref{line:record_proof}).
The proof tree is a binary tree structure, where each node represents a neuron and its left and right edges represent its activation decision (active or inactive).
At the end of the verification process, the proof tree is returned as the proof of \texttt{unsat} result. 



% \bab{}, a reference architecture~\cite{nakagawa2014consolidating} 
% for modern DNN verifiers that we use to illustrate our observations.  
% \bab{} takes as input a ReLU-based DNN $\mathcal{N}$ and a formulae $\phi_{in}\Rightarrow \phi_{out}$ representing the property of interest.

%\bab{} iterates between two components: \texttt{Decide} (branching, \autoref{line:decide}), which decides (assigns) an activation status value for a neuron, and \texttt{Deduce} (bounding, \autoref{line:deduce}), which checks the feasibility of the current activation pattern. 
%To add proof generation capability, \bab{} is instrumented with a proof tree (\texttt{proof}) variable (\autoref{line:prooftree}) to record these branching decisions. The proof is represented as a binary tree structure, where each node represents a neuron and its left and right edges represent its activation decision (active or inactive). %The proof tree is then used to generate a proof in the \prooflang{} format (\autoref{sec:proof-format}).




\begin{figure}[t]
    \begin{minipage}[b]{\linewidth}
        \centering
        \begin{minipage}[t]{0.48\textwidth}
            \centering  
            \includegraphics[width=\linewidth]{figure/proof_net.pdf}
            \caption*{(a)}
        \end{minipage}
        \begin{minipage}[t]{0.48\textwidth}
            \centering
            \includegraphics[width=\linewidth]{figure/proof_tree.pdf}
            \caption*{(b)}
        \end{minipage}
        \caption{(a) A simple DNN  (a redrawn of~\autoref{fig:example1}), and (b) A proof tree produced verifying the property $(x_1, x_2) \in [-2.0, 2.0] \times [-1.0, 1,0] \Rightarrow (y_1 > y_2)$.}
        \label{fig:example}
    \end{minipage}
\end{figure}
%White nodes correspond to branching nodes where \neuralsat{} makes decisions to split ReLU neurons.

\paragraph{Example} We reuse the example in~\autoref{sec:bab} to illustrate \proofgen{}. Recall the goal is to verify that the DNN in \autoref{fig:example}(a) (a redraw of~\autoref{fig:example1}) has the property $(x_1, x_2) \in [-2.0, 2.0] \times [-1.0, 1,0] \Rightarrow (y_1 > y_2)$. \proofgen{} generates the proof tree in \autoref{fig:example}(b) to show unsatisfiability, i.e., the property is valid.

First, \proofgen{} initializes the activation pattern set \functiontextformat{ActPatterns} with an empty activation pattern $\emptyset$. Then \proofgen{} enters a loop (\autoref{line:dpllstart}-\autoref{line:dpllend}) to search for a satisfying assignment or a proof of unsatisfiability. In the first iteration, \proofgen{} selects the only available activation pattern $\emptyset \in \functiontextformat{ActPatterns}$. 
It calls~\functiontextformat{Deduce} to check the feasibility of the problem based on the current activation pattern. \functiontextformat{Deduce} uses abstraction to approximate that from the input constraints the output values are feasible for the given network. 
Since \functiontextformat{Deduce} cannot decide infeasibility, \proofgen{} randomly selects a neuron to split (\functiontextformat{Decide}). Let us assume that it chooses $v_4$ to split, which essentially means the problem is split into two independent subproblems: one with $v_4$ active and the other with $v_4$ inactive.
\proofgen{} then adds $v_4$ and $\overline{v_4}$ to \functiontextformat{ActPatterns}.

In the second iteration, \proofgen{} has two subproblems (that can be processed in parallel). For the first subproblem with $v_4$, \functiontextformat{Deduce} cannot decide infeasibility, so it selects $v_2$ to split. It then conjoins $v_4$ with $v_2$ and then with $\overline{v_2}$ and adds both conjuncts to \texttt{ActPatterns}. 
For the second subproblem with $\overline{v_4}$ inactive, \functiontextformat{Deduce} determines that the problem is unsatisfiable and \proofgen{} saves the node $v_4$ to the proof tree, as node 3, to indicate one unsatisfiable pattern, i.e., whenever the network has $v_4$ being inactive, the problem is unsatisfiable.

In the third iteration, \proofgen{} has two subproblems for $v_4 \land v_2$ and $v_4 \land \overline{v_2}$. For the first subproblem, \functiontextformat{Deduce} cannot decide infeasibility, so it selects $v_1$ to split. It then conjoins $v_1$ and then $\overline{v_1}$ to the current activation pattern and adds them to \functiontextformat{ActPatterns}. For the second subproblem, \functiontextformat{Deduce} determines that the problem is unsatisfiable and \proofgen{} saves the node $v_4 \land \overline{v_2}$ to the proof tree, as node 5.

In the fourth iteration, \proofgen{} has two subproblems for $v_4 \land v_2 \land v_1$ and $v_4 \land v_2 \land \overline{v_1}$. Both subproblems are determined to be unsatisfiable, and \proofgen{} saves them to the proof tree as nodes 6 and 7, respectively.

Finally, \proofgen{} has an empty \texttt{ActPatterns}, stops the search, and returns \texttt{unsat} and the proof tree. 

\section{Proof Language}\label{sec:prooflang}

\lstdefinelanguage{SMTLIB}{
    morekeywords={assert, declare-const, declare-pwl, or, and, },
    alsoletter={-},
    morecomment=[l];,
    morestring=[b]",
    morekeywords=[2]{Real, Int, ReLU},
    keywordstyle=[2]\color{codepurple},
}

\lstdefinestyle{SMTLIB-style}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{black},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false, 
    breaklines=true, 
    captionpos=b, 
    keepspaces=true, 
    numbers=left, 
    numbersep=5pt, 
    showspaces=false, 
    showstringspaces=false,
    showtabs=false, 
    tabsize=2,
}

\lstset{
    language=SMTLIB,
    style=SMTLIB-style
}

\newcommand{\lra}[1]{
    \textcolor{green!40!black}{\langle} 
    \textit{\textcolor{green!40!black}{#1}} 
    \textcolor{green!40!black}{\rangle}
}

\begin{figure}
{\small
\begin{align*}
    \lra{proof}         &::= \lra{declarations} \ \lra{assertions} \\
    \lra{declarations}  &::= \lra{declaration} \ | \ \lra{declaration} \ \lra{declarations} \\
    \lra{declaration}   &::= (\textbf{declare-const} \ \lra{input-vars} \ \textbf{Real}) \\
                        & \quad ~| \ (\textbf{declare-const} \ \lra{output-vars} \ \textbf{Real}) \\
                        & \quad ~| \ (\textbf{declare-pwl} \ \lra{hidden-vars} \ \lra{activation}) \\
    \lra{input-vars}    &::= \lra{input-var} \ | \ \lra{input-var} \ \lra{input-vars} \\
    \lra{output-vars}    &::= \lra{output-var} \ | \ \lra{output-var} \ \lra{output-vars} \\
    \lra{hidden-vars}    &::= \lra{hidden-var} \ | \ \lra{hidden-var} \ \lra{hidden-vars} \\
    \lra{activation}    &::= ~\text{ReLU} \ | \ \text{Leaky ReLU} \ | \ \ldots \\
    \lra{assertions}    &::= \lra{assertion} \ | \ \lra{assertion} \ \lra{assertions} \\
    \lra{assertion}     &::= (\textbf{assert} \ \lra{formula}) \\
    \lra{formula}       &::= (\lra{operator} \ \lra{term} \ \lra{term}) \\
                        & \quad ~| \ (\textbf{and} \ \lra{formula}+) \ | \ (\textbf{or} \ \lra{formula}+) \\
                        % & \quad ~| \ (\textbf{and} \ \lra{formula} \ \lra{formula}) \\
                        % & \quad ~| \ (\textbf{or} \ \lra{formula}+) \\
                        % & \quad ~| \ (\textbf{or} \ \lra{formula} \ \lra{formula}) \\
    \lra{term}          &::= \lra{input-var} \ | \ \lra{output-var} \\ 
                        & \quad ~| \ \lra{hidden-var} \ | \ \lra{constant} \\
    \lra{operator}      &::= ~ < \ | \ \leq \ | \ > \ | \ \geq \\
    \lra{input-var}     &::= ~\text{X\_}\lra{constant} \\
    \lra{output-var}    &::= ~\text{Y\_}\lra{constant} \\
    \lra{hidden-var}    &::= ~\text{N\_}\lra{constant} \\
    \lra{constant}      &::= ~\textbf{Int} \ | \ \textbf{Real}
\end{align*}
}
\caption{The \prooflang{} proof language.}\label{fig:grammar}
\end{figure}

In~\autoref{sec:proofgen} we have shown that the broad branch-and-bound (BaB) class of DNN verification techniques  can generate a binary tree that represents a proof of unsatisfiability.
Rather than record such proofs in an verifier-specific format, it is more desirable to have a standard format
that is human-readable, is compact, can be efficiently generated by verification tools, and can
be efficiently and independently processed by proof checkers.  

To meet this goal, we introduce \prooflang{}, a proof language to specify DNN proofs.
This language is inspired by the SMTLIB format~\cite{barrett2010smt} used for SMT solving, which has also been adopted by the  VNNLIB language~\cite{vnnlib} to specify DNNs and their properties for  verification.

%accepted by our proof check algorithm (\autoref{sec:proof-checker}

\autoref{fig:grammar} outlines the \prooflang{} syntax and grammar, represented as production rules. 
A proof is composed of \textit{declarations} and \textit{assertions}. Declarations define the variables and their types within the proof. Specifically, \textit{input variables} (prefixed with \functiontextformat{X}) and \textit{output variables} (prefixed with \functiontextformat{Y}) are declared as real numbers, representing the inputs and outputs of the neural network, respectively. Additionally, \textit{hidden variables} are declared with specific piece-wise linear (PWL) activation functions, such as ReLU or Leaky ReLU. These hidden variables correspond to the internal nodes of the neural network that process the input data through various activation functions.

Assertions are logical statements that specify the conditions or properties that must hold within the proof. Assertions over input variables are \emph{preconditions} and those over output variables are \emph{post-conditions}. Each assertion is composed of a \textit{formula}, which can involve terms and logical operators. Formulas include simple comparisons between terms (e.g., less than, greater than) or more complex logical combinations using \functiontextformat{and} and \functiontextformat{or} operators. The terms used in these formulas can be input variables, output variables, hidden variables, or constants.

\begin{figure}
\begin{lstlisting}[style=SMTLIB-style, language=SMTLIB, basicstyle=\ttfamily\scriptsize]
; Declare variables
(declare-const X_0 X_1 Real)
(declare-const Y_0 Y_1 Real)
(declare-pwl N_1 N_2 N_3 N_4 ReLU)

; Input constraints
(assert (>= X_0 -2.0))
(assert (<= X_0  2.0))
(assert (>= X_1 -1.0))
(assert (<= X_1  1.0))

; Output constraints
(assert (<= Y_0 Y_1)) 

; Hidden constraints
(assert (or 
    (and (<  N_4 0))
    (and (<  N_2 0) (>= N_4 0))
    (and (>= N_2 0) (>= N_1 0) (>= N_4 0))
    (and (>= N_2 0) (<  N_1 0) (>= N_4 0))))
\end{lstlisting}
\caption{\prooflang{} example format of the proof tree in \autoref{fig:example}b. 
%\tvn{seems to take too much space. Could we put multiple statements in a line?}\hd{is it compact enough?}\tvn{yep. thanks}
}
\label{fig:proof_example}
\end{figure}

%\autoref{fig:proof_example} shows an example of a proof in \prooflang{} using the network in \autoref{fig:example}a. 

The \texttt{declare-*} statements declare input, output, and hidden variables, while the \texttt{assert} statements specify the constraints on these variables (i.e., the pre and postcondition of the desired property).
The hidden constraints represent the activation patterns of the hidden neurons in the network (i.e., the proof tree). Each \texttt{and} statement represents a tree path that represents an activation pattern. 



\subsection{Example} The proof in \autoref{fig:proof_example} corresponds to the proof tree in \autoref{fig:example}b. The statement \texttt{(and (< N\_4 0))} corresponds to the rightmost path of the tree with $\overline{v_4}$ decision (leaf 3).  The statement \texttt{(and (< N\_2 0) (>= N\_4 0))} corresponds to the path with $v_4 \land \overline{v_2}$ (leaf 5). 

The \prooflang{} language is intentionally designed to (a) not explicitly include weights/bias terms to minimize size of the proof structure, and (b) explicitly reflect a DNF structure to enable easy parallelization.
The DNN weight and bias terms are readily available in the standard ONNX~\cite{onnx} format, which is typically used to represent the DNN input to a \proofgen{}-based DNN verification tool and can be accessed by any \prooflang{} checker like the one described next in~\autoref{sec:proofchecking}.


\section{Proof Checker}\label{sec:proofchecking}
Finally, we need to check that the generated proof is correct and that the original NN verification problem is indeed unsatisfiable. The checker must be efficient to handle large proofs and trusted of its results (if it verifies the proof, then the original NNV problem is proved).


To achieve this, we present \proofcheck{}, a proof checker for \prooflang{} proofs.
\proofcheck{} is verifier-independent and support \prooflang{} proofs generated by different verification tools. \proofcheck{} also has several optimizations to handle large proofs efficiently.

\subsection{The Core \proofcheck{} Algorithm}


The goal of \proofcheck{} is to verify that the \prooflang{} tree generated by a DNN verification tool is correct (i.e., the proof tree is a proof of unsatisfiability of the DNN verification problem).
\proofcheck{} thus must verify that the constraint represented by each \emph{leaf} node in the proof tree is unsatisfiable. To check each node, \proofcheck{} forms an MILP problem (\autoref{sec:milp}) consisting of the constraint in \autoref{eq:prob} (the DNN, the input condition, and the negation of the output) with the constraints representing the activation pattern encoded by the tree path to the leaf node. \proofcheck{} then invokes an LP solver to check that the MILP problem is infeasible, which indicates unsatisfiability of the leaf node. 

\begin{algorithm}[t]
    \footnotesize

    \Input{DNN $\mathcal{N}$, property $\phi_{in} \Rightarrow \phi_{out}$, $\prooftree$}
    \Output{\certified if proof is valid, otherwise \uncertified}
    \BlankLine

    \If{$\neg$ \RepOK(\prooftree)}{
        \RaiseError{Invalid proof tree}
    }

    \tcp{initialize MILP model with inputs}
    $\model \leftarrow \CreateStabilizedMILP(\mathcal{N}, \phi_{in}, \phi_{out})$ \label{line:build_model}

    
    $\leaf \gets \mynull$ \tcp{initialize current processing node}
    
    \While{$\prooftree$}{\label{line:proof_loop}
        
        % \tcp{Get $k$ leaf nodes from $\prooftree$}
        % $[\leaf_1, ..., \leaf_k] \gets \GetLeafNodes(\prooftree, k)$ \\ \label{line:get_leaf}
        $\leaf \gets \Select(\prooftree, \leaf)$ \tcp{get next node to check} \label{line:select} \label{line:get_leaf}

        % \tcp{Process $k$ nodes in parallel}
        % \Parfor{$\leaf_i ~\In~ [\leaf_1, ..., \leaf_k]$}{ \label{line:parfor}
        
        $\model \gets \AddConstrs(\model, \leaf)$  \tcp{add constraints} \label{line:add_proof_leaf_constrs}
        
        % $\minimum \gets \Minimize(\model)$ \tcp{} \label{line:optimize1}
        \If{$\Feasible(\model)$}{\label{line:proof_check_objective1}
            \Return{\uncertified} \tcp{cannot certify}
        }
        
        % $\Backtrack(\prooftree, \leaf)$ \tcp{process interior node}
        
        % \While{$\StoppingConditions(\leaf_i)$}{ \label{line:stop_condition}
        %     % \tcp{remove constraints of old $\leaf_i$ }
        %     % $\model \gets \RemoveConstrs(\model, \leaf_i)$  \\
        %     % \tcp{move to $\leaf_i$'s parent}
        %     % $\leaf_i \gets \ShortenSplitConstrs(\leaf_i)$ \\
        %     % \tcp{add constraints of new (looser) $\leaf_i$ }

        %     \tcp{move to $\leaf_i$'s parent}
        %     $\model \gets \Backjump(\leaf_i)$  \\ \label{line:proof_backjump}

        %     \tcp{Prove the new $\leaf_i$}
        %     $\minimum \gets \Minimize(\model)$ \\ \label{line:optimize2}
        %     \If(\tcp*[h]{cannot certify}){$\minimum \le 0$}{
        %         \Break
        %     }
        %     \tcp{if parent is proved, remove its children}
        %     $\prooftree \gets \RemoveLeafNodes(\prooftree, \leaf_i)$ \label{line:proof_prune}
        % }
        }
    % }
    \Return{\certified}

    \caption{\proofcheck{} algorithm.}
    % \Description{}
    \label{fig:algorithm}
\end{algorithm}


\autoref{fig:algorithm} shows a minimal (core)  \proofcheck{} algorithm, which takes as input a DNN $\mathcal{N}$, a property $\phi_{in} \Rightarrow \phi_{out}$, a proof tree $\prooftree$, and returns \certified if the proof tree is valid and \uncertified otherwise. 
\proofcheck{} first checks the validity of the proof tree (\autoref{line:build_model}), i.e., the input must represent a proper \prooflang{} proof tree (\autoref{sec:prooflang}).  
If the proof tree is invalid, \proofcheck{} raises an error.
\proofcheck{} next creates a MILP model (\autoref{line:build_model}) representing the input. % (\autoref{sec:neuron-stabelization}).
\proofcheck{} then enters a loop (\autoref{line:proof_loop}) that selects a (random) leaf node from the proof tree (\autoref{line:select}) and adds its MILP constraint to the model (\autoref{line:add_proof_leaf_constrs}). It then checks the model using an LP solver to determine whether the leaf node is unsatisfiable. If the LP solver returns feasibility, \proofcheck{} returns \uncertified, i.e., it cannot verify the input proof tree. 
\proofcheck{} continues until all leaf nodes are checked and returns \certified, indicating the proof tree is valid.



\subsubsection{Example} For the \prooflang{} proof in \autoref{fig:proof_example}, we need to check that the four leaf nodes 3, 5, 6, and 7 of the proof tree in \autoref{fig:example}b are unsatisfiability. Assume \proofcheck{} first selects node 3, it forms the MILP problem for leaf node 3 by conjoining the constraint representing $0.6v_1 + 0.9v_2 - 0.1 \le 0$ (i.e., $\overline{v_4}$) %\tvn{Hai check}\hd{by setting $a_1^{(2)}=0$ representing that $\hat{z}_1^{(2)} \le 0$ (see \autoref{eq:mip}e) or \emph{implicitly} conjoining the inequality $0.6\hat{z}^{(1)}_0 + 0.9\hat{z}^{(1)}_1 - 0.1 \le 0$ (or ${z}_1^{(2)} \le 0$), where $\hat{z}^{(1)}_0$, $\hat{z}^{(1)}_1$ represent the outputs of $v_1$ and $v_2$, respectively.} 
with the constraints in \autoref{eq:prob} representing the input ranges and the DNN with the objective of optimizing the output. \proofcheck{} then invokes an LP solver, which determines that this MILP is infeasible, i.e., leaf node 3 indeed leads to unsatisfiability. \proofcheck{} continues this process for the other three leaf nodes and returns \certified as all leaf nodes are unsatisfiable.


\subsubsection{MILP Formulation}\label{sec:milp}

\proofcheck{} formulates MILP problems~\cite{tjeng2019evaluating} and check for feasible solutions using off-the-shelf LP solving. Formally, the MILP problem is defined as:
\begin{equation}
    \begin{aligned}
        &\mbox{(a)}\quad z^{(i)} = W^{(i)} \hat{z}^{(i-i)} + b^{(i)};\\
        &\mbox{(b)}\quad y = z^{(L)};  x = \hat{z}^{(0)}; \\
        &\mbox{(c)}\quad \hat{z}_j^{(i)} \ge {z}_j^{(i)}; \hat{z}_j^{(i)} \ge 0; \\
        &\mbox{(d)}\quad a_j^{(i)} \in \{ 0, 1\} ;\\
        &\mbox{(e)}\quad \hat{z}_j^{(i)} \le {a}_j^{(i)} {u}_j^{(i)}; \hat{z}_j^{(i)} \le {z}_j^{(i)} - {l}_j^{(i)}(1 - {a}_j^{(i)}); \\
    \end{aligned}
    \label{eq:mip}
\end{equation}

\noindent where $x$ is input, $y$ is output, and $z^{(i)}$, $\hat{z}^{(i)}$, $W^{(i)}$, and $b^{(i)}$ are the pre-activation, post-activation, weight, and bias vectors for layer $i$, respectively. 
This encodes the semantics of a ReLU-based DNN:  
(a) the affine transformation computing the pre-activation value for a neuron in terms of outputs in the preceding layer;
(b) the inputs and outputs in terms of the adjacent hidden layers;
(c) assertion that post-activation values are non-negative and no less than pre-activation values;
(d) neuron activation status indicator variables that are either 0 or 1; and
(e) constraints on the upper, $u_j^{(i)}$, and lower, $l_j^{(i)}$, bounds of the pre-activation value of the $j$th neuron in the $i$th layer.
Deactivating a neuron, $a_j^{(i)} = 0$, simplifies the first of the (e) constraints to $\hat{z}_j^{(i)} \le 0$, and activating a neuron simplifies the second to $\hat{z}_j^{(i)} \le z_j^{(i)}$, which is consistent with the semantics of $\hat{z}_j^{(i)} = max(z_j^{(i)},0)$.


\subsubsection{Correctness}\label{sec:checker-core-correctness}

 \autoref{fig:algorithm} returns \certified iff the input \prooflang{} proof tree is unsatisfiable. This proof tree encodes a disjunction of constraints, one per tree path, where each constraint represents an activation pattern of the network (the leaf node). The algorithm checks each constraint using LP solving and only returns certified iff every one of them is unsatisfiable.  
%Moreover, while pruning optimization (\autoref{sec:pruning}) in \autoref{fig:algorithm} allows the checker to skip children nodes if the parent node is unsatisfiable, this is still sound because the structure of the proof tree guarantees that the constraints of a child is more restricted its parent, and thus if the parent node is unsatisfiable, the children nodes must also be unsatisfiable. The algorithm  terminates because the proof tree (or the DNF) is finite and the checker will eventually check all nodes.
We note that this correctness argument assumes that the LP solver is correct -- in practice
multiple solvers could be used to guard against errors in that component.  We note that
it is standard for proof checkers to assume the correctness of a small set of external tools, e.g., checkers that use  theorem provers assume the correctness of the underlying prover~\cite{lammich2023grat}.

\subsection{Optimizations}
While the core \proofcheck{} algorithm in \autoref{fig:algorithm} is minimal, it can be inefficient. \proofcheck{} employs several optimizations to improve its efficiency. These are crucial for checking large proof trees generated by DNN verification tools for challenging problems.


\subsubsection{Neuron Stabilization}\label{sec:neuron-stabelization} 

\begin{algorithm}[t]
    \footnotesize

    \Input{DNN $\mathcal{N}$, property $\phi_{in} \Rightarrow \phi_{out}$, parallel factor $k$}
    \Output{MILP $\model$}
    \BlankLine
    

    $\model \leftarrow \InputMILP(\phi_{in})$  \tcp{input property}    \label{line:create_input}

    \tcp{Add MILP constraints for each layer of network}
    \For{$\layer ~\In ~\mathcal{N}$}{
        \If{$\isPiecewiseLinear(\layer)$}{
            \tcp{add constraints \autoref{eq:mip} (c), (d), (e)} 
            $\model \leftarrow \PiecewiseLinearMILP(\layer, \phi_{in}, \phi_{out})$ \\ \label{line:create_pwl_layer}
        }
        \Else(\tcp*[h]{this layer is linear}){
            \tcp{add constraints \autoref{eq:mip} (a), (b)} 
            $\model \leftarrow \LinearMILP(\layer, \phi_{in}, \phi_{out})$ \\ \label{line:create_linear_layer}

            \tcp{estimate upper and lower bounds} 
            $\layerbounds \gets \EstimateBounds(\layer)$ \\ \label{line:estimate_bounds}
            
            \tcp{select unstable neurons to be stabilized} 
            $[v_1, ..., v_k] \leftarrow \GetUnstableNeurons(\layerbounds)$ \\ \label{line:find_unstable}
        
            \tcp{stabilize selected neurons in parallel}
            \Parfor{$v_i ~\In~[v_1, ..., v_k]$}{ \label{line:parallel_stabilize}
                % \tcp{lower is closer to 0 than upper}
                \tcp{optimize lower first}
                \If{$(v_i.lower + v_i.upper) \ge 0$}{
                    $\Maximize(\model, v_i.lower)$ \\ \label{line:maximize1}
                    \If(\tcp*[h]{still unstable}){$v_i.lower < 0$}{
                        $\Minimize(\model, v_i.upper)$ \label{line:minimize1}
                    }
                }
                % \tcp{upper is closer to 0 than lower}
                % \tcp{optimize upper first}
                \Else(\tcp*[h]{optimize upper first}){
                    $\Minimize(\model, v_i.upper)$ \\ \label{line:minimize2}
                    \If(\tcp*[h]{still unstable}){$v_i.upper > 0$}{
                        $\Maximize(\model, v_i.lower)$  \label{line:maximize2}
                    }
                }
            }
        }
    }

    $\model \gets \AddObjective(\model, \phi_{out})$ \tcp{output property}
    
    \Return{$\model$}
    
    % \vspace*{-0.2in}
    \caption{\texttt{CreateStabilizedMILP} procedure.}
    \label{fig:stabilize}
    % \Description{}
\end{algorithm}

A primary challenge in DNN analysis is the presence of large numbers of piecewise-linear constraints (e.g., ReLU) which generate a large number of branches and yield large proof trees. In the MILP formulation, this creates many disjunctions which are hard to solve. To reduce the number of disjunctions, \proofcheck{} uses \emph{neuron stabilization}~\cite{duong2024harnessing}
to determine neurons that are \emph{stable}, either active or inactive, for all inputs defined by the property pre-condition.
For all stable neurons, the disjunctive ReLU constraint is replaced with a linear constraint that represents the neuron's value.   This simplifies the MILP problem.

\proofcheck{} uses the algorithm in \autoref{fig:stabilize} to traverse the DNN and compute stable neurons. The algorithm initializes the MILP model with input constraints (\autoref{line:create_input}) and then iterates over each layer of the network. 
%follows the~\autoref{eq:mip} to create corresponding 
Next, for each layer, it creates constraints (\autoref{line:create_pwl_layer} or \autoref{line:create_linear_layer}) depending on the layer type.
Moreover,  it uses approximation to estimate bounds of neuron values to determine neuron stability (\autoref{line:estimate_bounds}).
Next, it filters unstable neurons (\autoref{line:find_unstable}) and attempt to make them stable by optimizing either their lower ($\Maximize$) or upper ($\Minimize$) bounds.



\subsubsection{Pruning Leaf Nodes}\label{sec:pruning}
%\hd{``Pruning'' sounds better}

Another optimization \proofcheck{} employs is that it does not check child nodes if the parent node is unsatisfiable.
In an \prooflang{} proof tree, a child node adds constraints to the parent (e.g., node 6 adds the constraint of $v_1$ to node 4, which adds the constraint of $v_2$ to node 2 in \autoref{fig:example}b). Thus, if we determine that the constraint of the parent is unsatisfiable, we can skip the child nodes, which must also be unsatisfiable.  

%Simply checking parent nodes would actually increase MILP cost, because they have shorter tree paths and thus have a greater number of disjunctive constraints.

\proofcheck{} uses a backtracking mechanism to check the parent node only when the child nodes are infeasible. Specifically, it starts checking a leaf node $l$. If it determines unsatisfiability it will check the parent $p$ of $l$. If $p$ is unsatisfiable it immediately removes the children of $p$ (more specifically the sibling of $l$). Next it backtracks to the parent of $p$ and repeats until meeting a stopping criteria. This optimization reduces the number of LP problems that need to be solved, making the proof checking process more efficient. 

We implement a backjumping strategy that allows for backtracking multiple levels, $N$, rather than a single level at a time.
A large value of $N$ offers the chance for greater pruning if an unsatisfiable node is found by
backjumping, but such nodes also represent less constrained, and therefore, more complex MILP problems
and are less likely to be unsatisfiable.
The default value in $\proofcheck{}$ is $N=2$ is selected to enable a modest degree of pruning,
while being close enough to a proven unsatisfiable node that it has a reasonable chance of itself being unsatisfiable.
Future work will explore tuning $N$ to a given verification problem.


% \hd{Without X, \proofcheck{} will have to check all the children.}

% \hd{It backjumps when the leave node is proved and X optimization is enabled. 
% We can keep backing up until it cannot prove anymore, however, after N times, each time half the length of an activation pattern, an activation pattern is shortened to $1 / 2^{N}$ (e.g., if $N=2$, after 2 times of backing up, an activation pattern is shortened to 1/4 of its length). 
% This is when the pruned leaves by those proved interiors are getting overlapped (due to parallelization, we backjump on multiple leaves), so more backjumps might not helpful. 
% Instead, we start over with a new iteration with remaining leaves.
% I experienced with $N = \{1, 2, 3, 4, 5\}$ and I settled down $N=2$ for all experiments (fixed N) which seems to be good enough in our experiments. 
% Increasing $N$ does not improve the performances but hurts especially when dealing with CNNs.
% }
% \matt{I think my main point is ``How much of this is important to \proofcheck{}?'' whatever is important needs to be described.  You don't need to describe all of the discarded alternatives.  If we settled on a particular strategy, then define that strategy and say why it works well, e.g., balances cost of failing to prove unsat for parents with ability to prune sub-trees.  Right now we just say "simple backtracking mechanism", but your description makes seem like it is not all that simple.}
% This optimization reduces the number of LP problems that need to be solved, making the proof checking process more efficient.

%. \proofcheck{} thus uses a simple backtracking mechanism to check the parent node only when the child nodes are infeasible. This optimization reduces the number of LP problems that need to be solved, making the proof checking process more efficient.
%However, checking the parent node is more expensive than checking the child nodes (otherwise we can just directly check the root node). \proofcheck{} thus uses a simple backtracking mechanism to check the parent node only when the child nodes are infeasible. This optimization reduces the number of LP problems that need to be solved, making the proof checking process more efficient.

\subsubsection{Parallelization} Finally, the structure of \prooflang{} proof tree is designed to be easily parallelized.  Each tree path is an independent sub-proof and partitions of the tree allow checker to leverage multiprocessing to check large proof trees efficiently. \proofcheck{} uses a parameter $k$ to control the number of leaf nodes to be checked in parallel.


\subsubsection{Correctness} These three optimizations preserve the correctness of the \proofcheck{} algorithm (\autoref{sec:checker-core-correctness}). First, neuron stabilization just simplifies the MILP problem and removes irrelevant, i.e., stable, neurons, which by definition are not affected by activation patterns. Second, while backtracking allows the checker to skip some nodes, this is sound because the structure of the proof tree guarantees that the constraints of a child is more restrictive than its parent, and thus if the parent is unsatisfiable, the children must also be. Finally, parallelization is sound because each tree path is independent and the checker will eventually check all nodes.

%\subsection{The \proofcheck{} Algorithm and Implementation}


\section{Evaluation}\label{sec:evaluation}  
Our goals are to understand how checking of \prooflang{}s performs, how it can be optimized, and how robust checking is to
verification optimizations.
We focus our evaluation on the following research questions:

\noindent\mbox{~~}\textbf{RQ1} (\autoref{sec:rq1}): How does \proofcheck{} perform and how does it compare prior work? 

\noindent\mbox{~~}\textbf{RQ2} (\autoref{sec:rq2}): How does proof checking optimization improve performance?

\noindent\mbox{~~}\textbf{RQ3} (\autoref{sec:rq3}): How does proof checking performance vary with verification algorithms and optimizations?



\subsection{Experimental Design}\label{sec:exp_design}
We describe the selection of benchmarks, baselines, metrics, and treatments used
to explore the above RQs.

\ignore{
\begin{table*}[t]
    \footnotesize
    \centering
    \caption{Benchmark instances. U: \texttt{unsat}, S: \texttt{sat}, ?: \texttt{unknown}.}\label{tab:benchmarks}
    % \vspace*{-3mm}
    % \resizebox{\textwidth}{!}{
    \begin{tabular}{c|ccrr|cc}
        \toprule
        % \multirow{2}{*}{\textbf{Benchmarks}} &\multicolumn{3}{c}{\textbf{Networks}}  &\multicolumn{2}{|c}{\textbf{Tasks}}\\
        % \multirow{2}{*}{\textbf{Benchmarks}} &\multicolumn{2}{c}{\textbf{Networks}} &  \multicolumn{2}{c|}{\textbf{Per Network}} &\multicolumn{2}{c}{\textbf{Tasks}}\\
        \multirow{2}{*}{\textbf{Benchmarks}} &\multicolumn{4}{c}{\textbf{Networks}} & \multicolumn{2}{|c}{\textbf{Tasks}}\\
        & Type & Networks & Neurons & Parameters & Properties & Instances (U/S/?)\\
  
        \midrule
        \multirow{4}{*}{FNN\_SMALL}         & \multirow{4}{*}{FNN + ReLU} & 2 x FNN(32) &   64 & 27K & 25 & 25/0/0 \\
                                            &                             & 4 x FNN(32) &  128 & 29K & 25 & 25/0/0 \\
                                            &                             & 2 x FNN(64) &  128 & 55K & 25 & 25/0/0 \\
                                            &                             & 4 x FNN(64) &  256 & 63K & 25 & 25/0/0 \\
        
        \midrule
        \multirow{4}{*}{FNN\_MEDIUM}        & \multirow{4}{*}{FNN + ReLU} & 2 x FNN(256)  &    512 &  269K & 25 & 25/0/0 \\
                                            &                             & 4 x FNN(256)  &   1024 &  401K & 25 & 25/0/0 \\
                                            &                             & 6 x FNN(256)  &   1536 &  532K & 25 & 25/0/0 \\
                                            &                             & 6 x FNN(512)  &   3072 & 1.7M & 25 & 25/0/0 \\
        \midrule
        \multirow{4}{*}{CNN\_SMALL}         & \multirow{4}{*}{CNN + ReLU} & 1 x CNN(3, 21), 1 x FNN(128)  &   320 & 27K & 25 & 25/0/0 \\
                                            &                             & 1 x CNN(3, 19), 1 x FNN(128)  &   428 & 41K & 25 & 25/0/0 \\
                                            &                             & 1 x CNN(3, 17), 1 x FNN(128)  &   560 & 58K & 25 & 25/0/0 \\
                                            &                             & 1 x CNN(3, 13), 1 x FNN(128)  &   896 & 100K & 25 & 25/0/0 \\
        \midrule
        \multirow{4}{*}{CNN\_MEDIUM}        & \multirow{4}{*}{CNN + ReLU} & 1 x CNN(3, 9), 1 x CNN(5, 11), 1 x FNN(128)  &   1828 & 67K & 25 & 25/0/0 \\
                                            &                             & 1 x CNN(3, 7), 1 x CNN(5, 9),  1 x FNN(128)  &   2560 & 128K & 25 & 25/0/0 \\
                                            &                             & 1 x CNN(5, 9), 1 x CNN(7, 11), 1 x FNN(128)  &   2828 & 96K & 25 & 25/0/0 \\
                                            &                             & 1 x CNN(5, 7), 1 x CNN(7, 9),  1 x FNN(128)  &   3920 & 180K & 25 & 25/0/0 \\
        \midrule
        \textbf{Total}                      &                             & \textbf{16}                  &     &   & \textbf{400} & \textbf{400/0/0} \\
        \bottomrule
    \end{tabular}
    % }
\end{table*}
}


\paragraph{Benchmarks}
Evaluating these research questions requires neural network verification problems
that are valid, i.e., their formulation as satisfiability problems is UNSAT.

The yearly VNN-COMP competitions~\cite{brix2023first,brix2023fourth,bak2021second} includes a wide-variety of benchmarks,
but the competitions organizers observed that many benchmarks are too easy.
They contain large numbers SAT problems that can be solved by adversarial attacks 
or UNSAT problems that can be solved, without any state splitting, using coarse overapproximations.
They conclude that there is a need 
for benchmarks to be ``not so easy that every tool can solve all of them''~\cite{brix2023first}.
For this reason, we base our benchmark selection on a recent paper~\cite{duong2024harnessing} that developed harder
UNSAT benchmarks including:
ACAS Xu, RESNET\_A/B, CIFAR2020, MNISTFC, and MNIST\_GDVB.

We selected problems from these based on the applicability of our
current \proofcheck{} implementations.

We chose not to use ACAS Xu, because those problems have a very small
input dimension (5 dimensions) which triggers a divide-and-conquer problem
decomposition optimization in verifiers, like \neuralsat{} and \crown{}.
This results in a set of relatively small proofs, each of which
can be expressed in \prooflang{}, but whose simplicity do not add
value beyond the selected benchmarks discussed below.

We chose not to use the RESNET benchmarks because  \proofcheck{} does not currently handle residual blocks.  This is an engineering limitation and there is no fundamental reason the checking algorithm is not applicable.  

Of the CIFAR2020 networks, one contains BatchNorm layers which are not currently
supported by \proofcheck{}, but we note that adding this support is a straightforward
engineering issue.
The remaining CIFAR2020 benchmarks contain very large numbers of convolutional
kernels, which generates complex constraints that can be challenging to solve.
We adapt the smallest of the CIFAR2020 benchmarks, by varying the size of the
convolutional kernels and the number of CNN layers, to produce 
the CNN benchmark in the second row of \autoref{tab:benchmarks}.
\begin{table}[t]
    \footnotesize
    \centering
    \caption{Benchmarks consist of a 8 neural networks comprised of varying numbers of CNN (C) and FNN (F) layers, neurons, and parameters, each paired with 25 properties to form verification instances that have been confirmed to be UNSAT.}\label{tab:benchmarks}
    % \vspace*{-3mm}
    % \resizebox{\textwidth}{!}{
    \begin{tabular}{c|cccc|c}
        \toprule
        \multirow{2}{*}{\textbf{Name}} &\multicolumn{4}{c|}{\textbf{Networks}} & \textbf{Properties}\\
        & Num. & Layers & Neurons & Param. & Num. \\

        \midrule
        FNN        & 8 & 2-6F &  64-3072 & 27K-1.7M & 200  \\
        \midrule
        CNN        & 8 & 1-2C;1F  &  320-3920 & 41K-180K & 200  \\
        \bottomrule
    \end{tabular}
    % }
\end{table}

The MNISTFC and MNIST\_GDVB benchmarks contain 41 networks and a total of 106 UNSAT verification problems.
From those we selected 8 networks with varying numbers of layers and neurons, resulting
in models spanning several orders of magnitude in terms of parameters, to produce
the FNN benchmark in the first row of \autoref{tab:benchmarks}.

For each of the networks, following the approach from~\cite{duong2024harnessing}, 
we randomly selected centerpoints and radii for local-robustness properties until we found 25 property specifications that yielded UNSAT results.  This yielded 200 properties
each for the FNN and CNN benchmarks for a total of 400 verification problems and
their associated proofs.

\paragraph{Baselines}
To the best of our knowledge there is a single prior work published
on DNN verification proof checking~\cite{desmartin2023towards}.
That checker is designed to work only with a single verifier, \marabou{}, 
unlike \proofcheck{}.

While not strictly a baseline we also use the time to verify problems
as a baseline.  This is because it is instructive to understand 
the cost of proof checking after having generated a proof.
We have adapted two verifiers: \crown{} and \neuralsat{}, to 
generate \prooflang{} proofs for this study.

For RQ1 our focus is on proof checking performance, so we compare
\marabou{} and its proof checker with a single verifier, \neuralsat{},
and \proofcheck{}.  
RQ2 explores \proofcheck{} optimizations and it uses the unoptimized
\proofcheck{} performance as a baseline.
For RQ3 our goal is to understand how well \proofcheck{} accommodates
proofs generated by different verification algorithm variants, since no
other proof checker can do this there is no baseline for comparison.

\begin{figure}[t]
    \centering
    \begin{minipage}[t]{0.4\textwidth}
        \centering  
        \includegraphics[width=\linewidth]{figure/fnn.pdf}
    \end{minipage}
    \begin{minipage}[t]{0.4\textwidth}
        \centering  
        \includegraphics[width=\linewidth]{figure/cnn.pdf}
    \end{minipage}

    % \begin{minipage}[t]{0.45\textwidth}
    %     \centering  
    %     \includegraphics[width=\linewidth]{figure/MNIST_CNN_SMALL.pdf}
    % \end{minipage}
    % \hfill
    % \begin{minipage}[t]{0.45\textwidth}
    %     \centering  
    %     \includegraphics[width=\linewidth]{figure/MNIST_CNN_MEDIUM.pdf}
    % \end{minipage}
    \caption{Cactus-plots for verifiers and proof checkers of FNN (top) and CNN (bottom) benchmarks.}
    \label{fig:cactus-plots}
\end{figure}
\paragraph{Treatments}
RQ1 compares the best performing versions of \marabou{}'s proof checker and \proofcheck{}.
For RQ2, we consider two of the three optimizations
implemented in \proofcheck{} : proof tree pruning (X), and 
proof stabilization (S).  We kept a third optimization that controls the
degree of parallelization in proof checking fixed at a value of 64 to mitigate
experimental cost; since the independence of sub-proofs means that
proof checking is amenable to linear speedup we felt this aspect of experimentation
was less valuable.
For RQ3, we use both \neuralsat{} and \crown{} to generate proofs; this
constitutes a treatment for this research question as it varies the
verification algorithm.
For each verifier, we explore a base version of the verifier and an optimization:
the stabilization optimization for \neuralsat{} and replacing
the default branch-and-bound decision heuristic with the BaBSR~\cite{bunel2020branch} heuristic
in \crown{}.

\paragraph{Metrics}
In the verification community there are two metrics commonly used to
assess performance: time to solve the problems and number of problems solved
from a benchmark.  We report them both here.

For each \proofcheck{} problem we record:
the time to verify that the problem is UNSAT, the time to generate a proof,
the time for \proofcheck{} to finish.
If the sum of these for a problem is less than a specified timeout,
1000 seconds in our evaluation, then we say the problem is ``solved''.
For verifiers run alone, a problem is solved if the verification completes
within the timeout.

For each benchmark, we provide cactus plots which plot the time for a problem on the y-axis, and the number of problems solved on the x-axis; problems are sorted on the x-axis from least to largest.
As shown in~\autoref{fig:cactus-plots}, these plots allow one to observe both the time difference between baselines and treatments (vertical distance between lines at a point on the x-axis) and the ability of techniques to solve problems (the maximum x-coordinate for a given line).

We also report the size of proof trees that are generated in \prooflang{}.
In the absence of optimizations this defines the \textit{number of sub-proofs}
that need to be checked, but with optimizations the number of sub-proofs may
be reduced, e.g., when an interior node in the tree can be proven.
The complexity of  sub-proofs
may vary significantly, so to provide a more detailed characterization we
also report \textit{MILP complexity}.  \autoref{eq:mip} defines the general form
of each MILP problem, but the problems will vary based on how many of the $a$ variables
defined in \autoref{eq:mip}(d) have a fixed value -- either 0 or 1.   When this
happens the constraint in \autoref{eq:mip}(e) are simplified.
Consequently, we measure MILP complexity as the number of neurons that do \textit{not}
have a fixed value, i.e., the number of unstable neurons.  
\ignore{This does not account for the contribution of \autoref{eq:mip}a,b which is directly
related to network size and input/output dimension.   Is there any way to measure that?}
% \hd{I see. It seems hard since "a"s and constraints do not have the same unit.}

\paragraph{Experimental Setup}
All experiments were run on a Linux machine with an AMD Threadripper 64-core 4.2GHZ CPU, 128GB RAM, and an NVIDIA GeForce RTX 4090 GPU with 24 GB VRAM. 
We used a timeout of 1000 seconds for the combined time of running the verifier,
generating the proof, and then checking that proof.

\subsection{RQ1 : Proof Checking Performance}
\label{sec:rq1}
\autoref{fig:cactus-plots} presents data on the performance
of \proofcheck{} relative to both an underlying verifier, \neuralsat{}, and prior work on neural network proof checking, \marabou{}'s proof checker.  
In cactus plots like this, lines that extend further on the x-axis
are better -- more problems solved -- and lines that are lower are better -- faster solve times.
Another way to view these is to pick a point on the x-axis where the plots for two techniques are defined and think of the areas under the two curves as the ``total cost'' to solve that number of problems.
The dashed lines in the plots show the performance of the verifier and the solid lines show the performance of the verifier, proof generation, and the proof checker.  Several configurations of
\proofcheck{} are shown, but in this RQ we draw the readers attention
to the plots for the \proofcheck{}(S+X) configurations; the rest are discussed in detail below.

The cactus plot for the FNN benchmark (top)
shows that \marabou{} and its checker are able to solve 69 problems or 35\% of the benchmark, whereas \proofcheck{} can solve 186 or 93\%.  
For the CNN benchmark (bottom) \marabou{} and its checker can solve a single benchmark, whereas \proofcheck{} can solve 177 problems or 89\%.
In total, \proofcheck{} solved 363 problems or 91\%, whereas \marabou{}  solved 70 problems or 18\% of all instances.

\begin{figure*}[t]
\begin{subfigure}{0.4\linewidth}
    \centering
    % 
    \begin{minipage}[t]{0.75\textwidth}
        \centering  
        \includegraphics[width=\linewidth]{figure/SUB_PROOFS_NONE.pdf}
    \end{minipage}
    %
    \begin{minipage}[t]{0.235\textwidth}
        \centering  
        \includegraphics[width=\linewidth]{figure/SUB_PROOFS_SX.pdf}
    \end{minipage}
    \caption{Number of sub-proofs per problem with (right) and without (left) \proofcheck{} optimizations.}
    \label{fig:sub-proofs-plots}
\end{subfigure}
\hfill
\begin{subfigure}{0.59\linewidth}
    \centering 
    %
    \begin{minipage}[t]{0.5\textwidth}
        \centering  
        \includegraphics[width=\linewidth]{figure/MILP_COMPLEXITY_NONE.pdf}
    \end{minipage}%
    \begin{minipage}[t]{0.5\textwidth}
        \centering  
        \includegraphics[width=\linewidth]{figure/MILP_COMPLEXITY_SX.pdf}
    \end{minipage}

    \caption{Number of constraints of a given complexity per problem with (right) and without (left) \proofcheck{} optimizations.}
    \label{fig:constrs-proofs-plots}
\end{subfigure}
\caption{Data on proof size and complexity.   Y-axes are log-scale due to the range of values.}
\end{figure*}

% \matt{Some of the line number references are no longer resolving below.}
The shape of these cactus plots indicates a high-degree of variability in the cost of proof
checking relative to verification.
From~\autoref{fig:algorithm} it is clear that both the number of leaves in the tree
structure,~\autoref{line:get_leaf}, and the complexity of the model to be checked,~\autoref{line:proof_check_objective1}, are factors that contribute to the cost of proof checking.
To explore those factors we plot their variation across the benchmarks when running \proofcheck{}.

\autoref{fig:sub-proofs-plots} (left) plots a histogram of the number of sub-proofs solved per verification 
problem, i.e., the number of nodes of the proof tree.
When interpreting these plots, understand that the y-axis log scale means that vertical
distances have a different meaning as you move upward in the plot.
While the vast majority of the verification problems have proof trees of fewer then 2000 leaves, but 17 of them have larger trees up to a maximum of more than 10000 leaves.
Note also that even among the smaller sized proof trees, there are some problems that cannot be solved.
This is due to complexity of solving the MILP constraints at the leaves of those proof trees.

\autoref{fig:constrs-proofs-plots} (top) plots a histogram of the number of occurrences of MILP problems of
a given complexity across the benchmarks.  Here again we see a spread in data, but unlike with the number of sub-proofs the CNN benchmarks seem to have consistently larger constraints and there is a clear bias among the unsolved problems towards larger constraint size.
To optimize proof checking, we must address both of these sources of complexity.

\begin{tcolorbox}[left=1pt,right=1pt,top=1pt,bottom=1pt]
\textbf{RQ1 Findings}: Proof checking performance varies with both the size of the proof tree and the complexity of the MILP problems at the nodes of the tree.  \proofcheck{} can solve 91\% of the problems across the benchmarks and improves on prior work which can solve less than 18\%.
% \tvn{are these numbers correct?  the 2nd paragraph of RQ1 says prior work can do 24\% and \proofcheck{} can do 92\%}
\end{tcolorbox}

\subsection{RQ2 : Proof Checking Optimizations}
\label{sec:rq2}
The performance cactus plots~\autoref{fig:cactus-plots} present an ablation
study of the 
pruning (X) and stabilization (S) optimizations of \proofcheck{}.
The trend across both benchmarks is consistent with pruning (yellow) and stabilization (blue)
 improving the number of problems solved by 5\% and 36\%, respectively, over the unoptimized
\proofcheck{} (green).
The combination of optimizations (red) improves the number of solved problems by 46\%, which is more than the sum of their individual improvements demonstrating that the methods create opportunities for one another for further optimization.
\ignore{
- neither X nor S: 168 (F) + 88 (C) = 256
- X: 175 (F) + 93 (C) = 268
- S: 178 (F) + 170 (C) = 348
}

The \autoref{fig:sub-proofs-plots} (right) and \autoref{fig:constrs-proofs-plots} (right) explore the impact of the S and X optimizations on the number of sub-proofs and MILP complexity.  Across the benchmarks optimizations reduce 
the number of sub-proofs is to less than 1000 and
MILP complexity to less than 2000.   
The reduction in sub-proofs directly contributes to the increase
in performance of \proofcheck{}, but the reduction in
MILP complexity is more subtle.   
Integer programming, and thus MILP, is known to be
NP-Hard in general~\cite{garey1979computers}.
The stabilization optimization addresses this complexity by
calculating sets of variables that are forced to take on specific
values based on other constraints in the MILP problem.  For each
such variable, the constraints associated with it is effectively
eliminated.  We can observe this in comparing the left and
right of \autoref{fig:constrs-proofs-plots} where we see both
constraints of higher complexity eliminated and the peak of
the constraint distribution shifted downward from 400 to 100
constraints.  

\ignore{
Sub-problem size
- without S+X:
    + mean: 388
    + std: 1062
- with S+X
    + mean: 142
    + std: 152

MILP complexity
- without S+X: 
    + mean: 329
    + std: 274
- with S+X
    + mean: 493
    + std 449

We performed an analysis of the relationship between size of constraints and number of sub-proofs and determined that these factors are not strongly correlated.
For example, there is a 1 layer CNN model with 27k parameters whose verification
generates 680 sub-proofs where the complexity of the constraints in that proof are at most 109. 
On the other end of the spectrum, verification of a 2 layer CNN model with 180k parameters only requires 81 sub-proofs, but those proofs consist of constraints with complexity of at least 1721.
}

\begin{tcolorbox}[left=1pt,right=1pt,top=1pt,bottom=1pt]
\textbf{RQ2 Findings}: The \proofcheck{} optimizations each independently increase the number of proofs that can be checked and in combination they allow
an additional 46\% of the proofs in the benchmarks to be checked.
\end{tcolorbox}

\subsection{RQ3 : Proof Checking and Verifier Optimizations}
\label{sec:rq3}
\autoref{fig:algvariation} shows cactus-plots for two configurations of \neuralsat{} and \crown{} generated proofs across the benchmarks.   The performance of the verifiers, dashed lines, differ across configurations and they are able to 
verify between 337 and 400 problems in the benchmark.
\ignore{Number of Verified/Proved problems
    + abcrown(babsr):   337/335 = 99.4%
    + abcrown:          368/366 = 99.4%
    + neuralsat(SX):    387/363 = 93.7%
    + neuralsat(S)(SX): 400/384 = 96%
}
For both of the verifiers and configurations,
\proofcheck{} is able to check between 93.7\% and 99.4\% of the proofs that are generated.
This demonstrates that the \prooflang{} is able to encode proofs
generated by differing neural network verification algorithms, and
that \proofcheck{} can check them.

% \autoref{tab:sizestats}.
\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{figure/ablation.pdf}
    \caption{\proofcheck{} performance with different verification algorithms.}
    \label{fig:algvariation}
\end{figure}

\begin{table}[t]
    \caption{Proof statistics for best verifier configurations.}
    \label{tab:sizestats}
    \centering
    \begin{tabular}{c|cc|cc}
        \toprule
         \multirow{2}{*}{\textbf{Verifier}} & \multicolumn{2}{c}{\textbf{Num. Sub-Proofs}} & \multicolumn{2}{|c}{\textbf{MILP Complexity}}\\
         & Mean & Median & Mean & Median \\ 
         \midrule
         \neuralsat{}(S) & 95 & 36 & 601 & 545 \\ 
         \midrule
         \crown{} & 230 & 180 & 414 & 179\\
         \bottomrule
    \end{tabular}
\end{table}

We performed an analysis of both the number of sub-proofs and MILP complexity for the proofs generated by the two best performing verifier
configurations.  These values follow a skewed
distribution, so we report the mean and median values in \autoref{tab:sizestats}.   One can observe variation in the
structure of the proofs generated by these verifiers.
\neuralsat{} generates smaller proof trees, but where the
MILP problems are more complex.
In contrast, \crown{} generates significantly larger proof trees,
but with less complex MILP problems.
This variation suggests potential avenues for future work, especially,
when proof checking is important.

For example, \neuralsat{} might include an option to generate larger proof trees, but with smaller MILP problems.  Such proofs would then
be amenable to higher-degrees of parallel solving and mitigate the 
performance bottleneck presented by MILP solver implementations.
One might even consider strategies that use fast verification 
options during development and then when all properties are proven, shift to slower verification options that are more amenable to proof checking.

\ignore{
1. Longer verification time means proof tree will be larger since verifier has to explore larger space
2. If an instance is verified by \crown{}, it will likely be proved by \proofcheck{}. In other words, most of timeout instances are due to \crown{} cannot verify (1st phase).

- MILP complexity:
    + neuralsat(S)(SX)  : mean=601.36, std=475.88, median=545.00, min=3, max=1944
    + neuralsat(SX)     : mean=492.40, std=448.81, median=342.00, min=2, max=1956
    + abcrown           : mean=414.35, std=447.02, median=179.00, min=0, max=1957
    + abcrown(babsr)    : mean=409.00, std=448.89, median=166.00, min=0, max=1952

They share the same MILP base model since their base models are all generated from SX setting.
neuralsat(S)(SX) has larger MILP complexity in average means that its proof trees are often shallower than others. In other words, neuralsat(S) explores smaller space than others -- which is true.

- number of sub proofs:
    + neuralsat(S)(SX): mean=95.11,   std=117.13, median=36.00,  min=0, max=829
    + neuralsat(SX)   : mean=142.00,  std=151.72, median=106.00, min=1, max=1132
    + abcrown         : mean=229.96,  std=199.23, median=180.00, min=0, max=1152
    + abcrown(babsr)  : mean=228.10,  std=170.88, median=188.00, min=0, max=857

- Number of leaves:
    + neuralsat(S)(SX): mean=236.59,   std=447.80,   median=37.50,  min=0, max=3627
    + neuralsat(SX)   : mean=902.25,   std=4957.22,  median=219.00, min=6, max=91318
    + abcrown         : mean=3007.42,  std=12742.09, median=561.00, min=0, max=138918
    + abcrown(babsr)  : mean=11714.62, std=41909.72, median=781.50, min=0, max=509765
}


\begin{tcolorbox}[left=1pt,right=1pt,top=1pt,bottom=1pt]
\textbf{RQ3 Findings}: \prooflang{} is robust to variation in different verification algorithms and \proofcheck{} is applicable to any such proof and effective in checking
the vast majority of those arising from the benchmarks.
\end{tcolorbox}


\chapter{DNN Verification Benchmarks}\label{chap:benchmarks}
Here we survey the latest benchmarks in DNN verification and how SOTA tools perform on them.  These results are taken from the Verification Neural Network Competitions (VNN-COMP)~\cite{brix2024fifth} and the recent work by Duong et al.~\cite{duong2024harnessing}.




\section{VNN-COMP'24 Benchmarks}

Artifacts for all benchmarks are available in the repository\footnote{\url{https://github.com/ChristopherBrix/vnncomp2024_benchmarks/tree/main/benchmarks}}.

               
\begin{table}[h]
    \centering
    \caption{Overview of all scored benchmarks.}\label{tab:my_label}
    \resizebox{\textwidth}{!}{
    \renewcommand{\arraystretch}{1.4}
    \begin{tabular}{ccccccc}
    \toprule
    Category &
    Benchmark &
    Application &
    Network Types &
    \# Params &
    Effective Input Dim &
    Track
    \\
    \midrule
    \multirow{8}{*}{Complex} 
    %& Carvana UNet  & Image Segmentation & Complex UNet              & 275k - 373k & 4.3k \\
    & cGAN & \makecell{Image Generation \\ \& Image Prediction} & Conv. + Vision Transformer & 500k - 68M & 5 & regular \\
    & NN4Sys & \makecell{Dataset Indexing \\ \&  Cardinality Prediction}   & ReLU + Sigmoid & 33k - 37M & 1-308 & regular \\
    & LinearizeNN & NN controller approximation & FC. + Conv. + Vision Transformer + Residual + ReLU & 203k & 4 & regular \\
    %& ml4acopf & Power System & Complex (ReLU + Trigonometric + Sigmoid) & 4k-680k & 22 - 402 & extended \\
    %& ViT & Vision & Conv. + Residual + Softmax + BatchNorm & 68k - 76k & 3072 & extended \\
    %& Collins Aerospace & - & FC + Conv. + Residual, LeakyReLU + MaxPool + Square & 1.8M & 1.2M & extended \\
    %& LSNC & Lyapunov stability of NN controllers & FC + Residual, ReLU + Sin + Cos & 210, 406 & 8 & extended \\
    %& CCTSDB & - & FC + Conv. + Residual, ReLU +  MaxPool + Clip & 100k & 2 & extended \\
    %\cmidrule(lr){1-7}
    %\multirow{7}{*}{\makecell{CNN \\ \& ResNet}} 
    % & Cifar Bias Field      & Image Classification & Conv. + ReLU  & 45k   & 16 \\
    & Collins RUL CNN & Condition Based Maintenance & Conv. + ReLU, Dropout  & 60k - 262k   & 400 - 800 & regular \\
    % & Large ResNets         & Image Classification & ResNet (Conv. + ReLU)  & 55k - 286k   & 3.1k - 12k \\
    % & Oval21 & Image Classification & Conv. + ReLU  & 3.1k - 6.2k   & 3.1k \\
    % & SRI ResNet A/B & Image Classification  & ResNet (Conv. + ReLU)  & 11k   & 3.1k \\
    %& VGGNet16 & Image Classification & Conv. + ReLU + MaxPool    & 138M & 150k & extended \\
    & Traffic Signs Recognition & Image Classification & Conv. + Sign + MakPool + BatchNorm & 905k - 1.7M & 2.7k - 12k & extended \\
    % & cifar100 & Image Classification & FC + Conv. + Residual, ReLU + BatchNorm & 2.5M - 3.8M & 3072 & regular \\
    % & tinyimagenet & Image Classification & FC + Conv. + Residual, ReLU + BatchNorm & 3.6M & 9408 & regular \\
    % & Metaroom & - & Conv. + FC, ReLU & 466k - 7.4M & 5376 & regular \\
    % & Yolo & - & FC + Conv. + Residual, ReLU + Sigmoid & 22k - 37M & 1 - 308 & extended \\
    % %
    % \cmidrule(lr){1-7} %
    % \multirow{5.5}{*}{\makecell{FC}}
    % %& MNIST FC & Image Classification & FC. + ReLU    & 512 - 1.5k & 784 \\
    % %& Reach Prob Density    & Probability density estimation & FC. + ReLU    & 64 - 192 & 3 - 14 \\
    % %& RL Benchmarks  & Reinforcement Learning & FC. + ReLU    & 128 - 512  & 4 - 8 \\
    % & TLL Verify Bench & Two-Level Lattice NN & \makecell{Two-Level Lattice NN \\(FC. + ReLU)}  & 17k - 67M & 2 & regular \\
    % & Acas XU & Collision Detection & FC. + ReLU & 13k & 5 & regular \\
    % & Dist Shift & Distribution Shift Detection & FC. + ReLU + Sigmoid & 342k - 855k & 792 & regular \\
    % & safeNLP & Sentence classification & FC. + ReLU & 4k & 30 & regular \\
    % & CORA & Image Classification & FC. + ReLU & 575k, 1.1M & 784, 3072 & regular \\
    \bottomrule
    \end{tabular}
    }
\end{table}


\subsection{cGAN}
\paragraph*{Proposed by} Feiyang Cai, Ali Arjomandbigdeli, Stanley Bak (Stony Brook Univ.). Link: \url{https://github.com/feiyang-cai/cgan_benchmark2023}

This benchmark targets robustness verification for generative modelsan area often overlooked compared to discriminative networks.
It uses conditional GANs trained to generate images of vehicles at specific distances. The generator takes a 1D distance input and a 4D noise vector; the discriminator outputs a real/fake score and a predicted distance.
Models vary in architecture (CNNs, vision transformers) and image size (3232, 6464).
The verification task checks whether the predicted distance from the generated image matches the input condition, under small input perturbations.


% \paragraph*{Motivation}
% While existing neural network verification benchmarks focus on discriminative models, the exploration of practical and widely used generative networks remains neglected in terms of robustness assessment.
% This benchmark introduces a set of image generation networks specifically designed for verifying the robustness of the generative networks.
% \paragraph*{Networks}
% The generative networks are trained using conditional generative adversarial networks (cGAN), whose objective is to generate camera images that contain a vehicle obstacle located at a specific distance in front of the ego vehicle, where the distance is controlled by the input distance condition.
% The network to be verified is the concatenation of a generator and a discriminator.  The generator takes two inputs: 1) a distance condition (1D scalar) and 2) a noise vector controlling the environment (4D vector). The output of the generator is the generated image. The discriminator takes the generated image as input and outputs two values: 1) a real/fake score (1D scalar) and 2) a predicted distance (1D scalar).
% Several different models with varying architectures (CNN and vision transformer) and image sizes (32x32, 64x64) are provided for different difficulty levels.
% \paragraph*{Specifications}
% The verification task is to check whether the generated image aligns with the input distance condition, or in other words, verify whether the input distance condition matches the predicted distance of the generated image.
% In each specification, the inputs (condition distance and latent variables) are constrained in small ranges, and the output is the predicted distance with the same center as the condition distance but with slightly larger range.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\pagebreak
\subsection{NN4Sys}
\paragraph*{Proposed by} the $\alpha,\!\beta$-CROWN team with collaborations with Cheng Tan, Haoyu He and Shuyi Lin at Northeastern University.
\paragraph*{Application}
The benchmark contains networks for database learned index, video streaming learned adaptive bitrate, and learned cardinality
estimation which map inputs from various dimensions to 1-dimension outputs.

\begin{itemize}

\item \textit{Background}: learned index, learned cardinality, and learned
    adaptive bitrate are all instances in neural networks for computer systems
        (NN4Sys), which are neural network based methods performing system
        operations. These classes of methods show great potential but have one
        drawback---the outputs of an NN4Sys model (a neural network) can be
        arbitrary, which may lead to unexpected issues in systems.

\item \textit{What to verify}: our benchmark provides multiple pairs of (1) trained NN4Sys model
and (2) corresponding specifications. We design these pairs with different parameters such
that they cover a variety of user needs and have varied difficulties for verifiers. 
We describe benchmark details in our NN4SysBench report:
        \url{http://naizhengtan.github.io/doc/papers/nn4sys23lin.pdf}.

\item \textit{Translating NN4Sys applications to a VNN benchmark}: 
the original NN4Sys applications have some sophisticated structures that are hard to verify.
We tailored the neural networks and their specifications to be suitable for VNN-COMP.
For example, learned index~\cite{kraska18case} contains multiple NNs in a tree structure that together serve one purpose.
However, this cascading structure is inconvenient/unsupported to verify
because there is a ``switch" operation---choosing one NN in the second stage
based on the prediction of the first stage's NN.
To convert learned indexes to a standard form, we train a monolithic (larger) NN.

\item \textit{A note on broader impact}: using NNs for systems is a broad topic, but many existing works
lack strict safety guarantees. We believe that NN Verification can help system developers gain confidence
to apply NNs to critical systems. We hope our benchmark can be an early step toward this vision.

\end{itemize}

\paragraph*{Networks}
This benchmark has twelve networks with different parameters: two for learned
indexes, four for learned cardinality estimation and six for learned adaptive bitrate.
The learned index uses fully-connected feed-forward neural networks. The other
two---the learned cardinality and the learned adaptive bitrate---has a
relatively sophisticated internal structure. Please see our NN4SysBench report
(URL listed above) for details


\paragraph*{Specifications}
For learned indexes,
the specification aims to check if the prediction error is bounded.
The specification is a collection of pairs of input and output intervals such that
any input in the input interval should be mapped to the corresponding output interval.
For learned cardinality estimation and learned adaptive bitrate,
the specifications check the prediction error bounds (similar to the learned indexes)
and monotonicity of the networks.
By monotonicity specifications, we mean that for two inputs, the network should produce a larger
output for the larger input, which is required by cardinality estimation or adaptive bitrate.


\paragraph{Link:} \url{https://github.com/Khoury-srg/VNNComp23_NN4Sys}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% sec: LinearizeNN %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{LinearizeNN}
\paragraph*{Proposed by}  Ali Arjomandbigdeli, Stanley Bak (Stony Brook University).
\paragraph*{Motivation}
Assuming having a neural network controller approximation with a piecewise linear model in the form of a set of linear models with added noise to account for local linearization error. The objective of this benchmark is to investigate the neural network output falls within the range we obtain from our linear model output plus some uncertainty.

The idea of this benchmark came from one of our recent paper~\cite{ArjomandBigdeli2024} in which we approximated the NN controller with a piecewise linear model, and we wanted to check if the neural network output falls within the range we obtained from our linear model output plus some uncertainty.
\paragraph*{Networks} The neural network controller we used in this benchmark is an image-based controller for an Autonomous Aircraft Taxiing System whose goal is to control an aircraft's taxiing at a steady speed on a taxiway. This network was introduced  in the paper "Verification of Image-based Neural Network Controllers Using Generative Models"~\cite{katz2021veri}. The neural network integrates a concatenation of the cGAN (conditional GAN) and controller, resulting in a unified neural network controller with low-dimensional state inputs. In this problem, the inputs to the neural network consist of two state variables and two latent variables. The aircraft's state is determined by its crosstrack position (p) and heading angle error ($\theta$) with respect to the taxiway center line. Two latent variables with a range of -0.8 to 0.8 are introduced to account for environmental changes.

Because in this case the output spec depends on both the input and output and considering the VNN-LIB limitation, we added a skip-connection layer to the neural network to have the input values present in the output space. We also added one linear layer after that to create a linear equation for each local model.
\paragraph*{Specifications} As mentioned earlier, the aim of this benchmark is to examine whether the neural network output stays within the range defined by the linear model's output, including a margin for uncertainty.Given input $x \in X$ and output $Y = f_{NN}(x)$, the query is of the form: $A_{mat}\times X + b + U_{lb} \leq Y \leq A_{mat}\times X + b + U_{ub}$ for each linear model in its abstraction region.
\paragraph*{Link} \url{https://github.com/aliabigdeli/LinearizeNN_benchmark2024}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
\pagebreak
% \subsection{Cifar Bias Field}

% \paragraph{Proposed by} The VeriNet team.

% \paragraph*{Motivation} This benchmark considers verification of a Cifar 10 network against bias field perturbations. The bias field perturbations are encoded by creating augmented networks with only 16 input parameters; thus, the problem has a significantly lower input dimensionality than many other image-based benchmarks.

% \paragraph*{Networks} For each image to be verified, a separate bias field transform network is created~\cite{Henriksen+21} which consists of the FC transform layer followed by the Cifar CNN. The Cifar CNN consists of 8 convolutional layers followed by ReLUs. Each bias field transform network has 363k parameters and 45k nodes.  

% \paragraph*{Specifications} The specification considers bias field perturbations of the input. The task is reduced to a standard $\ell_\infty$ specification by encoding bias field the bias field transformation into fully connected layers which are prepended to the network under consideration. The bias field perturbations used $\epsilon = 0.06$ (as described in~\cite{Henriksen+21}) and a timeout of 5 minutes. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% sec: ml4acopf %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{ml4acopf}
\paragraph*{Proposed by} Haoruo Zhao, Michael Klamkin, Mathieu Tanneau, Wenbo Chen, and Pascal Van Hentenryck (Georgia Institute of Technology), and Hassan Hijazi, Juston Moore, and Haydn Jones (Los Alamos National Laboratory).

\paragraph*{Motivation}
Machine learning models are utilized to predict solutions for an optimization model known as AC Optimal Power Flow (ACOPF) in the power system. Since the solutions are continuous, a regression model is employed. The objective is to evaluate the quality of these machine learning model predictions, specifically by determining whether they satisfy the constraints of the optimization model. Given the challenges in meeting some constraints, the goal is to verify whether the worst-case violations of these constraints are within an acceptable tolerance level.

\paragraph*{Networks}
The neural network designed comprises two components. The first component predicts the solutions of the optimization model, while the second evaluates the violation of each constraint that needs checking. The first component consists solely of general matrix multiplication (GEMM) and rectified linear unit (ReLU) operators. However, the second component has a more complex structure, as it involves evaluating the violation of AC constraints using nonlinear functions, including sigmoid, quadratic, and trigonometric functions such as sine and cosine. This complex evaluation component is incorporated into the network due to a limitation of the VNNLIB format, which does not support trigonometric functions. Therefore, these constraints violation evaluation are included in the neural network.

\paragraph*{Specifications}
In this benchmark, four different properties are checked, each corresponding to a type of constraint violation:
\begin{enumerate}
    \item Power balance constraints: the net power at each bus node is equal to the sum of the power flows in the branches connected to that node.
    \item Thermal limit constraints: power flow on a transmission line is within its maximum and minimum limits.
    \item Generation bounds: a generator's active and reactive power output is within its maximum and minimum limits.
    \item Voltage magnitude bounds: a voltage's magnitude output is within its maximum and minimum limits.
\end{enumerate}

The input to the model is the active and reactive load. The chosen input point for perturbation is a load profile for which a corresponding feasible solution to the ACOPF problem is known to exist. For the feasibility check, the input load undergoes perturbation. Although this perturbation does not exactly match physical laws, the objective is to ascertain whether a machine learning-predicted solution with the perturbation can produce a solution that does not significantly violate the constraints.

The scale of the perturbation and the violation threshold are altered by testing whether an adversarial example can be easily found using projected gradient descent with the given perturbation. The benchmark, provided with a fixed random seed, is robust against the simple projected gradient descent that is implemented.

\paragraph*{Link} \url{https://github.com/AI4OPT/ml4acopf_benchmark}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% sec: ViT %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{ViT}
\paragraph*{Proposed by} the $\alpha,\!\beta$-CROWN team.
\paragraph*{Motivation}
Transformers~\cite{vaswani2017attention} based on the self-attention mechanism have much more complicated architectures and contain more kinds of nonlinerities, compared to simple feedforward networks with relatively simple activation functions. 
It makes verifying Transformers challenging. We aim to encourage the development of verification techniques for Transformer-based models, and we also aim to benchmark neural network verifiers on relatively complicated neural network architectures and more general nonlinearities. Therefore, we propose a new benchmark with Vision Transformers (ViTs)~\cite{dosovitskiy2020image}. This benchmark is developed based on our work on neural network verification for models with general nonlinearities~\cite{shi2024genbab}.

\paragraph*{Networks}
The benchmark contains two ViTs, as shown in \Cref{tab:vits}.
Considering the difficulty of verifying ViTs, we modify the ViTs and make the models relatively shallow and narrow, with significantly reduced number of layers and attention heads.
Following \cite{shi2019robustness}, we also replace the layer normalization with batch normalization.
The models are mainly trained with PGD training~\cite{madry2017towards}, and we also add a weighted IBP~\cite{gowal2018effectiveness,shi2021fast} loss for one of the models as a regularization.

\begin{table}[ht]
\centering
\caption{Networks in the ViT benchmark.}
\label{tab:vits}
\begin{tabular}{ccc}
\toprule 
Model & \texttt{PGD\_2\_3\_16} & \texttt{IBP\_3\_3\_8} \\
\midrule
Layers & 2 & 3\\
Attention heads & 3 & 3\\
Patch size & 16 & 8\\
Weight of IBP loss & 0 & 0.01\\
Training $\epsilon$ & $\frac{2}{255}$ & $\frac{1}{255}$\\
Clean accuracy & 59.78\% & 62.21\%\\
\bottomrule
\end{tabular}
\end{table}

\paragraph*{Specifications} 
The specifications are generated from the robustness verification problem with $\ell_\infty$ perturbation. 
We use the CIFAR-10 dataset with perturbation size $\epsilon=\frac{1}{255}$ at test time.
We have filtered the CIFAR-10 test set to exclude instances where either adversarial examples can be found (by PGD attack~\cite{madry2017towards} with 100 steps and 1000 restarts) or the vanilla CROWN-like method~\cite{zhang2018efficient,shi2019robustness} can already easily verify. 
We randomly keep 100 instances for each model, with a timeout threshold of 100 seconds. 
Note that since instances with adversarial examples have mostly been excluded during the filtering process, this version of the benchmark may not be able to reflect soundness issues in verifiers, and we refer readers to \cite{zhou2024testing} for discussions on testing soundness with models including ViT.

\paragraph*{Link} \url{https://github.com/shizhouxing/ViT_vnncomp2023}


\subsection{LSNC}

\paragraph*{Proposed by} the $\alpha,\!\beta$-CROWN team.
\paragraph*{Motivation}
We develop a benchmark for the problem of verifying the Lyapunov stability of NN controllers in nonlinear dynamical systems within a region-of-intrest and a region-of-attraction. This is important for providing stability guarantees that are essential for safety-critical applications with NN controllers. It is also a useful application of neural network verification as recently demonstrated in \cite{yang2024lyapunov,shi2024certified}, and we refer readers to those works for more details on the problem.
\paragraph*{Networks and Specifications}
Models are adopted from \cite{yang2024lyapunov}. 
We adopt two models for the 2D quadrotor dynamical system with state feedback and output feedback, respectively. Each model consists of a controller which is a shallow ReLU network, a Lyapunov function which is a quadratic function, and nonlinear operators modelling the dynamics of a 2D quadrotor. The model for output feedback further consists of a shallow LeakyReLU network as the observer. The verification objective of the Lyapunov stability has been encoded in the ONNX graphs and VNNLIB specifications. Specifications for the benchmark are randomly generated and consist of random sub-regions within the original region-of-interest. The size of the random sub-regions is controlled by a factor $\epsilon~(0<\epsilon\leq 1)$ which is applied to each input dimension, and it has been adjusted for a suitable difficulty given the timeout. For the state feedback model, we set $\epsilon=0.5$ and the timeout is 100s; for the output feedback model, we set $\epsilon=0.3$ and timeout is 200s. For each of the two models, we randomly generate 20 instances. 
\paragraph*{Link} \url{https://github.com/shizhouxing/LSNC_VNNCOMP2024}

\subsection{Collins-RUL-CNN}
\paragraph*{Proposed by} Collins Aerospace, Applied Research \& Technology (\href{https://www.collinsaerospace.com/what-we-do/capabilities/technology-and-innovation/applied-research-and-technology}{website}).

\paragraph*{Motivation} Machine Learning (ML) is a disruptive technology for the aviation industry. This particularly concerns safety-critical aircraft functions, where high-assurance design and verification methods have to be used in order to obtain approval from certification authorities for the new ML-based products. Assessment of correctness and robustness of trained models, such as neural networks, is a crucial step for demonstrating the absence of unintended functionalities~\cite{ForMuLA, kirov2023formal}. The key motivation for providing this benchmark is to strengthen the interaction between the VNN community and the aerospace industry by providing a realistic use case for neural networks in future avionics systems~\cite{kirov2023benchmark}.

\paragraph*{Application} Remaining Useful Life (RUL) is a widely used metric in Prognostics and Health Management (PHM) that manifests the remaining lifetime of a component (e.g., mechanical bearing, hydraulic pump, aircraft engine). RUL is used for Condition-Based Maintenance (CBM) to support aircraft maintenance and flight preparation. It contributes to such tasks as augmented manual inspection of components and scheduling of maintenance cycles for components, such as repair or replacement, thus moving from preventive maintenance to \emph{predictive} maintenance (do maintenance only when needed, based on components current condition and estimated future condition). This could allow to eliminate or extend service operations and inspection periods, optimize component servicing (e.g., lubricant replacement), generate inspection and maintenance schedules, and obtain significant cost savings. Finally, RUL function can also be used in airborne (in-flight) applications to dynamically inform pilots on the health state of aircraft components during flight. Multivariate time series data is often used as RUL function input, for example, measurements from a set of sensors monitoring the component state, taken at several subsequent time steps (within a time window). Additional inputs may include information about the current flight phase, mission, and environment. Such highly multi-dimensional input space motivates the use of Deep Learning (DL) solutions with their capabilities of performing automatic feature extraction from raw data.

\paragraph*{Networks} The benchmark includes 3 convolutional neural networks (CNNs) of different complexity: different numbers of filters and different sizes of the input space. All networks contain only convolutional and fully connected layers with ReLU activations. All CNNs perform the regression function. They have been trained on the same dataset (time series data for mechanical component degradation during flight).

\paragraph*{Specifications} We propose 3 properties for the NN-based RUL estimation function. First, two properties (robustness and monotonicity) are local, i.e., defined around a given point. We provide a script with an adjustable random seed that can generate these properties around input points randomly picked from a test dataset. For robustness properties, the input perturbation (delta) is varied between 5\% and 40\%, while the number of perturbed inputs varies between 2 and 16. For monotonicity properties, monotonic shifts between 5\% and 20\% from a given point are considered. Properties of the last type ("if-then") require the output (RUL) to be in an expected value range given certain input ranges. Several if-then properties of different complexity are provided (depending on range widths).

\paragraph*{Link} \url{https://github.com/loonwerks/vnncomp2022}

\paragraph*{Paper} Available in~\cite{kirov2023benchmark} or on request.


\subsection{VGGNET16}
\paragraph*{Proposed by} Stanley Bak, Stony Brook University

\paragraph*{Motivation} This benchmark tries to scale up the size of networks being analyzed by using the well-studied VGGNET-16 architecture~\cite{simonyan2014very} that runs on ImageNet. Input-output properties are proposed on pixel-level perturbations that can lead to image misclassification. 

\paragraph*{Networks} All properties are run on the same network, which includes 138 million parameters. The network features convolution layers, ReLU activation functions, as well as max pooling layers.

\paragraph*{Specifications} Properties analyzed ranged from single-pixel perturbations to perturbations on all 150528 pixles (L-infinity perturbations). A subset of the images was used to create the specifications, one from each category, which was randomly chosen to attack. Pixels to perturb were also randomly selected according to a random seed.

\paragraph*{Link} \url{https://github.com/stanleybak/vggnet16_benchmark2022/}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% sec: Traffic Signs Recognition %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Traffic Signs Recognition}
\paragraph*{Proposed by} M\u{a}d\u{a}lina Era\c{s}cu and Andreea Postovan (West University of Timisoara, Romania)
\paragraph*{Motivation} Traffic signs play a crucial role in ensuring road safety and managing traffic flow in both city and highway driving. The recognition of these signs, a vital component of autonomous driving vision systems, faces challenges such as susceptibility to adversarial examples~\cite{szegedy2013intriguing} and occlusions~\cite{zhang2020lightweight}, stemming from diverse traffic scene conditions.

\paragraph*{Networks} Binary neural networks (BNNs) show promise in computationally limited and energy-constrained environments within the realm of autonomous driving~\cite{hubara2016binarized}. BNNs, where weights and/or activations are binarized to $\pm 1$, offer reduced model size and simplified convolution operations for image recognition compared to traditional neural networks (NNs).

We trained and tested various BNN architectures using the German Traffic Sign Recognition Benchmark (GTSRB) dataset~\cite{GTSRB}. This multi-class dataset, containing images of German road signs across 43 classes, poses challenges for both humans and models due to factors like perspective change, shade, color degradation, and lighting conditions. The dataset was also tested using the Belgian Traffic Signs \cite{BelgianTrafficSignDatabase} and Chinese Traffic Signs \cite{ChineseTrafficSignDatabase} datasets. The Belgium Traffic Signs dataset, with 62 classes, had 23 overlapping classes with GTSRB. The Chinese Traffic Signs dataset, with 58 classes, shared 15 classes with GTSRB. Pre-processing steps involved relabeling classes in the Belgium and Chinese datasets to match those in GTSRB and eliminating non-overlapping classes (see \cite{postovan2023architecturing} for details).

We provide three models with the structure in Figures \ref{fig:Acc-Efficient-Arch-GTSRB-Belgium}, \ref{fig:Acc-Efficient-Arch-Chinese}, and \ref{fig:XNOR(QConv)-arch}. They contain QConv, Batch Normalization (BN), Max Pooling (ML), Fully Connected/Dense (D) layers.  Note that the QConv layer binarizes the corresponding convolutional layer. All models were trained for 30 epochs. The model from Figure \ref{fig:Acc-Efficient-Arch-GTSRB-Belgium} was trained with images having the dimension 64px x 64 px, the one from Figure \ref{fig:Acc-Efficient-Arch-Chinese} with 48px x 48 px and the one from Figure \ref{fig:XNOR(QConv)-arch} with 30px x 30 px. The two models involving Batch Normalization layers introduce real valued parameters besides the binary ones, while the third one contains only binary parameters (see Table \ref{tab:stats}) for statistics.

\begin{figure}[h]
  \centering
    \includegraphics[width=0.7\textwidth]{figure/AccEfficientGTSRBBelgium.png}
    \caption{Accuracy Efficient Architecture for GTSRB and Belgium dataset}
    \label{fig:Acc-Efficient-Arch-GTSRB-Belgium}
\end{figure}

\begin{figure}[h]
  \centering
    \includegraphics[width=0.7\textwidth]{figure/AccEfficientArchChinese.png}
    \caption{Accuracy Efficient Architecture for Chinese dataset}
    \label{fig:Acc-Efficient-Arch-Chinese}
\end{figure}

\begin{figure}[h]
  \centering
    \includegraphics[width=0.3\textwidth]{figure/XNORQConv.png}
    \caption{XNOR(QConv) architecture}
    \label{fig:XNOR(QConv)-arch}
\end{figure}

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table}[h]
\caption{Training and Testing Statistics}
\label{tab:stats}
\centering
\scriptsize
\begin{tabular}{|c|c|ccc|ccc|}
\hline
\multirow{2}{*}{\textbf{Input size}} & \multirow{2}{*}{\textbf{Model name}} & \multicolumn{3}{c|}{\textbf{Accuracy}}                                      & \multicolumn{3}{c|}{\textbf{\#Params}}                                      \\ \cline{3-8} 
                            &                             & \multicolumn{1}{c|}{\textbf{German}} & \multicolumn{1}{c|}{\textbf{China}} & \textbf{Belgium} & \multicolumn{1}{c|}{\textbf{Binary}}  & \multicolumn{1}{c|}{\textbf{Real}} & \textbf{Total}   \\ \hline
64px $\times$ 64px          & Figure \ref{fig:Acc-Efficient-Arch-GTSRB-Belgium}                  & \multicolumn{1}{c|}{96.45}  & \multicolumn{1}{c|}{81.50} & 88.17   & \multicolumn{1}{c|}{1772896} & \multicolumn{1}{c|}{2368} & 1775264 \\ \hline
48px $\times$ 48px          & Figure \ref{fig:Acc-Efficient-Arch-Chinese}                  & \multicolumn{1}{c|}{95.28}  & \multicolumn{1}{c|}{83.90} & 87.78   & \multicolumn{1}{c|}{904288}  & \multicolumn{1}{c|}{832}  & 905120  \\ \hline
30px $\times$ 30px          & Figure \ref{fig:XNOR(QConv)-arch}                  & \multicolumn{1}{c|}{81.54}  & \multicolumn{1}{c|}{N/A}   & N/A     & \multicolumn{1}{c|}{1005584} & \multicolumn{1}{c|}{0}    & 1005584 \\ \hline
\end{tabular}
\end{table}
\paragraph*{Specifications} To evaluate the \emph{adversarial robustness} of the networks above, we assessed perturbations within the infinity norm around zero, with the radius denoted as $\epsilon = \{1, 3, 5, 10, 15\}$. This involved randomly selecting three distinct images from the GTSRB dataset's test set for each model and generating \textsc{VNNLIB} files for each epsilon in the set. In total, we created 45 \textsc{VNNLIB} files. Due to a 6-hour total timeout constraint for solving all instances, each instance had a maximum timeout of 480 seconds. To review the generated \textsc{VNNLIB} specification files submitted to VNNCOMP 2023, as well as to generate new ones, please refer to \url{https://github.com/apostovan21/vnncomp2023}.

\paragraph*{Link} \url{https://github.com/apostovan21/vnncomp2023}



\subsection{CIFAR100}

\paragraph*{Proposed by} the $\alpha,\!\beta$-CROWN team.
\paragraph*{Motivation} This benchmark is reused from VNN-COMP 2022 with a reduced complexity (only two out of the four models with medium sizes are retained). 
See details in Section 4.5 of the report of VNN-COMP 2022~\cite{muller2022vnncomp}.

\paragraph*{Networks} We provide two ResNet models on CIFAR-100 with different model widths and depths (input dimension $32 \times 32 \times 3$, 100 classes):
\begin{itemize}
    \item \texttt{CIFAR100-ResNet-medium}: 8 residual blocks, 17 convolutional layers + 2 linear layers
    \item \texttt{CIFAR100-ResNet-large}: 8 residual blocks, 19 convolutional layers + 2 linear layers (almost identical to standard ResNet-18 architecture)
\end{itemize}

\paragraph*{Specifications} 
We randomly select 100 images from the CIFAR-100 test set with a verification timeout of 100 seconds for each of the two models. 
We filtered out the samples which can be verified by vanilla CROWN (which is used during training) to make the benchmark more challenging. The filtering process is done offline on a machine with a GPU due to the large sizes of these models. 
A small proportion of instances (around 18\%) with adversarial examples have been retained for potentially identifying unsound results. 

\paragraph*{Link} \url{https://github.com/huanzhang12/vnncomp2024_cifar100_benchmark}


\subsection{TinyImagenet}

\paragraph*{Proposed by} the $\alpha,\!\beta$-CROWN team.
\paragraph*{Motivation} This benchmark is reused from VNN-COMP 2022. See details in Section 4.5 of the report of VNN-COMP 2022~\cite{muller2022vnncomp}.

\paragraph*{Networks} We provide a ResNet for TinyImageNet (input dimension $64 \times 64 \times 3$, 200 classes):
\begin{itemize}
    \item \texttt{TinyImageNet-ResNet-medium}: 8 residual blocks, 17 convolutional layers + 2 linear layers
\end{itemize}

\paragraph*{Specifications} We randomly select 200 images from the TinyImageNet test set with a verification timeout of 100 seconds for each of the two models. A filtering procedure has been adopted similar to the CIFAR100 benchmark.

\paragraph*{Link} \url{https://github.com/huanzhang12/vnncomp2024_tinyimagenet_benchmark}



\subsection{TLL Verify Bench}
\paragraph*{Proposed by} James Ferlez (University of California, Irvine)

\paragraph*{Motivation} This benchmark consists of Two-Level Lattice (TLL) NNs, which have been shown to be amenable to fast verification algorithms (e.g. \cite{FerlezKS22}). Thus, this benchmark was proposed as a means of comparing TLL-specific verification algorithms with general-purpose NN verification algorithms (i.e. algorithms that can verify arbitrary deep, fully-connected ReLU NNs).

\paragraph*{Networks}  The networks in this benchmark are a subset of the ones used in \cite[Experiment 3]{FerlezKS22}. Each of these TLL NNs has $n=2$ inputs and $m=1$ output. The architecture of a TLL NN is further specified by two parameters: $N$, the number of local linear functions, and $M$, the number of selector sets. This benchmark contains TLLs of sizes $N = M = 8, 16, 24, 32, 40, 48, 56, 64$, with $30$ randomly generated examples of each (the generation procedure is described in \cite[Section 6.1.1]{FerlezKS22}). At runtime, the specified verification timeout determines how many of these networks are included in the benchmark so as to achieve an overall 6-hour run time; this selection process is deterministic. Finally, a TLL NN has a natural representation using multiple computation paths \cite[Figure 1]{FerlezKS22}, but many tools are only compatible with fully-connected networks. Hence, the ONNX models in this benchmark implement TLL NNs by ``stacking'' these computation paths to make a fully connected NN (leading to sparse weight matrices: i.e. with many zero weights and biases). The \texttt{TLLnet} class (\url{https://github.com/jferlez/TLLnet}) contains the code necessary to generate these implementations via the \texttt{exportONNX} method.

%\vfill
%\pagebreak
\paragraph*{Specifications}  All specifications have as input constraints the hypercube $[-2,2]^2$. Since all networks have only a single output, the output properties consist of a randomly generated real number and a randomly generated inequality direction. Random output samples from the network are used to roughly ensure that the real number property has an equal likelihood of being within the output range of the NN and being outside of it (either above or below all NN outputs on the input constraint set). The inequality direction is generated independently and with each direction having an equal probability. This scheme biases the benchmark towards verification problems for which counterexamples exist. 

\paragraph*{Link} \url{https://github.com/jferlez/TLLVerifyBench}
\paragraph*{Commit}
199d2c26d0ec456e62906366b694a875a21ff7ef


%%%%%%% Unscored %%%%%%%
\subsection{ACAS Xu}
\paragraph{Networks} The ACASXu benchmark consists of ten properties defined over 45 neural networks used to issue turn advisories to aircraft to avoid collisions. The neural networks have 300 neurons arranged in 6 layers, with ReLU activation functions. There are five inputs corresponding to the aircraft states, and five network outputs, where the minimum output is used as the turn advisory the system ultimately produces.

\paragraph{Specifications} We use the original 10 properties~\cite{katz2017reluplex}, where properties 1-4 are checked on all 45 networks as was done in later work by the original authors~\cite{katz2019marabou}. Properties 5-10 are checked on a single network. The total number of benchmarks is therefore 186. The original verification times ranged from seconds to days---including some benchmark instances that did not finish. This year we used a timeout of around two minutes (116 seconds) for each property, in order to fit within a total maximum runtime of six hours.

% \subsection{Cifar2020 (unscored)}

% \paragraph*{Motivation} This benchmark combines two convolutional CIFAR10 networks from last year's VNN-COMP 2020 with a new, larger network with the goal to evaluate the progress made by the whole field of Neural Network verification.

% \paragraph*{Networks} The two ReLU networks \texttt{cifar\_10\_2\_255} and \texttt{cifar\_10\_8\_255} with two convolutional and two fully-connected layers were trained for $\ell_\infty$ perturbations of $\epsilon = \frac{2}{255}$ and $\frac{8}{255}$, respectively, using COLT \cite{balunovic:20}  and the larger \texttt{ConvBig} with four convolutional and three fully-connected networks, was trained using adversarial training \cite{madry:17} and $\epsilon = \frac{2}{255}$.

% \paragraph*{Specifications} We draw the first 100 images from the CIFAR10 test set and for every network reject incorrectly classified ones. For the remaining images, the specifications describe a correct classification under an $\ell_\infty$-norm perturbation of at most $\frac{2}{255}$ and $\frac{8}{255}$ for \texttt{cifar\_10\_2\_255} and \texttt{ConvBig} and \texttt{cifar\_10\_8\_255}, respectively and allow a per sample timeout of 5 minutes.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{safeNLP}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\columnwidth]{./figure/NLP-Flow.png}
    \caption{\small\emph{Generic approach to generating the NLP verification pipelines~\cite{casadio2023antonio,casadio2024nlp} deployed to obtain the safeNLP benchmark.}}
   \label{fig:antonio}
\end{figure}

\paragraph*{Proposed by} Marco Casadio, Ekaterina Komendantskaya, Luca Arnaboldi, Tanvi Dinkar.

\paragraph*{Motivation}
While considerable research has been dedicated to the verification of DNN-based systems in domains such as computer vision, there has been a notable lack of focus on the verification of natural language processing (NLP) systems. This is particularly critical given the rise of conversational agents across various domains, where inaccurate or misleading responses can cause real-world harm. For example, recent EU legislation~\cite{EUlaw} requires chatbots to disclose their non-human nature when queried, and developers of the chatbots should provide firm, and if possible, formal, guarantees that such disclosure will be given in an accurate manner. Medical assistants give another example where formal guarantees about the conversational agent responses are needed in order to safeguard against chatbots  generating harmful medical advice~\cite{bickmore2018patient}. While some initial work has been done in this area of NLP verification~\cite{jia2019certified,huang2019achieving,welbl2020towards,zhang2021certified,wang2023robustness,ko2019popqorn,du2021cert,shi2020robustness,bonaert2021fast}, no agreement on commonly  accepted benchmarks has been reached in this domain. To address this gap, we introduce safeNLP, the first such benchmark. 

\paragraph*{Application}
In~\cite{casadio2024nlp}, we have undertaken a large-scale study of the existing literature on  NLP verification, and distilled common patterns among the existing approaches. Usually, given a dataset consisting of sentences divided into classes, Large Language Models (LLMs) are used to embed these sentences into real-vector spaces, after which smaller neural networks are trained to classify the embedded vectors (relative to the originally given classes). For verification, one can generate meaning-preserving sentence perturbations, again embed them into vector spaces, and verify that subspaces that contain the (embeddings of) the perturbed sentences are classified correctly. Also, in line with classical verification pipelines~\cite{CasadioKDKKAR22}, one can use these input subspaces to train the neural network to be robust on them.  The problem was that each of the existing approaches~\cite{jia2019certified,huang2019achieving,welbl2020towards,zhang2021certified,wang2023robustness,ko2019popqorn,du2021cert,shi2020robustness,bonaert2021fast} used parts of this pipeline in different ways, which made it difficult to compare or audit the results. In~\cite{casadio2023antonio,casadio2024nlp}, we made a generic implementation of this pipeline, 
%(released as a Python package ANTONIO), 
where each of the components of the pipeline is implemented in a modular and transparent way. For example, we can choose  and vary embedding functions, training modes, algorithms for sentence perturbations and algorithms for robust training, independently and modularly; as shown in Figure~\ref{fig:antonio}. This implementation was used to generate the presented VNNCOMP benchmark.

\begin{itemize}
    \item \emph{Datasets:} Although there was no clear consensus in~\cite{jia2019certified,huang2019achieving,welbl2020towards,zhang2021certified,wang2023robustness,ko2019popqorn,du2021cert,shi2020robustness,bonaert2021fast}, the most frequently used dataset in prior works was the IMDB dataset containing film reviews. Its disadvantage is unclear relation to safety critical domains that usually motivate verification efforts. On the other hand, none of the previously used datasets concerned safety-critical applications of NLP. We decided to address this problem, and therefore applied our generic NLP verification pipeline on two safety-critical datasets:  R-U-A-Robot~\cite{gros2021ruarobot}, which focuses on the chatbot disclosure problem, and Medical~\cite{abercrombie2022risk}, which addresses the issue of harmful advice provided by medical chatbots. Both datasets are pre-processed into two classes, positive and negative, to simplify the verification task. For further details on the pre-processing steps and datasets, see~\cite{casadio2024nlp} and the \href{https://github.com/ANTONIONLP/safeNLP}{benchmark GitHub repository}.
    \item \emph{Input Space:} In both datasets, sentences are transformed into fixed-size vector representations, i.e. embeddings, which serve as the inputs to the neural networks. For this VNNCOMP benchmark, we used Sentence-BERT~\cite{reimers-gurevych-2019-sentence}. 
    %This part is handled by ANTONIO, which employs .
    \item \emph{What to Verify:} For each dataset, we generated meaning-preseving sentence perturbations at character and word level as in Moradi et al.~\cite{moradi2021evaluating} and at sentence level with Vicuna~\cite{vicuna2023}. For each positive sentence in the dataset, the smallest hypercube containing the embeddings of all of its obtained perturbations formed one input subspace for verification. Such subspaces were obtained for all positive sentences from the given data set, and were subject to VNCCOMP verification challenge. 
    %ANTONIOs modular approach allows us to independently chose both the perturbation algorithms and the subspaces form.
    %The goal is to verify that every point within these subspaces is classified as positive by the network.
    \item \emph{A note on broader impact:} Verified models can serve as filters for larger NLP systems: e.g. to screen inputs to ensure they meet safety criteria before being passed on to more complex models.
\end{itemize}

\paragraph*{Networks} The safeNLP benchmark includes two neural networks, each corresponding to a different dataset (R-U-A-Robot and Medical). Both networks share the same architecture, consisting of two fully-connected layers. The hidden layer has 128 units with a ReLU activation function, while the output layer has 2 units representing the two classification classes (positive/negative). To enhance the robustness of the networks to the specified safety requirements, they are trained using a custom PGD (Projected Gradient Descent)~\cite{madry2018towards} adversarial training technique. In particular, the PGD attack explores the above-mentioned subspaces of the input space (cf. also Figure~\ref{fig:antonio}). 
%ANTONIO plays a crucial role in this process, allowing us to
%flexibly select the training mode and robustness algorithm. Specifically, we 
%customise the PGD algorithm to work effectively with the subspaces used in our specifications.

\paragraph*{Specifications} The benchmark uses hyper-rectangles in the 30-dimensional embedding space as the subspaces of choice, offering a computationally efficient way to define more precise and adaptable regions compared to the traditional $\epsilon$-cubes. 
%These hyper-rectangles are defined by a series of lower and upper bounds. 
The specifications require verifying that, for a given network and hyper-rectangle, every point within the hyper-rectangle is classified as the positive class by the network. To meet time constraints, we randomly select 1,080 such specifications, each linked to one of the two networks and a corresponding hyper-rectangle, with a timeout of 20 seconds per specification.

\paragraph*{Link} \url{https://github.com/ANTONIONLP/safeNLP}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% sec: dist_shift %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Real-world distribution shifts}
\paragraph*{Proposed by} the Marabou team.
\paragraph*{Motivation}
While robustness against handcrafted perturbations (e.g., norm-bounded) for perception networks are more commonly investigated, robustness against real-world distribution shifts~\cite{wu2022toward} are less studied but of practical interests. This benchmark set contains queries for verifying the latter type of robustness.  
\paragraph*{Networks} The network is a concatenation of a generative model and a MNIST classifier. The generative model is trained to take in an unperturbed image and an embedding of a particular type of distribution shifts in latent space, and produce a perturbed image. The distribution shift captured in this case is the "shear" perturbation. 
\paragraph*{Specifications} The verification task is to certify that a classifier correctly classifies all images in a perturbation set, which is a set of images generated by the generative model given a fixed image and a ball centering the mean perturbations on this image (in the latent space). This mean perturbation is computed by a prior network.
\paragraph*{Link} \url{https://github.com/wu-haoze/dist-shift-vnn-comp}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% sec: cora %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{CORA Benchmark}
\paragraph*{Proposed by} the CORA team.
\paragraph*{Motivation}
The verification of neural networks can be quite slow, i.e., the verification of a single instance can take multiple days -- which is often hard to justify, particularly in safety-critical scenarios. To encourage the fast verification of neural networks, our benchmark focuses on the verification time by setting a small timeout and testing three different (adversarial) training techniques that aim to ease the verifiability.


\paragraph*{Networks} The benchmark consists of one ReLU-neural network architecture (7x250 + ReLU), which was trained on three datasets, (MNIST, SVHN, and CIFAR10), using three different (adversarial) training methods, i.e., standard (point), interval-bound propagation, and set-based. Both interval-bound propagation and set-based training are training methods that improve the robustness of the trained neural network and aim to ease later verification. The neural networks are taken from the first evaluation run of~\cite{koller_et_al_2024}; please refer to~\cite{koller_et_al_2024} for the training details.
\paragraph*{Specifications} All networks are trained on classification tasks. The goal is to verify that no image within a given input set is incorrectly classified.
\paragraph*{Link} \url{https://github.com/kollerlukas/cora-vnncomp2024-benchmark}

\section{Results}


\paragraph{Results} \autoref{tab:score} shows the results. We report the Rank (\textbf{\#}) and  \textbf{\%} is the percentage of solved problems over all problem instances of the corresponding benchmark.   The last two columns break down the number of problems each verifier was able to verify and falsify. For example, for ACAS Xu, all tools other than \crowndefault{} were able to verify all 186 problems (139 + 47), and \crowndefault{} was only able to solve 113 problems (78 + 35), which is 60.8\% of the total problems. 

\newcommand{\nsTwentyFour}{\texttt{NeuralSAT}$_\text{VNN-COMP'24}$}
\newcommand{\ns}{\texttt{NeuralSAT}}

\begin{table}
  \small
  \centering
  \caption{Results over VNN-COMP'24 Benchmarks}\label{tab:score}
       \renewcommand{\arraystretch}{0.6}
  \begin{tabular}{c|ccr|rr}
    \toprule
    \textbf{Benchmark} & \textbf{\#} & \textbf{Tool} & \textbf{\%} & \textbf{Verify} & \textbf{Falsify}  \\

\midrule
\multirow{4}{*}{\rotatebox[origin=c]{0}{ACAS Xu}}
& 1 & \crown{} & 100.0\% & \textbf{139} & \textbf{47} \\
& 1 & \ns{} & 100.0\% & \textbf{139} & \textbf{47} \\
& 1 & \nsTwentyFour{} & 100.0\% & \textbf{139} & \textbf{47} \\
& 4 & \crowndefault{} & 60.8\% & 78 & 35 \\
\midrule
\multirow{4}{*}{\rotatebox[origin=c]{0}{Cgan}} 
& 1 & \crown{} & 100.0\% & \textbf{8} & \textbf{13} \\
& 1 & \ns{} & 100.0\% & \textbf{8} & \textbf{13} \\
& 1 & \nsTwentyFour{} & 100.0\% & \textbf{8} & \textbf{13} \\
& 4 & \crowndefault{} & 33.3\% & 0 & 7 \\
\midrule
\multirow{4}{*}{\rotatebox[origin=c]{0}{Cifar100}} 
& 1 & \crown{} & 77.5\% & \textbf{123} & \textbf{32} \\
& 2 & \ns{} & 76.5\% & 122 & 31 \\
& 3 & \crowndefault{} & 71.0\% & 110 & \textbf{32} \\
& 4 & \nsTwentyFour{} & 64.5\% & 98 & 31 \\
\midrule
\multirow{4}{*}{\rotatebox[origin=c]{0}{Collins Rul CNN}} 
& 1 & \crown{} & 100.0\% & \textbf{30} & \textbf{32} \\
& 1 & \crowndefault{} & 100.0\% & \textbf{30} & \textbf{32} \\
& 1 & \ns{} & 100.0\% & \textbf{30} & \textbf{32} \\
& 1 & \nsTwentyFour{} & 100.0\% & \textbf{30} & \textbf{32} \\
\midrule
\multirow{4}{*}{\rotatebox[origin=c]{0}{Cora}} 
& 1 & \crown{} & 43.9\% & \textbf{24} & \textbf{134} \\
& 1 & \crowndefault{} & 43.9\% & \textbf{24} & \textbf{134} \\
& 1 & \ns{} & 43.9\% & \textbf{24} & \textbf{134} \\
& 1 & \nsTwentyFour{} & 43.9\% & \textbf{24} & \textbf{134} \\
\midrule
\multirow{4}{*}{\rotatebox[origin=c]{0}{Dist Shift}} 
& 1 & \crown{} & 100.0\% & \textbf{64} & \textbf{8} \\
& 1 & \ns{} & 100.0\% & \textbf{64} & \textbf{8} \\
& 3 & \nsTwentyFour{} & 98.6\% & 63 & \textbf{8} \\
& 4 & \crowndefault{} & 94.4\% & 60 & \textbf{8} \\
\midrule
\multirow{4}{*}{\rotatebox[origin=c]{0}{Linearize NN}} 
& 1 & \crown{} & 100.0\% & \textbf{59} & \textbf{1} \\
& 1 & \ns{} & 100.0\% & \textbf{59} & \textbf{1} \\
& 1 & \nsTwentyFour{} & 100.0\% & \textbf{59} & \textbf{1} \\
& 4 & \crowndefault{} & 68.3\% & 40 & \textbf{1} \\
\midrule
\multirow{4}{*}{\rotatebox[origin=c]{0}{Meta Room}} 
& 1 & \crown{} & 98.0\% & \textbf{91} & \textbf{7} \\
& 1 & \ns{} & 98.0\% & \textbf{91} & \textbf{7} \\
& 1 & \nsTwentyFour{} & 98.0\% & \textbf{91} & \textbf{7} \\
& 4 & \crowndefault{} & 0.0\% & 0 & 0 \\
\midrule
\multirow{4}{*}{\rotatebox[origin=c]{0}{Nn4sys}} 
& 1 & \crown{} & 100.0\% & \textbf{194} & 0 \\
& 1 & \ns{} & 100.0\% & \textbf{194} & 0 \\
& 1 & \nsTwentyFour{} & 100.0\% & \textbf{194} & 0 \\
& 4 & \crowndefault{} & 4.1\% & 8 & 0 \\
\midrule
\multirow{4}{*}{\rotatebox[origin=c]{0}{Safe NLP}} 
& 1 & \crown{} & 98.1\% & \textbf{411} & \textbf{648} \\
& 1 & \ns{} & 98.1\% & \textbf{411} & \textbf{648} \\
& 3 & \crowndefault{} & 96.9\% & 401 & 646 \\
& 4 & \nsTwentyFour{} & 94.3\% & 378 & 640 \\
\midrule
\multirow{4}{*}{\rotatebox[origin=c]{0}{Tiny ImageNet}} 
& 1 & \crown{} & 91.5\% & \textbf{140} & \textbf{43} \\
& 2 & \ns{} & 91.0\% & 139 & \textbf{43} \\
& 3 & \crowndefault{} & 89.5\% & 136 & \textbf{43} \\
& 4 & \nsTwentyFour{} & 72.5\% & 102 & \textbf{43} \\
\midrule
\multirow{4}{*}{\rotatebox[origin=c]{0}{TLL Verify Bench}} 
& 1 & \crown{} & 100.0\% & \textbf{15} & \textbf{17} \\
& 1 & \ns{} & 100.0\% & \textbf{15} & \textbf{17} \\
& 1 & \nsTwentyFour{} & 100.0\% & \textbf{15} & \textbf{17} \\
& 4 & \crowndefault{} & 65.6\% & 5 & 16 \\
\midrule
\multirow{4}{*}{\rotatebox[origin=c]{0}{\textbf{Overall}}} 
& 1 & \crown{} & 88.8\% & \textbf{1298} & \textbf{982} \\
& 2 & \ns{} & 88.7\% & 1296 & 981 \\
& 3 & \nsTwentyFour{} & 84.7\% & 1201 & 973 \\
& 4 & \crowndefault{} & 71.9\% & 892 & 954 \\
\bottomrule
  \end{tabular}
\end{table}



\chapter{Benchmarks Generation}\label{chap:benchmarks-generation}




\chapter{Conclusion}

\appendix

\chapter{Comparing neural networks with software}\label{app:nn-vs-software}

\begin{table}
    \caption{Similarities}\label{tab:nn-software-similarities}
    \scriptsize
    \centering
\begin{tabular}{l|l}
    \toprule
    Aspect & Description\\
    \midrule
    Representation &	Represented using if-then-else statements\\
    Input-Output Mapping &	Take Inputs and produce outputs\\
    Determinism	& Can produce deterministic outputs\\
    Execution Path	& Paths from input to output\\
    Specifications & Can have specifications involving preconditions over inputs and postconditions over outputs\\
    Logical Constraints	& Can be represented as logical constraints\\
    Bugs and Errors & Can have bugs that violate specifications\\
    \bottomrule
\end{tabular}    
\end{table}

\begin{table}
    \caption{Differences}\label{tab:nn-software-differences}
    \scriptsize
    \centering
\begin{tabular}{l|l|l}
    \toprule
    Aspect	& Traditional Software	& Neural Networks \\
    \midrule
    % Control Flow	Complex flow with loops, recursion, and branching.	Feedforward execution; no explicit loops or recursion during inference.
    % State Representation	Uses variables, objects, and explicit state transitions.	Encodes state in weights and neuron activations.
    % Granularity	Operates at a higher abstraction level with fewer conditional branches.	Operates on thousands or millions of neurons, creating many implicit conditional branches.
    % Object-Oriented Features	Incorporates OOP principles like encapsulation, inheritance, and polymorphism.	Lacks explicit OOP features but can be modular (e.g., layers, sub-networks).
    % Execution Path	Determined by explicit logic and relatively few branches.	Implicit execution paths defined by piecewise-linear regions from activations.
    % Termination	Explicitly handles termination in loops and recursion.	Fixed-length inference process with no explicit termination conditions.
    % Semantics	Implements precise, deterministic logic for well-defined tasks.	Approximates functions based on training, often non-deterministic during learning.
    % Verification	Formal verification methods are established for correctness.	Verification is challenging due to non-linearity and high dimensionality.
    % Debugging	Tools like debuggers allow step-by-step inspection of logic.	Focuses on diagnosing data issues, architecture problems, or training convergence.
    Explainability & Can be explained through code inspection & Lacks explicit explainability mechanisms\\
    Designing &	Written by human & Created by machine through learning from training data\\
    Size & Small to medium-sized codebases & Large number of neurons and layers\\
    Error Causing & arise from bugs in logic or implementation & arise from insufficient training data, overfitting, etc\\
    Error Incurring & Can cause crashes, incorrect outputs, or unexpected behavior & Can cause incorrect predictions or unexpected behavior\\
    Error Handling & try-catch blocks for exception handling &	Lacks explicit error-handling mechanisms\\
    % Learning	Does not learn; behavior must be explicitly defined.	Learns to approximate functions through training.
    % Non-Linearity	Handles non-linearity explicitly through conditional logic or complex mathematical operations.	Uses activation functions like ReLU, Sigmoid, or Tanh to introduce non-linearity.
    % Parallelism	Often single-threaded or relies on explicit multi-threading for parallelism.	Intrinsic large-scale parallelism due to the structure of layers and neurons.
    % Adversarial Vulnerability	Less vulnerable to adversarial inputs, but bugs can lead to unexpected behavior.	Susceptible to small, imperceptible perturbations in input (adversarial examples).
    % Generalization	Executes pre-defined logic; does not generalize beyond explicit instructions.	Generalizes from training data, allowing predictions on unseen inputs.    
    % Dynamic Resource Management	Explicitly handles memory, files, and resources during execution.	Resources are statically allocated during training and inference; no dynamic management.
    % Concurrency	Explicitly handles concurrent operations using threads or async programming.	No explicit concurrency; parallelism occurs naturally due to network architecture.
    \bottomrule
\end{tabular}    
\end{table}

\chapter{Logics and Linear Programming}\label{app:logics}

\section{Logics and Satisfiability}\label{app:logics}
\section{Linear Programming}\label{app:lp}

\tvn{Hai, give some LP background here.}

\chapter{Software vs DNN Verification}\label{app:software-vs-dnn}


\bibliographystyle{abbrv}
\bibliography{book.bib}

\appendix

\end{document}
