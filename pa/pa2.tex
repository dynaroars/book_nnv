\section{PA2: Abstract Domain Analysis of Neural Networks}\label{sec:pa2}

In this PA you will implement neural network (NN) verification using (1) \emph{interval abstraction} as described in~\autoref{sec:interval-abstraction} and (2) \emph{zonotope abstraction} as described in~\autoref{sec:zonotope-abstraction}. 
While symbolic execution (PA1~\autoref{sec:pa1}) computes exact symbolic representations, abstract interpretation uses over-approximations to make verification more scalable at the cost of some precision. You will implement interval and zonotope abstract transformers and compare their precision-scalability trade-offs with symbolic execution.

You will use Python and PyTorch for this assignment. \textbf{Do not} use external verification libraries (other than standard numerical libraries like \texttt{torch}, \texttt{numpy}).

This PA has three main parts: 
\begin{enumerate}
    \item implement interval abstraction for linear layers and ReLU activations.
    \item implement zonotope abstraction for linear layers and ReLU activations.
    \item evaluate the precision/scalability compared to symbolic execution from PA1.
\end{enumerate}

\subsection{Interval Abstraction}

Interval abstraction is the simplest form of abstract interpretation for neural networks. Each variable is represented by an interval $[l, u]$ indicating its lower and upper bounds.

\subsubsection{Part 1: Interval for Linear Layers}

Linear transformations on intervals can be computed exactly using interval arithmetic.

\begin{lstlisting}[language=Python]
class LinearTransformer:
    def __init__(self, W, b):
        # TODO: store weight matrix and bias vector
    
    def forward(self, input_lower, input_upper):
        """
        Apply affine transformation f(x) = Wx + b to intervals [l, u]
        
        Mathematical formulation:
        - W+: positive part of weights
        - W-: negative part of weights 
        - output_lower = W+ @ input_lower + W- @ input_upper + bias
        - output_upper = W+ @ input_upper + W- @ input_lower + bias
        
        Args:
            input_lower: lower bounds of input intervals
            input_upper: upper bounds of input intervals
            
        Returns:
            (output_lower, output_upper): transformed interval bounds
        """
        # TODO:
        # 1. Split weights into positive and negative parts
        # 2. Compute output bounds using interval arithmetic
        # 3. Add bias to both bounds
        # 4. Return (output_lower, output_upper)
        
        return output_lower, output_upper
        
\end{lstlisting}

\subsubsection{Part 2: Interval for ReLU Activations}

ReLU transformation on intervals is straightforward: clamp lower bounds to 0.

\begin{lstlisting}[language=Python]
class ReluTransformer:
    
    def forward(self, input_lower, input_upper):
        """
        Apply ReLU transformation to intervals: ReLU(x) = max(0, x)
        Args:
            input_lower: lower bounds of input intervals  
            input_upper: upper bounds of input intervals
            
        Returns:
            (output_lower, output_upper): intervals after ReLU
        """
        # TODO: For each dimension:
        # 1. Clamp lower bounds to be >= 0
        # 2. Clamp upper bounds to be >= 0  
        # 3. Return transformed bounds
        
        return output_lower, output_upper
        
\end{lstlisting}

\subsubsection{Part 3: Putting It All Together}

\begin{lstlisting}[language=Python]
class IntervalAbstraction:

    def __init__(self, DNN):
        # TODO: convert DNN to corresponding interval abstraction transformers
        self.transformers = ...

    def forward(self, lb, ub):
        # propagate zonotope through layers
        for transformer in self.transformers:
            lb, ub = ...

        return (lb, ub)

\end{lstlisting}

\subsubsection{Part 4: Test Your Implementation}

In this DNN, the outputs of the neurons in the hidden layers (prefixed with \texttt{n}) are applied with the \texttt{relu} activation function, but the outputs of the DNN (prefixed with \texttt{o}) are not.  These settings are controlled by the \texttt{True}, \texttt{False} parameters as shown above.  Also, this example does not use \texttt{bias}, i.e., bias values are all 0.0's as shown. 

\begin{lstlisting}[language=Python]
    # DNN specification
    n00 = ([1.0, 1.0], 2.0, True)
    n01 = ([1.0, -1.0], 2.0, True)
    hidden_layer0 = [n00, n01]

    n10 = ([1.0, 1.0], -0.5, True)
    n11 = ([1.0, -1.0], -1.0, True)
    hidden_layer1 = [n10, n11]

    n20 = ([-1.0, 1.0], 7.0, False)
    n21 = ([0.0, 1.0], 0.0, False)
    hidden_layer2 = [n20, n21]

    o0 = ([1.0, -1.0], 0.0, False)
    output_layer = [o0]
    
    dnn = [hidden_layer0, hidden_layer1, hidden_layer2, output_layer]
\end{lstlisting}

Note that all of these settings are parameterized and I deliberately use this example to show these how these parameters are used (e.g., \texttt{relu} only applies to hidden neurons, but not outputs).


\begin{lstlisting}[language=Python]
    net = IntervalAbstraction(dnn)
    input_lower = torch.tensor([-1, -1], dtype=torch.float)
    input_upper = torch.tensor([1, 3], dtype=torch.float)
    output_lower, output_upper = net(input_lower, input_upper)
    print(f'{output_lower=}')
    print(f'{output_upper=}')
    # output_lower=tensor([-7.5000])
    # output_upper=tensor([12.])
\end{lstlisting}


\subsection{Zonotope Abstraction}

\subsubsection{Part 1: Zonotope Representation}

\paragraph{From Bounds to Zonotope}

A zonotope is represented by a tuple of a center vector and a generator matrix. 
Given the bounds of a zonotope, we can convert it to a zonotope.
For a concrete example, consider~\autoref{ex:zonotope_linear_example}.
\begin{lstlisting}[language=Python]
def zonotope(lb, ub):
    """
    Computes zonotope from interval bounds [l, u].
    TODO: 
    - compute center from bounds
    - generator from bounds
    """
    return center, generator
\end{lstlisting}

\paragraph{From Zonotope to Bounds}

Given the center and generators of a zonotope, we can concretize the bounds of the zonotope.
\begin{lstlisting}[language=Python]
def concrete(center, generator):
    """
    Computes interval bounds [l, u] from zonotope (center, generator).
    
    A zonotope $\mathcal{Z} = \{c + \sum_{i=1}^{m} \epsilon_i g_i \mid \epsilon_i \in [-1,1]\}$ can be converted to interval bounds:
    Lower bound: $l = c - \sum_{i=1}^{m} |g_i|$ (when all $\epsilon_i = -\text{sign}(g_i)$)
    Upper bound: $u = c + \sum_{i=1}^{m} |g_i|$ (when all $\epsilon_i = +\text{sign}(g_i)$)
    
    Args:
        center: Center vector c of the zonotope
        generator: Generator matrix G 
        
    Returns:
        (lb, ub): Interval bounds for each dimension

    # TODO: compute interval bounds from zonotope
    # 1. compute lower bound
    # 2. compute upper bound
    """
    return lb, ub
    
\end{lstlisting}

\subsubsection{Part 2: Zonotope for Linear Layers}
Linear (affine) transformations can be handled exactly in zonotope abstraction. You will implement the missing components in the provided zonotope framework.

\begin{lstlisting}[language=Python]
class LinearTransformer(nn.Module):
    def __init__(self, W, b):
        # TODO: store weights and bias
    
    def forward(self, center, generator):
        """
        Apply affine transformation f(x) = Wx + b to zonotope Z = (c, G)
        Result: f^a(Z) = (Wc + b, WG)
        
        Args:
            center: center vector c of input zonotope
            generator: generator matrix G of input zonotope  
            
        Returns:
            (new_center, new_generator): transformed zonotope
        """
        # TODO: 
        # 1. Transform center using weight matrix and bias
        # 2. Transform generator matrix using weight matrix 
        # 3. Return transformed (center, generator)

        return new_center, new_generator

    \end{lstlisting}


\subsubsection{Part 3: Zonotope for ReLU Activations}

ReLU activations are non-linear and require over-approximation in zonotope abstraction. For each neuron with bounds $[l, u]$, there are three cases to handle.

\begin{lstlisting}[language=Python]
class ReluTransformer(nn.Module):
    
    def forward(self, center, generator):
        """
        Apply ReLU abstraction to zonotope representation: ReLU(x) = max(0, x)
        
        Three cases for each neuron i with bounds [l_i, u_i]:
        1. Active case (l_i >= 0): ReLU is identity, Z' = Z  
        2. Inactive case (u_i <= 0): ReLU outputs 0, Z' = {0}
        3. Unstable case (l_i < 0 < u_i): requires over-approximation
        
        Args:
            center: center vector of input zonotope
            generator: generator matrix of input zonotope  
            
        Returns:
            (new_center, new_generator): over-approximated zonotope after ReLU
        """
        # TODO:
        # 1. Compute current bounds using concrete(center, generator)
        # 2. Create new generator matrix with extra row for ReLU error
        # 3. For each neuron, handle based on bounds [l, u]:
        #    - Active (l >= 0): keep unchanged
        #    - Inactive (u <= 0): set to zero
        #    - Unstable (l < 0 < u): apply slope approximation
        # 4. Return transformed (center, generator)

        return new_center, new_generator

\end{lstlisting}

\subsubsection{Part 4: Putting It All Together}

\begin{lstlisting}[language=Python]
    class ZonotopeAbstraction(nn.Module):

        def __init__(self, DNN):
            # TODO: convert DNN to corresponding zonotope abstraction transformers
            self.transformers = ...

        def forward(self, lb, ub):
            # 1. convert bounds to zonotope
            center, generator = ...
            # 2. propagate zonotope through layers
            for transformer in self.transformers:
                center, generator = ...

            # 3. convert zonotope to bounds
            lb, ub = ...
            return (lb, ub)

\end{lstlisting}

\subsubsection{Part 5: Test Your Implementation}


\begin{lstlisting}[language=Python]
    net = ZonotopeAbstraction(dnn) # same DNN specification as interval
    input_lower = torch.tensor([-1, -1], dtype=torch.float)
    input_upper = torch.tensor([1, 3], dtype=torch.float)
    output_lower, output_upper = net(input_lower, input_upper)
    print(f'{output_lower=}')
    print(f'{output_upper=}')
    # output_lower=tensor([0.1667])
    # output_upper=tensor([6.1667])
\end{lstlisting}



\subsection{Evaluation}

\begin{itemize}
    \item Run your abstract domains on DNNs in PA1 (\autoref{sec:pa1}).
    \item Compare the precision and scalability trade-offs between interval/zonotope abstraction with Symbolic Execution.
    \item How to improve the precision of your abstract domains?
\end{itemize}

\subsection{Grading Rubric (out of 30 points)}

\begin{itemize}
\item 10 points — for complete interval abstraction implementation
    \begin{itemize}
    \item 4 points for \texttt{LinearTransformer} implementation
    \item 4 points for \texttt{ReluTransformer} implementation
    \item 2 points for \texttt{IntervalAbstraction} class integration
    \end{itemize}
\item 15 points — for complete zonotope abstraction implementation
    \begin{itemize}
    \item 3 points for zonotope representation functions (\texttt{zonotope} and \texttt{concrete})
    \item 5 points for \texttt{LinearTransformer} implementation
    \item 5 points for \texttt{ReluTransformer} implementation
    \item 2 points for \texttt{ZonotopeAbstraction} class integration
    \end{itemize}
\item 5 points — for evaluation and analysis
    \begin{itemize}
    \item 3 points for precision and scalability comparison between interval/zonotope/symbolic execution
    \item 2 points for discussion on improving abstract domain precision
    \end{itemize}
\item 5 points — for implementation quality and documentation
    \begin{itemize}
    \item 3 points for clean, well-documented code with clear variable names
    \item 2 points for testing on additional example
    \end{itemize}
\item \textbf{BONUS} (up to 5 extra points) — for novel improvements to abstract domains
    \begin{itemize}
    \item 3 points for implementing improvements
    \item 2 points for thorough analysis and comparison of the improvements
    \end{itemize}
\end{itemize}