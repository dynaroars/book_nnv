\section{PA2: Abstract Domain Analysis of Neural Networks}\label{sec:pa2}

In this PA you will implement neural network (NN) verification using \emph{zonotope abstraction} as described in~\autoref{sec:zonotope-abstraction}. While symbolic execution (PA1) computes exact symbolic representations, abstract interpretation uses over-approximations to make verification more scalable at the cost of some precision. You will implement zonotope abstract transformers and compare their precision-scalability trade-offs with symbolic execution.

You will use Python and PyTorch for this assignment. \textbf{Do not} use external verification libraries (other than standard numerical libraries like \texttt{torch}, \texttt{numpy}).

This PA has two main parts: (1) implement zonotope abstract transformers for linear layers and ReLU activations, and (2) evaluate the precision and scalability compared to symbolic execution from PA1.


\subsection{Part 1: Zonotope Representation}

\subsubsection{From Bounds to Zonotope}

A zonotope is represented by a tuple of a center vector and a generator matrix. 
Given the bounds of a zonotope, we can convert it to a zonotope.
\begin{lstlisting}[language=Python]
def zonotope(lb, ub):
    """
    Computes zonotope from interval bounds [l, u].
    """
    center = (lb + ub) / 2
    generator = (ub - lb) / 2
    return center, generator
\end{lstlisting}

\subsubsection{From Zonotope to Bounds}

Given the center and generators of a zonotope, we can concretize the bounds of the zonotope.
\begin{lstlisting}[language=Python]
def concrete(center, generator):
    """
    Computes interval bounds [l, u] from zonotope (center, generator).
    
    A zonotope $\mathcal{Z} = \{c + \sum_{i=1}^{m} \epsilon_i g_i \mid \epsilon_i \in [-1,1]\}$ can be converted to interval bounds:
    Lower bound: $l = c - \sum_{i=1}^{m} |g_i|$ (when all $\epsilon_i = -\text{sign}(g_i)$)
    Upper bound: $u = c + \sum_{i=1}^{m} |g_i|$ (when all $\epsilon_i = +\text{sign}(g_i)$)
    
    Args:
        center: Center vector c of the zonotope
        generator: Generator matrix G 
        
    Returns:
        (lb, ub): Interval bounds for each dimension

    # TODO: compute interval bounds from zonotope
    # 1. compute lower bound
    # 2. compute upper bound
    """
    return lb, ub
    
\end{lstlisting}

\subsection{Part 2: Zonotope for Linear Layers}
Linear (affine) transformations can be handled exactly in zonotope abstraction. You will implement the missing components in the provided zonotope framework.

\begin{lstlisting}[language=Python]
class LinearTransformer(nn.Module):
    def __init__(self, W, b):
        # TODO: store weights and bias
    
    def forward(self, center, generator):
        """
        Apply affine transformation f(x) = Wx + b to zonotope Z = (c, G)
        Result: f^a(Z) = (Wc + b, WG)
        
        Args:
            center: center vector c of input zonotope
            generator: generator matrix G of input zonotope  
            
        Returns:
            (new_center, new_generator): transformed zonotope
        """
        # TODO: 
        # 1. Transform center using weight matrix and bias
        # 2. Transform generator matrix using weight matrix 
        # 3. Return transformed (center, generator)

        return new_center, new_generator

    \end{lstlisting}


\subsection{Part 3: Zonotope for ReLU Activations}

ReLU activations are non-linear and require over-approximation in zonotope abstraction. For each neuron with bounds $[l, u]$, there are three cases to handle.

\begin{lstlisting}[language=Python]
class ReluTransformer(nn.Module):
    
    def forward(self, center, generator):
        """
        Apply ReLU abstraction to zonotope representation: ReLU(x) = max(0, x)
        
        Three cases for each neuron i with bounds [l_i, u_i]:
        1. Active case (l_i >= 0): ReLU is identity, Z' = Z  
        2. Inactive case (u_i <= 0): ReLU outputs 0, Z' = {0}
        3. Unstable case (l_i < 0 < u_i): requires over-approximation
        
        Args:
            center: center vector of input zonotope
            generator: generator matrix of input zonotope  
            
        Returns:
            (new_center, new_generator): over-approximated zonotope after ReLU
        """
        # TODO:
        # 1. Compute current bounds using concrete(center, generator)
        # 2. Create new generator matrix with extra row for ReLU error
        # 3. For each neuron, handle based on bounds [l, u]:
        #    - Active (l >= 0): keep unchanged
        #    - Inactive (u <= 0): set to zero
        #    - Unstable (l < 0 < u): apply slope approximation
        # 4. Return transformed (center, generator)

        return new_center, new_generator

\end{lstlisting}

\subsection{Part 4: Putting It All Together}

\begin{lstlisting}[language=Python]
    class ZonotopeAbstraction(nn.Module):

        def __init__(self, dnn):
            # TODO: convert dnn layers to corresponding zonotope abstraction transformers
            self.transformers = ...

        def forward(self, lb, ub):
            # 1. convert bounds to zonotope
            center, generator = ...
            # 2. propagate zonotope through layers
            for transformer in self.transformers:
                center, generator = ...

            # 3. convert zonotope to bounds
            lb, ub = ...
            return (lb, ub)

\end{lstlisting}

\subsection{Part 5: Evaluation}